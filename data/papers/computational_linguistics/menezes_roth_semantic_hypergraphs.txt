Semantic Hypergraphs
Telmo Menezes*1 and Camille Roth†1,2
1Computational Social Science Team, Centre Marc Bloch Berlin (CNRS/HU), Friedrichstr. 191, 10117 Berlin, Germany
2CAMS (Centre Analyse et Mathématique Sociales, UMR 8557 CNRS/EHESS), 54 Bd Raspail, 75007 Paris, France
Abstract
Approaches to Natural language processing (NLP) may
be classified along a double dichotomy open/opaque –
strict/adaptive. The former axis relates to the possibility
of inspecting the underlying processing rules, the latter
to the use of fixed or adaptive rules. We argue that many
techniques fall into either the open-strict or opaque-
adaptive categories. Our contribution takes steps in the
open-adaptive direction, which we suggest is likely to
provide key instruments for interdisciplinary research.
The central idea of our approach is the Semantic Hyper-
graph (SH), a novel knowledge representation model that
is intrinsically recursive and accommodates the natural
hierarchical richness of natural language. The SH model
is hybrid in two senses. First, it attempts to combine the
strengths of ML and symbolic approaches. Second, it is
a formal language representation that reduces but tol-
erates ambiguity and structural variability. We will see
that SH enables simple yet powerful methods of pattern
detection, and features a good compromise for intelligi-
bility both for humans and machines. It also provides
a semantically deep starting point (in terms of explicit
meaning) for further algorithms to operate and collabo-
rate on. We show how modern NLP ML-based building
blocks can be used in combination with a random for-
est classifier and a simple search tree to parse NL to SH,
and that this parser can achieve high precision in a diver-
sity of text categories. We define a pattern language rep-
resentable in SH itself, and a process to discover knowl-
edge inference rules. We then illustrate the efficiency of
the SH framework in a variety of tasks, including con-
junction decomposition, open information extraction,
concept taxonomy inference and co-reference resolu-
tion, and an applied example of claim and conflict anal-
ysis in a news corpus.
Keywords: natural language understanding; knowledge
representation; information extraction; inference sys-
tems; explainable artificial intelligence; hypergraphs
*menezes@cmb.hu-berlin.de ()
†roth@cmb.hu-berlin.de
1 Introduction
Natural language processing (NLP) approaches generally
belong to either one of two main strands, which also of-
ten appear to be mutually exclusive. On the one hand
we essentially have symbolic methods and models which
are open, in the sense that their internal mechanisms as
well as their conclusions are easy to inspect and under-
stand, but which deal with linguistic patterns in a rela-
tively strict manner. On the other hand, we have adap-
tive models based on machine learning (ML) which are
usually opaque to inspection and too complex for their
reasoning to be intelligible, but which achieve increas-
ingly impressive feats that suggest deeper understand-
ing.
Presently, there is a strong research focus on the lat-
ter, and for good reason. Among adaptive models, deep
neural networks, for one, managed to jointly learn and
improve performance in classic NLP tasks such as part-
of-speech tagging, chunking, named-entity recognition,
and semantic role-labeling [as early as 19]. In other
cases, modern ML enabled methods that did not ex-
ist before e.g., estimation of semantic similarity using
word embeddings [45]. More recently, Bidirectional En-
coder Representations from Transformers (BERT) have
shown that pre-trained general models can be fine-
tuned to achieve state-of-the-art performance in specific
language understanding tasks such as question answer-
ing and language inference [21]. Nonetheless, symbolic
methods possess several proper and important features,
namely that they can offer human-readable knowledge
representations of knowledge, as well as language under-
standing through formal and inspectable rule-based log-
ical inference.
Why do we observe this apparent trade-off between
openness and adaptivity? Initial approaches to NLP were
of a symbolic nature, based on rules written by hand,
or in algorithms akin to the ones that are used for pro-
gramming language interpreters and compilers, such as
recursive descent parsers. It became apparent that the
diversity of grammatical constructs that can be found in
natural language is too large to be tackled in such a way.
The problem is compounded by the frequent use of un-
grammatical constructs that are nevertheless frequent in
real-world language usage (e.g. simple mistakes, neol-
1
arXiv:1908.10784v2 [cs.IR] 18 Feb 2021
ogisms, slang). In other words, content in natural lan-
guage is generated by actors that are much more com-
plex, and also more error-prone and error-tolerant than
conventional algorithms. ML is a natural fit for this type
of problem and, as we mentioned, vastly surpasses the
capabilities of human-created symbolic systems in a va-
riety of tasks.
We suggest however that there is a “hidden rela-
tionship” between explicit symbolic manipulation rules
and modern ML: the latter can be seen as a form of
“automatic programming” through large-scale statisti-
cal learning processes, that amount to the generation of
highly complex programs through adaptive pressure in-
stead of human programmers’ efforts. It does not matter
if it is gradient descent on a multi-layered network topol-
ogy, or something more prosaic like entropy reduction
in a decision tree, it is still program generation through
adaptation. The capability of these methods to generate
such complex programs is what allows them to tackle the
complexities of NL, but it is also this very complexity that
makes them opaque.
We can thus imagine a double dichotomy
open/opaque – strict/adaptive. We argue that existing
approaches generally fall into either the open-strict or
opaque-adaptive categories. A few approaches have
ventured into the open-adaptive domain [7, 40] and our
contribution aims at significantly expanding this direc-
tion. Before discussing our approach, let us consider
why open-adaptive is a desirable goal. The work we
present here was performed in the context of a computa-
tional social science (CSS) research team, where NLP is a
scientific instrument capable of assisting in the analysis
of text corpora that are too vast for humans to study
in detail. We argue that further progress in the study
of socio-technical systems and their dynamics could
be enabled by open-adaptive scientific instruments for
language understanding.
In current CSS research, the more common ap-
proaches aim to transform natural language documents
into structured data that can be more easily analyzed
by scholars and are referred to by a variety of umbrella
terms such as “text mining” [69], “automated text anal-
ysis” [29] or “text-as-data methods” [77]. They exhibit
a wide range of sophistication, from simple numerical
statistics to more elaborate ML algorithms. Some meth-
ods indeed rely essentially on scalar numbers, for in-
stance by measuring text similarity (e.g., with cosine dis-
tance [64]) or attributing a valence to text, as in the case
of ideological estimation [63] or sentiment analysis [54],
which in practice may be used to appraise the emotional
content of a text (anger, happiness, disagreement, etc.)
or public sentiment towards political candidates in so-
cial media [76]. Similarly, political positions in docu-
ments may be inferred from so-called “Wordscores” [39]
– a popular method in political science that also relies
on the summation of pre-computed scores for individ-
ual words, and has more refined elaborations, e.g. with
Bayesian regularization [47]. Other methods preserve
the level of words: such is the case with term and pattern
extraction (i.e., discovering salient words through the use
of helper measures like term frequency–inverse docu-
ment frequency (TF-IDF) [60]), so-called “Named Entity
Recognition” [50] (used to identify people, locations, or-
ganizations, and other entities mentioned in corpuses,
for example in news corpora [22] or Twitter streams [57])
and ad-hoc uses of conventional computer science ap-
proaches such as regular expressions to identify chunks
of text matching against a certain pattern (for example,
extracting all p-values from a collection of scientific arti-
cles [17]). Another strand of approaches operates at the
level of word sets, including those geared at topic detec-
tion (such as co-word analysis [37], Latent Dirichlet Allo-
cation (LDA) [12] and TextRank [44], used to extract the
topics addressed in a text) or used for relationship ex-
traction (meant at deriving semantic relations between
entities mentioned in a text, e.g., is(Berlin, City)) [4]. Re-
cent advances in embedding techniques have also made
it possible to describe topics extensionally as clusters of
documents in some properly defined space [5, 33].
Overall, these techniques provide useful approaches
to analyze text corpora at a high level, for example, with
regard to their main entities, relationships, sentiment,
and topics. However, there is limited support to detect,
for instance, more sophisticated claim patterns across
a large volume of texts, what recurring statements are
made about actors or actions, and what are the qual-
itative relationships among actors and concepts. This
type of goal, for example, extends semantic analysis to a
socio-semantic framework [58] which also takes into ac-
count actors who make claims or who are the target of
claims [22].
It is also particularly interesting to consider the
model of knowledge representation that is implicitly
or explicitly associated with the various NLP/text min-
ing/information extraction approaches. To illustrate,
on one extreme we can consider traditional knowledge
bases and semantic graphs, which are open in our sense,
but also limited in their expressiveness and depth. On
the other, we have the extensive knowledge opaquely en-
coded in neural network models such as BERT or GPT-
2/3 [e.g. 15]. Beyond the desirability of open knowledge
bases for their own sake, we propose that a language rep-
resentation that is convenient for both humans and ma-
chines can constitute a lingua franca, through which sys-
tems of cognitive agents of different natures can coop-
erate in a way that is understandable and inspectable.
Such systems could be used to combine the strengths of
symbolic and statistical inference.
The central idea of our approach is the Semantic Hy-
pergraph (SH), a novel knowledge representation model
that is intrinsically recursive and accommodates the nat-
ural hierarchical richness of NL. The SH model is hybrid
2
in two senses. First, it attempts to combine the strengths
of ML and symbolic approaches. Second, it is a formal
language representation that reduces but tolerates ambi-
guity, and that also reduces structural variability. We will
see that SH enables simple methods of pattern detec-
tion to be more powerful and less brittle, that it is a good
compromise for intelligibility both for humans and ma-
chines, and that it provides a semantically deeper start-
ing point (in terms of explicit meaning) for further algo-
rithms to operate and collaborate on.
In the next section we discuss the state of the art,
comparing SH to a number of approaches from various
fields and eras. We then describe the structure and syn-
tax of SH, followed by an explanation on how modern
and standard NLP ML-based building blocks provided
by an open source software library [31] can be used in
combination with a random forest classifier and a sim-
ple search tree to parse NL to SH. Here we also pro-
vide precision benchmarks of our current parser, which
is then employed in the experiments that follow. We at-
tempted to perform a set of experiments of a rather di-
verse nature, to gather evidence of SH usefulness in a
variety of roles, and of its potential to tackle the chal-
lenge that we started by stating in this introduction, and
to gather empirical insights. One important language
understanding task is information extraction from text.
One formulation of such a task that attracts significant
attention is that of Open Information Extraction (OIE)
— the domain-free extraction from text of tuples (typ-
ically triplets) representing semantic relationships [24].
We will show that a small and simple set of SH patterns
can produce competitive results in an OIE benchmark,
when pitted against more complex and specialized sys-
tems in that domain. We will demonstrate concept tax-
onomy inference and co-reference resolution, followed
by claim and conflict identification in a database of news
headers. We will show how SH can be used to generate
semantically rich visual summaries of text.
2 Related Work
Knowledge bases. As a knowledge representation for-
malism, it is interesting to compare SH with traditional
approaches. Let us start with triplet-based ones. For
example, the Semantic Web [10, 62] community tends
to use standards such as RDFa [1], which represent
knowledge as subject-predicate-object expressions, and
are conceptually equivalent to semantic graphs [3, 66]
(similarly, a particular type of hypergraph has been used
in [16] to represent tagged resources by users, yet this
also reduces to fixed triplet conceptualization). Despite
their usefulness for simple cases, such approaches can-
not hope to match the semantic sophistication of what
can be conveyed with open text. Binary relationships
and lack of recursion limit the expressive power of se-
mantic graphs, and we sill see how SHs can represent se-
mantic information that is lost in the graphic represen-
tation, for example the ability to express n-ary relation-
ships, propositions about propositions and constructive
definitions of concepts.
A further type of approaches relying on knowledge
bases is epitomized by the famous Cyc [36] project, a
multi-decade enterprise to build a general-purpose and
comprehensive system of concepts and rules. It is an im-
pressive effort, nevertheless hindered by the limitations
that we alluded to in the previous section concerning
the ambiguity and diversity of semantic structures con-
tained in NL, given that it relies purely on symbolic rea-
soning. Cyc belongs to a category of systems that are
mostly concerned with question answering, a different
aim that the one of the work that we propose here, which
is more concerned with aiding in the analysis and sum-
marization of large corpora of text for research purposes,
especially in the social sciences, while not requiring full
disambiguation of meaning nor perfect reasoning or un-
derstanding.
Several other notable knowledge bases of a similar se-
mantic graph nature have been developed, some relying
on collaborative human efforts to gather ground asser-
tions, for example MIT’s ConceptNet [68], ATOMIC [61]
from the Allen Institute, or very rigorous scholarly ef-
forts of annotation, as is the case with WordNet [46]
and its multiple variants, or relying on wiki-like plat-
forms such as WikiData [75], or mining relationship
from Wikipedia proper, as is the case with DBPedia [6],
and more recently a transformer language model has
been proposed to automatically extend common-sense
knowledge bases [14]. We envision that such general-
knowledge bases could be fruitfully integrated with SHs
for various purposes, but such endeavours are beyond
the scope of this work. We are instead interested in
demonstrating what can be achieve by going beyond
such non-hypergraphic appraches.
Hypergraphic approaches to knowledge representa-
tion. Hypergraphs have been proposed already in the
1970s as a general solution for knowledge representa-
tion [13]. More recently, Ben Goertzel produced simi-
lar insights [28], and in fact included an hypergraphic
database called AtomSpace as the core knowledge rep-
resentation of his OpenCog framework [30], an attempt
to make Artificial General Intelligence emerge from the
interaction of a collection of heterogeneous system. As
is the case with Cyc, the goals of OpenCog are however
quite distinct from the aim of our work.
A model that shares similarities with ours but purely
aims at solving a meaning matching problem is that
of Abstract Meaning Representation (AMR) [7]. AMR is
based on PropBank verbal propositions and their ar-
guments [53], ensuring that all such meaning struc-
tures can be represented. SH completeness is based in-
stead on Universal Dependencies [52], ensuring instead
3
that all cataloged grammatical constructs can be repre-
sented. AMR’s goal is to purely abstract meaning, while
SH accommodates the ambiguity of the original NL ut-
terances, bringing several important benefits: it makes
their computational processing tractable in further ways,
tolerates mistakes better and preserves communication
nuance that would otherwise be lost. Furthermore, it
remains open to structures that may not be currently
envisioned. Parsing AMR to NL is a particularly hard
task and, to our knowledge, there is currently no parser
that approaches the capabilities of what we will demon-
strate in this work. In part, this is a practical problem:
we will see how we can take advantage of intermediary
NLP tasks that are well studied and developed to achieve
NL to SH parser. Doing the same for AMR requires the
construction of training data by extensive annotation ef-
forts by humans. It could be argued that this is still a
preferable goal, no matter how distant, given that AMR
removes all ambiguity from statements. Here we point
out that this aspect of AMR is also a downside, firstly be-
cause it makes all failures of understanding catastrophic
(we will see how this is not the case for SH), and secondly
because NL is inherently ambiguous. It is often the case
that even human beings cannot fully resolve ambigui-
ties, or that an ambiguous statement gains importance
later on, with more information. We aim to define SH as
a lingua franca for the collaboration of human an algo-
rithmic actors of several natures, a less rigid goal than the
one embodied by AMR.
Free text parsing. A classical NLP task is that of mak-
ing explicit the grammatical structure of a sentence in
the form of a parse tree. A particularly common type of
such a tree in current use is the Dependency Parse Tree
(DPT), based on dependency grammars. We will see that
our own parser takes advantage of DPTs (among other
high-level grammatical / linguistic features) as interme-
diary steps, but it is also interesting to notice that DPTs
themselves can be considered as a type of hypergraphic
representation of language [56]. In fact, as we will discuss
below, they are already employed in various targeted lan-
guage understanding tasks in a CSS context.
From the perspective of hypergraphic representation
of language, the fundamental difference between DPTs
and SHs is that the former aims at expressing the gram-
matical structure of language, while the latter its seman-
tic structure, in the simplest possible way that enables
meaning extraction in a principled and predictable way.
In contrast to the ad-hoc nature of information extrac-
tion from DPTs, we will see that SHs structure NL in a way
akin to functional computer languages, and allow for ex-
ample for a generic methodology of extracting patterns.
The expressive power of such patterns will be demon-
strated in several ways, namely by demonstrating com-
petitive results in a standard Open Information Extrac-
tion task. We will see that the type system of SHs (relying
on 8 types) is much simpler than the diversity of gram-
matical roles contained in a typical set of dependency la-
bels (such as Universal Dependencies), and we will also
provide empirical evidence that SHs are not isomorphic
to DPTs.
In the realm of OIE, one approach in particular with
which our work shares some similarities is that of learn-
ing open pattern templates [40]. These pattern templates
combine at the same symbolic level dependency parse
labels and structure, part-of-speech tags, explicit lexical
constraints and higher-order inferences (e.g. that some
term refers to a person), to achieve sophisticated lan-
guage understanding in the extraction of OIE tuples, be-
ing able to extract relations that are not only of a verbal
nature, and demonstrating sensitivity to context. The
work we will present does not attempt to directly com-
bine diverse linguistic features at the service of a spe-
cific language understanding task. Instead, we propose
to use such features to aid in the translation of NL into
a structured representation, which relies by comparison
on a very simple and uniform type system, and from
which complex NL understanding tasks become easier,
and that is of general applicability to a diversity of such
tasks, while remaining fully readable and understand-
able by humans. Furthermore, it defines a system of
knowledge representation in itself, that is directly fo-
cused on meaning instead of grammar.
Text mining. We have already covered in the previ-
ous section the most commonly used text mining ap-
proaches, while emphasizing the relative lack of sophis-
tication in understanding text meaning. The need for
such sophistication is all the more pregnant for social
sciences. On the one hand, qualitative social science
methods of text analysis do not scale to the enormous
datasets that are now available. Furthermore, quantita-
tive approaches allow for other types of analysis that are
enriching and complementary to qualitative research,
yet may simplify extensively the processing in such a way
that it hinders their adoption by scholars used to the re-
finement of qualitative approaches. And the more so-
phisticated the NLP techniques become, the further they
tend to be from being used for large-scale text analy-
sis purposes. Indeed, these systems are fast and accu-
rate enough to form a starting point for more advanced
computer-supported analysis in a CSS context, and they
enable approaches that are substantially more sophis-
ticated than the text mining state of the art discussed
above. Yet, the results of such systems may seem rela-
tively simplistic compared to human-level understand-
ing of natural language.
The literature already features some works which at-
tempt at going beyond language models based on word
distributions (such as bags of words, co-occurrence clus-
ters, or so-called “topics”) or triplets. For instance, State-
ment Map [49] is aimed at mining the various viewpoints
4
expressed around a topic of interest in the web. Here
a notion of claim is employed. A statement provided
by the user is compared against statements from a cor-
pus of text extracted from various web sources. Text
alignment techniques are used to match statements that
are likely to refer to the same issue. A machine learn-
ing model trained over NLP-annotated chunks of text
classifies pairs of claims as “agreement”, “conflict”, “con-
finement” and “evidence”. More broadly, the subfield
of argumentation mining [38] also makes extensive use
of machine learning and statistical methods to extract
portions of text corresponding to claims, arguments and
premises. These approaches generally rely on surface
linguistic features, there is however an increasing trend
of dealing with structured and relational data. Already in
2008, [73] proposed a system to extract binary semantic
relationships from Dutch newspaper articles. A recent
work [59] presents a system aimed at analysing claims in
the context of climate negotiations. It leverages depen-
dency parse trees and general ontologies [70] to extract
tuples of the form: 〈actor, predicate, negotiation_point〉
where the actors are stakeholders (e.g., countries), the
predicates express agreement, opposition or neutrality
and the negotiation point is identified by chunk of text.
Similarly, in another recent work [74], parse trees are
used to automatically extract source-subject-predicate
clauses in the context of news reporting over the 2008-
2009 Gaza war, and used to show differences in citation
and framing patterns between U.S. and Chinese sources.
These works help demonstrate the feasibility of using
parse trees and other modern NLP techniques to iden-
tify viewpoints and extract more structured claims from
text. Being a step forward from pure bag-of-words analy-
sis, they still leave out a considerable amount of informa-
tion contained in natural language texts, namely by rely-
ing on topic detection, or on pre-defined categories, or
on working purely on source-subject-predicate clauses.
We propose to introduce a more sophisticated language
model, where all entities participating in a statement are
identified, where entities can be described as combina-
tions of other entities, and where statements can be enti-
ties themselves, allowing for claims about claims, or even
claims about claims about claims. The formal backbone
of this model consists of an extended type of hypergraph
that is both recursive and directed, thus generalizing se-
mantic graphs and inducing powerful representation ca-
pabilities.
3 Semantic hypergraphs – structure
and syntax
3.1 Structure
The SH model is essentially a recursive, ordered hyper-
graph that makes the structure contained in natural lan-
guage (NL) explicit. On one hand, NL is recursive, al-
lowing for concepts constructed from other concepts as
well as statements about statements, and on the other
hand, it can express n-ary relationships. We will see how
a hypergraphic formalism provides a satisfactory struc-
ture for NL constructs.
While a graph G = (V, E ) is based on a vertex set V
and an edge set E ⊂ V × V describing dyadic connec-
tions, a hypergraph [8, 9] generalizes such structure by
allowing n-ary connections. In other words, it can be de-
fined as H = (V, E ), where V is again a vertex set yet E
is a set of hyperedges (ei )i ∈1..M connecting an arbitrary
number of vertices. Formally, ei = {v1, ...vn } ∈ E = P (V ).
We further generalize hypergraphs in two ways: hyper-
edges may be ordered [23] and recursive [32]. Ordering
entails that the position in which a vertex participates
in the hyperedge is relevant (as is the case with directed
graphs). Recursivity means that hyperedges can partici-
pate as vertices in other hyperedges. The corresponding
hypergraph may be defined as H = (V, E ) where E ⊂ EV
the recursive set of all possible hyperedges generated
by V : EV = {(ei )i ∈{1..n} | n ∈ N, ∀i ∈ {1..n}, ei ∈ V ∪ EV
}. In
this sense, V configures a set of irreducible hyperedges
of size one i.e., atomic hyperedges which we also de-
note as atoms, similarly to semantic graphs. From here
on, we simply call these recursive ordered hyperedges as
“hyperedges”, or just “edges”, and we denote the corre-
sponding hypergraph as a “semantic hypergraph”.
Let us consider a simple example, based on a set V
made of four atoms: the noun “(berlin)”, the verb “(is)”,
the adverb “(very)” and the adjective “(nice)”. They may
act as building blocks for both hyperedges “(is berlin
nice)” and “(very nice)”. These structures can further be
nested: the hyperedge “(is berlin (very nice))” represents
the sentence “Berlin is very nice”. It illustrates a basic
form of recursivity.
3.2 Syntax
In a general sense, the hyperedge is the fundamental uni-
fying construct that carries information within the SH
formalism. We further introduce the notion of hyper-
edge types, which simply describe the type of construct
that some hyperedge represents: for instance, concepts,
predicates or relationships, as in the above examples —
respectively (berlin), (is) and (is berlin nice). We exten-
sively detail hyperedge types and their role in the next
subsections. For now, it is enough to know that predi-
cates, in particular and for instance, belong to a larger
family of types that are crucial for the construction of hy-
peredges and that we call connectors. In this regard, se-
mantic hypergraphs rely on a syntactic rule that is both
simple and universal: the first element in a non-atomic
hyperedge must be a connector.
In effect, a hyperedge represents information by com-
bining other (inner) hyperedges that represent informa-
tion. The purpose of the connector is to specify in which
sense inner hyperedges are connected. Naturally, it can
5
be followed by one or more hyperedges which play the
role of arguments with respect to the connector. As hy-
peredges, if they are not atoms, they must also start with
a connector themselves, in a recursive fashion.
We illustrate this on the hyperedge (is berlin (very
nice)): here, (is) is a predicate playing the role of con-
nector while (berlin) and (very nice) are arguments of the
initial hyperedge. (berlin) is an atomic hyperedge, while
(very nice) is a hyperedge made of two elements: the con-
nector, (very), an atomic hyperedge, and an argument,
(nice), also an atomic hyperedge. Both cannot be decom-
posed further.
Readers who are familiar with Lisp will likely
have noticed that hyperedges are isomorphic to S-
expressions [42]. This is not purely accidental. Lisp
is very close to λ-calculus, a formal and minimalist
model of computation based on function abstraction
and application. The first item of an S-expression
specifies a function, the following ones its arguments.
One can think of a function as an association between
objects. Albeit hyperedges do not specify computations,
connectors are similar to functions at a very abstract
level, in that they define associations. The concepts of
“race to space” and “race in space” are both associated to
the concepts “race” and “space”, but the combination of
these two concepts yields different meaning by applica-
tion of either the connector “in” or “to”. For this reason,
λ-calculus has also been applied to dependency parse
trees in the realm of question-answering systems [56].
3.3 Types
We now describe a type system that further clarifies the
role each entity plays in a hyperedge. In all, we distin-
guish 8 types, the smallest set we could find that appears
to cover virtually all possible information representation
roles cataloged in the Universal Dependencies. We first
present the types that atoms may have and discuss their
use in constructing higher-order entities. We then show
how hyperedge types are recursively inferable from the
types of the connector and subsequent arguments.
Atomic concepts. The first, simplest and most funda-
mental role that atoms can play is that of a concept. This
corresponds to concepts that can be expressed as a sin-
gle word in the target language, for example “apple”; they
are labeled by this human-readable string, as could be
guessed from the previous subsection.
This defines an eponymous type, “concept”. The
nomenclature we propose further indicates the type of
an atom by appending a more machine-oriented code
after this label and a slash (/). For concepts, this code
is “C”:
(apple/C)
As we shall see, these machine-oriented codes remove
ambiguity, facilitate automatic inference and computa-
tions. The full list of types as well as their codes and pur-
poses can be seen in table 1.
Connectors The second and last role that atoms can
play is the role of connector. We then have five types of
connectors, each one with a specific function that relates
to the construction of specific types of hyperedges.
The most straightforward connector is the predicate,
whose code is “P”. It is used to define relations, which are
frequently statements. Let us revisit a previous example
with types:
(is/P berlin/C nice/C)
The predicate (is/P) both establishes that this hyperedge
is a relation between the entities following it, and gives
meaning to the relation. This is isomorphic to typical
knowledge graphs [3, 66] where (berlin) and (nice) would
be connected by an edge labeled with (is).
The modifier type (“M”) applies to one (and only one)
existing hyperedge and defines a new hyperedge of the
same type. In practice, as the name indicates, it modi-
fies things and can be applied to concepts, predicates or
other modifiers, and also to triggers, a type that we will
subsequently address. For concepts, a typical case is ad-
jectivation, e.g.:
(nice/M shoes/C)
Note here that “nice” is being considered as a modifier,
while “nice” was a concept in the previous case: this is
due to the fact that (nice/M) and (nice/C) refer to two
distinct atoms which share the same human-readable la-
bel, “nice”. To illustrate modification of predicates, let us
revisit a previous example, but suppose that we declare
that Berlin is not nice. Then we can apply a modifier to
the predicate, such as (not/M), so that:
((not/M is/P) berlin/C nice/C)
Finally, modifiers may modify other modifiers:
((very/M nice/M) shoes/C)
The builder type (“B”) combines several concepts to cre-
ate a new one. For example, atomic concepts (capital/C)
and (germany/C) can be combined with the builder atom
(of/B) to produce the concept of “capital of Germany”:
(of/B capital/C germany/C)
A very common structure in English and many other lan-
guages is that of the compound noun e.g., “guitar player”
or “Barack Obama”. To represent these cases, we intro-
duce a special builder atom that we call (+/B). Unlike
what we have seen so far, this is an atom that does not
correspond to any word, but indicates that a concept is
6
Code Type Purpose Example Atom Non-atom
C concept Define atomic concepts apple/C × ×
P predicate Build relations (is/P berlin/C nice/C) × ×
M modifier Modify a concept, predicate, modifier,
trigger
(red/M shoes/C) × ×
B builder Build concepts from concepts (of/B capital/C germany/C) ×
T trigger Build specifications (in/T 1994/C) ×
J conjunction Define sequences of concepts or rela-
tions
(and/J meat/C potatoes/C) ×
R relation Express facts, statements, questions, or-
ders,...
(is/P berlin/C nice/C) ×
S specifier Relation specification (e.g. condition,
time,...)
(in/T 1976/C) ×
Table 1: Hyperedge types with use purposes and examples. Connector types are emphasized with a gray background.
The rightmost columns specify whether this type may be encountered in atomic or non-atomic hyperedges.
formed by the compound of its arguments; it is neces-
sary to render such compound structures. The previous
examples can be represented respectively as (+/B gui-
tar/C player/C) and (+/B barack/C obama/C).
Conjunctions (“J”), like the English grammatical con-
struct of the same name, join or coordinate concepts or
relations:
(and/J meat/C potatoes/C)
(but/J (likes/P mary/C meat/C) (hates/P potatoes/C))
We also introduce a special conjunction symbol, (:/J),
to denote implicit sequences of related concepts. For
example, the phrase: “Freud, the famous psychiatrist”,
would be represented as:
(:/J freud/C (the/M (famous/M psychiatrist/C)))
The remaining case, triggers (T), concerns additional
specifications of a relationship, for example conditional
(“We go if it rains.”), or temporal (“John and Mary trav-
eled to the North Pole in 2015”), local (“Pablo opened a
bar in Spain”), etc.:
(opened/P pablo/C (a/M bar/C) (in/T spain/C))
Hyperedge type inference. Atomic types are entirely
covered by these six types, of which three exclusively
concern atoms (builders, triggers and conjunctions). We
already hinted at the fact that non-atomic hyperedges
also have types. These are implicit and inferable from the
types of the connector and its arguments. Given, for ex-
ample, that (germany/C) is an atom of type concept (C),
the hyperedge (of/B capital/C germany/C) is also a con-
cept, and this can be inferred from the fact that its con-
nector is of type builder (B). Builders need to be followed
by at least two concepts. Modifiers (M) only accept one
argument, and the hyperedge in which they participate
has the type of the single argument of the modifier. For
example, the hyperedge (northern/M germany/C) is a con-
cept (C), and (not/M is/P) is a predicate (P).
Table 2 lists all type inference rules and their re-
spective requirements. They also induce syntactic con-
straints which close the SH type system.
We may now introduce the two last types of our type
system, relation (R) and specifier (S), which only concern
non-atomic hyperedges: they are always defined as the
result of a composition of hyperedges. Relations are typ-
ically used to state some fact (even though they can also
be used to represent questions, orders and other things).
(is/P Berlin/C nice/C) is an obvious example of relation.
In our context, they thus turn out to be a crucial hyper-
edge type. Specifiers are types that play a more peripheral
role, in the proper sense, in that they are supplemental
to relations. Specifiers are produced by triggers. For ex-
ample, the trigger “(in/T)” can be used to construct the
specification: (in/T 1976/C). Specifications, as the name
implies, add precisions to relations e.g., when, where,
why or in which case something happened.
3.4 Argument roles
We introduce a last notion that we employ to make
meaning more explicit: argument roles for builders and
predicates. They are represented as sequences of char-
acters that indicate the role of the respective arguments
following such connectors.
Concept builders. Given a concept hyperedge, a key
issue is that of inferring its main concept, i.e. the con-
cept that can be assumed to be its hypernym. Beyond
the simple case of atoms, concept hyperedges may only
be formed by connectors that are either modifiers or
7
Element types → Resulting type
(M x) x
(B C C+) C
(T [CR]) S
(P [CRS]+) R
(J x y’+) x
Table 2: Type inference rules. We adopt the notation
of regular expressions: the symbol + is used to de-
note one or more entities with the type that precedes it,
while square brackets indicate several possibilities (for
instance, [CR]+ means “at least one of any of both C or
R” types). x means any type: (M x) is of type x.
builders. When the connector is a modifier, finding the
hypernym is admittedly trivial. When the connector is
a builder, it is often possible to infer the main concept
among the arguments. There are only two possible roles:
“main” (denoted by m) and “auxiliary” (denoted by a).
For example:
(+/B.am tennis/C ball/C)
The argument role annotation “.am” indicates that ball/c
is the main concept in the construct, meaning that
(+/B.am tennis/C ball/C) is a sort of ball/c — the main
concept is a hypernym of the whole construct.
With compound nouns ((+/B) builder), we simply
make use of part-of-speech and dependency labels to in-
fer the main concept. Another common situation where
finding roles is quite trivial is the case of builders de-
rived from a proposition, such as (of/B), which express
a relationship between the arguments. For example, in
(of/B.ma capital/C germany/C), the main concept is (cap-
ital/C). “Capital of Germany” is thus a type of capital. In
English and many other languages, it is always the case
that the main concept is the first argument after a builder
derived from a proposition.
Predicates. Predicates can induce specific roles that
the following arguments play in a relation. The need for
argument roles in relations arises from cases where the
role cannot be inferred from the type of the argument.
For example, the same concept could participate in a re-
lation as a subject or as an object. Consider for instance
the sentence “John gave Mary a flower”, represented as:
(gave/P.sio john/C mary/C (a/M flower/C))
In this relation, the argument role string “sio” indicates
that the three arguments following the predicate respec-
tively play the roles of subject, indirect object and direct
object. This relation involves three concepts united by
the predicate that represents the act of giving, but with-
out the argument roles, who the giver is, who the receiver
is, and what object is being given, would remain unde-
fined. Relying on ordering would not be enough, both
Role Code
active subject s
passive subject p
agent (passive) a
subject complement c
direct object o
indirect object i
parataxis t
interjection j
specification x
relative relation r
Table 3: Predicate argument roles.
due to the flexibility of NL in this regard, and to the fact
that the presence of a certain role after a predicate is of-
ten optional.
There are admittedly more possible roles than for
builders. They are shown in table 3. Once again, this set
is the result of an effort to cover all grammatical cases
listed in the Universal Dependencies in the most suc-
cinct way possible. Most of them (in fact, the first 8 in the
table) directly correspond to generic grammatical roles
of the same name. Of these, the first 6 are by far the
most frequent. Specifications were already discussed in
the previous subsection (3.3), and their purpose as hy-
peredges coincides with their role when participating in
relations: as an additional specification to the relation
(temporal, conditional, etc.). Finally, a relative relation
is a nested relation, that acts as a building block of the
outer relation that contains it. We will make extensive
use of this later, to identify what is being claimed by a
given actor.
4 Translating NL into SH
We now discuss the crucial task of translating NL into
this SH representation. This can, of course, be framed
as a conventional supervised ML task. A difficulty arises
from the lack of training data. SH is a novel repre-
sentation, and the effort necessary to annotate a suffi-
ciently large amount of text to train an NL to SH transla-
tor from scratch is far from trivial. We were motivated
to look for an alternative, and we hypothesized that it
would be much easier to infer the SH representation
from grammatically-enriched representations than from
raw text. We will show that this indeed appears to be the
case.
We propose a two-staged approach. The first (α-stage)
is a classifier that assigns a type to each token in a given
sentence. The second (β-stage) is a search tree-based al-
gorithm that recursively applies the rules in table 2 to
impose the hypergraphic structure on the sequence of
atoms produced by the α-stage. This restricts the ML
8
part of the process to the α-stage, making it a trivial clas-
sification problem.
4.1 α-stage
The classification categories correspond to the set of the
six atomic types shown in table 1, with one additional
category for tokens that should be discarded (typically
punctuation). The open question is the feature set. We
will see how, operating on the previous assumption re-
garding grammatical annotation, we use spaCy1 – a pop-
ular NLP tool – to generate appropriate features.
Using this library we perform segmentation of text
into sentences, followed by tokenization and annota-
tion of tokens with parts-of-speech, dependency labels
and named entity categories. In short, we deploy the
full arsenal of off-the-shelf NLP tasks that come avail-
able with spaCy. In this work we restrict ourselves to the
English language and we use the “en_core_web_lg-2.0.0”
language model.
We collected randomly selected texts in English from
five categories: fiction (5 books, 87738 sentences) and
non-fiction books (5 books, 51597 sentences), news (10
articles, 532 sentences), scientific articles (10 articles,
3467 sentences) and Wikipedia articles (10 articles, 2888
sentences). From these we selected 60 random sen-
tences in each category, thus a total of 300 sentences rep-
resenting 6936 tokens. An interactive computer script
was used to aid in the process of manually annotating
each word of these sentences with one of the alpha cate-
gories i.e., atomic types. These were used to train a ran-
dom forest classifier. For this purpose we employed the
one included with scikit-learn (version 0.23.2), a widely
used ML package. We did not perform any hyperparam-
eter tuning, and used the default parameters set by this
version of the package. There is possibly room for im-
provement here. For the aims of this work, we found it
preferable to avoid introducing potentially confounding
factors that could arise from hyperparameter optimiza-
tion efforts.
Feature definition. We consider an initial set that en-
compasses all the potentially useful features that we
could derive from a standard NLP pipeline such as spaCy.
As we mentioned, it provides dependency parse labels
(referred to, from now on, as DEP) and named entity
recognition categories (NER). Parts-of-speech are pro-
vided in two flavors: the more extensive OntoNotes tag
set (version 5) from the Penn Treebank, and the sim-
pler Universal Dependencies (UD) part-of-speech tag set
(version 2). Accuracy values for each of these elements
1An open-source library for NLP in Python which includes convo-
lutional neural network models for tagging, parsing and named entity
recognition in multiple languages. A relatively recent comparison of
ten popular syntactic parsers found spaCy to be the fastest, with an ac-
curacy within 1% of the best one [18]
are reported in [67] to be 0.97 for the fine grained part-of-
speech tagger (i.e., guessing the OntoNotes tag), 0.92 for
unlabeled dependencies (i.e., guessing the head of each
token) and 0.90 for labeled dependencies (i.e., the head
and the label).
Let us refer to the former as TAG, and to the latter as
POS. We can also consider the most common words in
the corpus. We consider as features the sets of 15, 25, 50
and 100 most common words (WORD15, WORD25 and
so on). Further features indicate if a token corresponds
to some punctuation symbol, if it is at the root of the de-
pendency parse trees, if it has left or right children in this
same tree, and finally its shape in terms of capitalization
(e.g. the shape of the word “Alice” is Xxxxx). Then, we
establish three types of relative tokens: the ones that ap-
pear directly after or before the current one in the sen-
tence, if they exist, and the one that is the parent of the
current one in the dependency parse tree, if it exists.
For each one of these tokens, all the previous features
are also applied (for example, the UD part-of-speech of
the dependency head is HPOS, and the part-of-speech of
the subsequent word in the sentence is POS_AFTER). We
thus have 33 candidate features in total. All of these fea-
tures are categorical, and we employ one-hot encoding
to feed them to the decision trees.
Feature selection. We tested two approaches for fea-
ture selection: a very simple genetic algorithm (GA) and
iterative ablation. For the GA, we encoded features as
bits (acting as switches to specify which features be-
long to the set). We used mutation only (bit-flip with a
probability of .05), a population of 100, and parent se-
lection through a tournament of 3. Search stopped at
100 generations without improvement. The fitness func-
tion was the mean of 5 evaluations of the accuracy of
the feature set, each with a distinct and randomly se-
lected split of the training / testing data. This even-
tually resulted in a set of 15 features: {WORD25, TAG,
DEP, HWORD25, HWORD50, HWORD100, HPOS, HDEP,
IS_ROOT , NER, WORD_BEFORE15, WORD_BEFORE100,
WORD_AFTER15, PUNCT_BEFORE, POS_AFTER}.
The iterative ablation procedure starts with the set of
all candidate features, and 100 runs of the learning algo-
rithm are performed, again each run randomly split into
two-thirds for training and one-third for testing. This
provides us with a set of 100 accuracy measurements.
The process is then repeated, excluding one feature at at
time. The feature that most degrades mean accuracy is
excluded. If no feature has a negative impact on accu-
racy, then the one with the highest p-value (according to
the non-parametric Kolmogorov–Smirnov test) above a
threshold is excluded. The procedure is repeated, ablat-
ing one feature at a time, until no remaining feature ful-
fills any of the previous two criteria. We performed this
procedure with threshold p-values of .05 and .005. The
first left us with a set of five features: F5 = {TAG, DEP,
9
HDEP, HPOS, POS_AFTER}; the second with three fea-
tures: F3 = {TAG, DEP, HDEP}.
The results of these experiments are shown on the left
side of figure 1. As can be seen, all of the three attempts
outperform the set of all features. Interestingly, F5 is sig-
nificantly better than F3, even at p < .005. The accu-
racy of the GA set falls between that of F3 and F5. We
performed these experiments not only as an endeavor to
achieve acceptable accuracy for the experiments that fol-
low, but also to obtain empirical evidence regarding the
relationship between SH types and traditional linguistic
features. We can conclude that SH does not correspond
to some trivial mapping of any single linguistic feature.
For subsequent experiments we will use F5, given that
it has the best accuracy and still uses a relatively small
number of features – something that can make a differ-
ence regarding the computational effort needed to parse
large quantities of text. It is interesting to notice that F3
still leads to a higher accuracy than the set of all features,
and having only three features, such a classifier could
be feasibly implemented in a purely programmatic way.
A completely human-understandable classification tree
could be produced, and also implemented in a very effi-
cient way, sacrificing relatively little in terms of accuracy.
On the right side of figure 1 we present the accuracy of
the classifier by text category, using F5. Here, it is inter-
esting to note that the best performing category (fiction)
and also one of the second-best (wikipedia, which is not
significantly different from news) are out-of-corpus for
the training set of the ML model of the underlying lin-
guistic features. It is remarkable that the accuracies that
we achieve are comparable and may even surpass the
values reported by spaCy (see above). In other words,
this suggests that, far from accumulating errors down the
stream of the various processing steps, our α stage ap-
pears to even correct upstream errors.
It is conceivable that more features become relevant,
if a larger number of exotic cases becomes available
through larger training corpora. It is also conceivable
that larger windows (beyond just previous and next to-
ken) become relevant with larger datasets and more so-
phisticated ML approaches. Such considerations are be-
yond the scope of this work.
4.2 β-stage
The β-stage transforms the sequence of atoms of the
original sentence, each typed by the α-stage, into a se-
mantic hyperedge that reflects the meaning of the sen-
tence and respects the SH syntactic rules. In practice,
this operation amounts to a bottom-up process that ag-
gregates the deeper structures of the sentence into in-
creasingly complex hyperedges, by recursively combin-
ing them until only a final, well-formed semantic hyper-
edge is left.
The process for this transformation is formalized in
algorithm 1. Let us nonetheless explain in plain words
Function ApplyPattern(seq, pos, pat)
Data: A sequence of edges seq, a position in the
sequence pos and a pattern pat
Result: A sequence of edges with the initial edges
replaced by a single one, if they match the
pattern.
if pat matches seq at pos then
ed g e ←− reorder matching elements of seq to
align with pat
seq′ ←− matching part of seq replaced with
ed g e
return seq′
else
return ∅
end if
end
Function BetaTransformation(seq)
Data: A sequence of edges seq
Result: An edge e
if |seq| = 1 then
return seq[0]
end if
heubest ←− −∞
seqbest ←− ∅
for pos = 1 to |seq| do
for pat ∈ P at t er ns do
seq′ ←− ApplyPattern(seq, pos, pat)
heu ←− h(seq, pos, pat )
if seq′ 6 = ∅ ∧ heu > heubest then
heubest ←− heu
seqbest ←− seq′
end if
end for
end for
if seqbest 6 = ∅ then
return BetaTransformation(seqbest )
else
return ((:/J) ++ seq[: 2] ) ++ seq[2 :]
end if
end
Algorithm 1: The β transformation recursively ap-
plies the patterns from type inference rules until
only the final hyperedge is left.
how β iteratively constructs a hyperedge, which need not
be a proper semantic hyperedge except at the final step.
The process starts indeed with an initial hyperedge as the
simple sequence of typed atoms of the original sentence.
At each step, the elements of the currently-formed hy-
peredge are scanned from left to right to look for a sub-
sequence of types that matches the list on the left side
of the type inference rules of table 2, taken as unordered
patterns i.e., up to any reordering. For instance, “capi-
tal of Germany” may have been parsed by α as a typed
sub-sequence “capital/C, of/B, germany/C”, which then
matches the second pattern (B C C). It may then be rear-
ranged as such by putting the connector in first position
10
Figure 1: Left: accuracy of the α-classifier, comparing several feature sets; all includes all features, GA a features set
obtained with a genetic algorithm, F3 is the outcome of iterative ablation with p < .005 and F5 with p < .05. Right:
accuracy by source text category using F5.
and preserving the order of the remainder of the hyper-
edge i.e., “(of/B capital/C germany/C)”, which conforms
to the second inference rule of table 2. Note that, in prac-
tice, we also restrict the second and fifth patterns, i.e.
the builder and conjunction patterns, to the minimum
number of two arguments: respectively (B C C) and (J x
x′). We find that it fits NL more naturally and thus leads
to more correct parses. Further tasks of knowledge in-
ference might later introduce builder- and conjunction-
based structures with more arguments. We complement
the patterns with one rule that corresponds to the special
connector (+/B). This extra rule is admittedly needed to
transform implicit builders (C C) into (+/B C C).
If only one sub-sequence matches, it is transformed
into a sub-hyperedge by application of the rule. If two or
more sub-sequences match, the β-stage needs to make
a decision on which one to choose and proceed with as
if only one sub-sequence matched. For this case, we use
a heuristic function (this is function h in algorithm 1).
This heuristic function relies on the grammatical struc-
ture of the sentence given by the dependency tree. Our
hypothesis is that grammatically connected edges are
more likely to belong to the same higher-order edge, so
the first criterion of h is to always assign a higher score
to sub-sequences where all items are directly connected
in the dependency tree. By “directly connected in the
dependency tree”, we mean that all hyperedges contain
one atom/token that is the head or the child of at least
one atom/token in another hyperedge, and that any hy-
peredge can be reached from any other, following such
grammatical links. In case there is a tie, the heuristic
function then prefers the sub-sequence that contains the
deepest atom/token in the dependency tree – again as-
suming a correlation with SH depth, and thus respecting
the bottom-up process of the β-transformation. Finally,
if there is still a tie, rules are applied by the order of pri-
ority expressed in table 2, which is empirically organized
by decreasing order of the depth at which each respec-
tive structure tends to appear in hyperedges. The special
rule for (+/B) is assigned the highest priority.
If no sub-sequence matches, the two first items in the
sequence are connected by prepending the special con-
junction (:/J), which is meant to convey the most generic
and abstract meaning of “these two things are related in
the most generic sense”. This captures cases often found
in natural language, such as: “A new era: quantum com-
putation is here.”, which translates to:
(:/J (a/M (new/M era/C)) (is/P (quantum/M
computation/C) here/C))
If the resulting hyperedge entirely conforms to one of
the type inference rules, the process stops successfully as
it managed to form a recursively correct semantic hyper-
edge. Otherwise, the process is reiterated on the newly-
formed hyperedge. The process is thus guaranteed to
converge on a syntactically valid hyperedge, but is of
course not guaranteed to produce the most desirable or
correct representation. However, we experimentally ver-
ify below that, given a correct classification from the α-
stage and a correct dependency parse tree, this process
consistently leads to the construction of a SH that cor-
rectly conveys the meaning of the original sentence.
Let us first illustrate the β-stage in figure 2, which pro-
vides one example of an entire parsing process (using
the F3 feature set for simplicity). In figure 2(c), the re-
cursive application of β-transformations to an initial se-
quence of atoms can be followed. In the first step, we
can see that the sequence (the/M, capital/C) matches the
11
Figure 2: (a) Dependency parse tree with dependency labels (green) and fine grained part-of-speech tags (red). (b)
α-stage classification of atom types. (c) β-stage structuring of sentence by iterative application of the patterns from
table 2. A non-selected pattern is greyed-out.
pattern (M C), and the sequence (capital/C, of/B, ger-
many/C) matches the pattern (B C C+). We thus rely on
the above-mentioned heuristic function, which causes
(of/B capital/C germany/C) to be preferred to (the/M cap-
ital/C). The reader can verify that selecting the latter at
this stage would lead to a dead-end. The rest of the SH
construction is straightforward.
Argument roles. Now that the core of the translation of
NL into SH has been specified, assigning the argument
roles introduced in Section 3.4 amounts to a trivial trans-
lation from the dependency labels. Sometimes however,
the parser may fail to determine an argument’s role, and
thus classify it as unknown (that we code “?” for this pur-
pose).
4.3 Validation of α and β
To test the accuracy of the complete translation from
NL to SH, we randomly selected 100 new sentences for
each text category, that were used neither for training
nor testing of the α-classifier. We establish three cate-
gories: completely correct hyperedges, hyperedges with
some defect and completely wrong hyperedges. A hyper-
edge is considered to have a defect if overall meaning is
preserved, but some subedge contains a defect. Let us
consider a real example from our dataset. The sentence:
“The scientists – who are part of a multi-year Interna-
tional Shelf Study Expedition – stressed their findings are
preliminary.” was parsed as:
(stressed/P (:/J (the/M scientists/C) (are/P who/C (of/B
part/C (a/M (+/B (+/B (+/B multi/C -/C) year/C)
(+/B international/C (+/B shelf/C (+/B study/C
expedition/C)))))))) (are/P (their/M findings/C)
preliminary/C))
The hyperedge preserves most of the meaning of the
sentence, but the concept (+/B (+/B multi/C -/C)
year/C) is not correctly formed. Either (-/B multi/C
year/C) or (multi/M year/C) would be much preferable.
However, this partially defective parse is still likely to be
useful in the methods that we will discuss in the follow-
ing sections. We also see how different type assignments
of the α-classifier can result, in practice, in correct hyper-
edges at the end. We can also use this example to illus-
trate another metric that we employ in this evaluation:
the relative defect size. This is simple the ratio of the size
of the defective part to the size of the entire hyperedge.
Size is measured in total number of atoms (at any depth).
A wrong hyperedge is one where the meaning of the
sentence is completely lost. For example, consider what
would happen if, in the above case, “stressed” was classi-
fied as a concept instead of a predicate. This also serves
to illustrate that there is a complex relationship between
α-classifier accuracy and overall parser accuracy. Some
mis-classifications at the α-stage can still allow for a
completely correct parse, while others can lead to catas-
trophic failures or just minor defects. Nonetheless, we
observe on this sample of 500 sentences that a correct
α classification and dependency parse tree always lead
to the construction of an SH that preserves the meaning
of the sentence. By contrast, a badly-structured depen-
dency tree appears to have a significant negative impact
on the functioning of β, through the heuristic function.
If this result generalizes, this suggests that, for a given
accuracy of the dependency parsing module, increasing
the quality of the NL to SH translation principally relies
on improving α and the heuristic function.
We show the results of this evaluation in table 4. It
is interesting to notice that “non-fiction” is one of the
worst performing categories in the α-classifier, but ends
up being the best one overall. Likewise, “fiction” is the
best category at α-stage but ends up being the second
worst here. Unsurprisingly, “fiction” sentences tend to
be richer in figures of speech and other complexities and
ambiguities that lead to a higher rate of catastrophic fail-
ure. Conversely, “non-fiction” is the category with the
most straight-forward sentences. In the “science” cate-
12
Category Correct Defect Wrong Total Mean relative defect size
Non-fiction 87 (.87) 8 (.08) 5 (.05) 100 .188
Wikipedia 81 (.81) 12 (.12) 7 (.07) 100 .190
News 77 (.77) 16 (.16) 7 (.07) 100 .147
Fiction 79 (.79) 5 (.05) 16 (.16) 100 .140
Science 71 (.71) 19 (.19) 10 (.10) 100 .290
All 395 (.79) 60 (.12) 45 (.09) 500 .206
Table 4: Global NL to SH parser evaluation.
gory, the difficulties are more related to a variety of un-
usual technical terms and notations, that lead more to
defects than catastrophic failures. Overall, we see that
a high percentage of the texts are correctly translated to
SH, even in the worst-performing categories.
We only work with English in this article, but support-
ing a new language essentially requires to generate a rel-
atively small number of α-classifier training examples.
The rest of the process is currently language-agnostic:
even though more research would be needed to explore
this issue thoroughly, the fact that we cover all of the Uni-
versal Dependency cases gives us good reasons to be-
lieve that NL to SH translation shall be applicable to any
language. The software package that we released to im-
plement all the ideas discussed in this article includes
the interactive script that we used to perform this anno-
tation task ourselves.
5 Knowledge Inference and Extrac-
tion
We are finally in the position to explore the use of SH ex-
tracted from open text to perform language understand-
ing tasks. First we will discuss how we naturally gener-
alize SH to represent patterns and inference rules, and
then we will manually define three such rules to per-
form conjunction resolution: a very useful and generic
task that will be used in every following practical appli-
cation discussed in this work, and very likely useful for
myriad knowledge inference and extraction tasks. Then
we will discuss how we systematized the process of dis-
covery of useful patterns, and how we use this process to
discover 8 patterns for the purpose of Open Information
Extraction (OIE), for which an abundant computer sci-
ence literature exists where scholars are interested in in-
ferring relations from free text. We will demonstrate the
expressive power of SH by showing that these simple pat-
terns produce competitive results when compared with
a number of contemporary systems targeted at OIE, us-
ing an external benchmark.
5.1 A pattern-matching language
From a text corpus, the NL to SH translation stage at-
tempts to convert each sentence into a hyperedge. In
practice, all resulting hyperedges are stored in a proper
SH database. From there, language understanding tasks
may be performed in the form of inferences, which
we define using SH notation with the help of patterns.
Broadly, inference rules and patterns may also be writ-
ten as hyperedges.
Variables and patterns. We introduce the concept of
variable. A variable simply indicates a placeholder that
can match a hyperedge, and then be used to refer to that
hyperedge. Unlike the other atoms we have seen so far,
variables are represented in capital letters. With vari-
ables we can define patterns, that can then be matched
against other hyperedges. For example, consider the pat-
tern:
(is/P.sc SUBJ PROP/C)
which matches, for example:
(is/P.sc (the/M sky/C) blue/C)
Notice that the variable PROP includes a type code,
while the predicate “is/P.sc” features argument roles. If
type codes or argument roles are added to variables, this
simply means that a hyperedge only matches this vari-
able if the types and argument roles also match.
We introduce a few more notation details for argu-
ment roles in patterns. In practice, we often allow the
various pattern elements to appear in any order, denot-
ing the order-indifferent roles between curly brackets “{
}”. The arguments can appear in any order, as long as all
of the pattern roles are present. For instance,
(is/P.{sc} SUBJ PROP/C)
would both cover (is/P.sc SUBJ PROP/C) and (is/P.cs
PROP/C SUBJ). Furthermore, it is possible to specify the
optional presence of certain arguments by listing them
as “...”, which simply indicates that any number (includ-
ing zero) hyperedges may be present at that point. In a
pattern, if the connector indicates argument roles, then
any further arguments may be present, unless indicated
otherwise. In case connectors do not indicate argument
13
roles, “...” can thus be used to indicate that more hyper-
edges at a certain point are permissible. For instance,
(is/P.{sc} SUBJ PROP/C ...)
matches “The sky is blue today” and “Today the sky is
blue”. It is also possible to denote an undefined sequence
of hyperedges with a variable name by using “X...”, which
thus refers to the same specific sequence everywhere it
is used.
Sequences of alternative arguments roles of which
anyone of them can be matched once are represented in-
side square brackets. For example, “[sp]” matches either
a subject or a passive subject, once. Finally, it is possi-
ble to forbid the presence of arguments with a certain
role by listing them after “-”. For example, in the pat-
tern “(PRED/P.-sp X...)”, arguments with roles “s” or “p”
are not allowed; it would however match (play/P.o foot-
ball/C).
Rules. We may now define rules which we denote with
a couple of patterns separated by the symbol “`”, as
in “PATTERN1 ` PATTERN2”. This notation indicates
that any hyperedge that contains a hyperedge matching
the left-hand-side PATTERN1 would incur the creation
of a duplicated hyperedge consisting of the matching
portion rewritten according to the right-hand-side ex-
pressed as PATTERN2. In a sense, these are replacement
rules, except that the original hyperedge is preserved.
For example, consider the rule:
(is/P.sc SUBJ PROP/C) ` (property/P PROP)
which, applied to the above example, produces the infer-
ence:
(property/P blue/C)
In essence, a rule makes it possible to populate an SH
database with new knowledge that is inferred from NL
yet need not, in turn, correspond to an actual sentence.
5.2 Conjunction Decomposition
Decomposing relations that include conjunctions into
simpler relations not only facilitates OIE tasks, but is also
of general usefulness in knowledge inference tasks. We
show the three rules that we developed manually to per-
form conjunction decomposition in table 5.
The first rule concerns conjunctions of concepts, such
as “Mary likes books and flowers.”:
(likes/P.so mary/C (and/J books/C flowers/C))
where we generate one relation for each element:
(likes/P.so mary/C books/C)
(likes/P.so mary/C flowers/C)
The second rule concerns conjunctions of relations
with explicit subjects, for example: “Mary likes astron-
omy and Alice plays football.”, which is parsed as:
(and/J (likes/P.so mary/C astronomy/C) (plays/P.so
alice/C football/C))
is decomposed into: “Mary likes astronomy.” and “Alice
plays football.” i.e.,:
(likes/P.so mary/C astronomy/C)
(plays/P.so alice/C football/C)
The third rule makes the subject explicit in situations
such as “Mary likes astronomy and plays football.” i.e.,
(and/J (likes/P.so mary/C astronomy/C) (plays/P.o
football/C))
inferring that “Mary” is the subject from the first relation
in the conjunction and applying it to the second one:
“Mary plays football.”, resulting in:
(likes/P.so mary/C astronomy/C)
(plays/P.so mary/C football/C)
In practice, this is done by remembering the last argu-
ment with the subject (s) role and applying it to the fol-
lowing relations that miss a subject.
Naturally, these rules have a lot of space for improve-
ment, not making distinctions for conjunctions with
special meaning (e.g. “but”, “instead”, etc.). Neverthe-
less, we will see that they are already quite successful in
the tasks that we will subsequently present.
5.3 Pattern Learning
With the help of hypergraphs extracted from corpora of
open text, it becomes possible to define a systematic pro-
cess of discovery of patterns that enable knowledge ex-
traction, with a human-in-the-loop. On the left side of
figure 3 we present the general template for such a pro-
cess. We will use this template to illustrate both how we
discovered the patterns for the OpenIE task as well as
the claim and conflict analysis that will be presented in
section 5.4, and also how more sophisticated and auto-
mated pattern learning systems can be created. The rest
of figure 3 is a step-by-step illustration on a simple exam-
ple aimed at generating patterns to detect claims.
In step (1), a hyperedge is selected from the hyper-
graph generated from a given training corpus. It can be
drawn at random, or by any other criterion adapted to
the pattern-learning task at hand. For instance, if we
want to learn patterns typical of claims, we can first fo-
cus on hyperedges starting with a predicate (*/P) and,
more precisely, the most frequent ones among them,
as a strategy to attain good coverage. We observe that
“says/P” is such a predicate and based on this, we draw
(says/P.sr alice/C (are/P.sc dogs/C nice/C)) i.e., “Alice says
14
# Rule Inferences
1 (*/J ... CONCEPT/C ...) ` (CONCEPT/C) 147
2
(
*/J ... (PRED/P.{[sp]} X Y...) ...
)
` (PRED/P.{[sp]} X Y...) 63
3
(
*/J (*/P.{[sp]} SUBJ/* ...) ... (PRED/P.-sp X...) ...
)
` (PRED/P.{s} SUBJ/* X...) 10
Table 5: Conjunction resolution rules and respective number of inferred hyperedges from the OIE benchmark
.(1) Select hyperedge
(2) Human inference
(3) Pattern inference
(4) Search with pattern
(5) Human feedback ACTOR = (the/M president/C)
CLAIM = ((will/M recover/P.o) (the/M economy/C)) ✓
FIRST PASS SECOND PASS
(says/P.sr alice/C (are/P.sc dogs/C nice/C))
“Alice says dogs are nice.”
“Alice says dogs are nice.”
(says/P.sr alice/C (are/P.sc dogs/C nice/C))
ACTOR = alice/C CLAIM = (are/P.sc dogs/C nice/C)
ACTOR CLAIM
(*/P.{sr} ACTOR CLAIM)
(says/P.sr alice/C (are/P.sc dogs/C nice/C))
(wants/P.sr bob/C ((to/M play/P.o) chess/C))
“Bob wants to play chess.”
(*/P.{sr} ACTOR CLAIM)
ACTOR = bob/C
CLAIM = ((to/M play/P.o) chess/C)
Wrong match, pattern is too generic:
(says/P.{sr} ACTOR CLAIM)
Make pattern more specific:
(says/P.sr (the/M president/C)
((will/M recover/P.o) (the/M economy/C)))
(says/P.{sr} ACTOR CLAIM)
“The president says the economy will recover.”
Figure 3: Pattern learning template and example with two passes. At the end of the second pass, the pattern (says/P.sr
ACTOR CLAIM) is confirmed to work.
dogs are nice”. Other pattern-learning tasks may natu-
rally require different selection criteria, and in the next
subsection (5.4) we will provide another and more gen-
eral example.
A human is then presented with this hyperedge in step
(2), and asked to manually perform an inference. The
inference consists of selecting sub-edges and assigning
them to variables according to some schema. In the ex-
ample shown, the inference aim is to detect which actors
making claims, and thus identify which “ACTOR” makes
which “CLAIM”.
Step (3) consists of generalizing the original hyperedge
into a pattern with the help of the variable assignments.
The idea is to create the most generic pattern that fits
the human inference. The matching parts are replaced
by the corresponding variables, and the remaining sub-
edges are replaced by wildcards, while maintaining type
annotations.
Then the process goes back to the whole training hy-
pergraph. Step (4) consists of finding hyperedges that
match the pattern, so that they can be presented to the
human for validation. Then, in step (5), the human can
simply indicate if these further matches are valid or not.
When a match is not valid, the process can then return
to step (3) and use this information to refine the pattern.
What is now the most general version of the pattern that
does not match the previously detected incorrect case?
In our work, we used a conventional Jupyter notebook
15
directly accessing the programmatic interface of Graph-
brain2, the open-source library that we developed to im-
plement all of the ideas discussed in this work. Refine-
ments at step (3) were performed manually, testing hy-
pothesis on the most general version of a pattern by sim-
ply asking Graphbrain to check how many actual hyper-
edges match each attempt, and choosing the one with
the highest value. This process can obviously be au-
tomated with a search tree, that attempts a number of
substitutions – more or less generic wildcards, lemma
matching, atom root matching, structural matching, etc.
– at each step, and then uses the training hypergraph
to empirically test them and discover the most generic
one that correctly matches both the positive and neg-
ative cases known so far. Then, even more sophisti-
cated possibilities arise, such as the integration with gen-
eral knowledge databases (e.g. specifying that a variable
must be a concept of type “country”, or that a predicate
must be the synonym of a certain action), or the use
of auxiliary methods such as semantic proximity with
word2vec-like embeddings, or hybridization with ML al-
gorithms.
Another possible improvement is in the domain of
software and user-interface development, allowing for
less technical users to provide inferences and feedback –
a user can be invited to directly select parts of a sentence
and assigning them to meanings (e.g. “actor”, “claim”,
“aggressor”, etc.), without having to see or interact with
hypergraphic notation. Such refinements are beyond the
scope of this work, but it is our hope to lay the founda-
tions for these and other possibilities.
5.4 Open Information Extraction
We will now show how 5 simple hyperedge patterns are
sufficient to rank first in a recent Open Information Ex-
traction (OIE) benchmark [34]. In fact, one pattern is
even sufficient to surpass a majority of the systems of
that benchmark. We recognize the limitations of such
benchmarks and do not claim that we have the best per-
forming OIE system, neither are we singly focused on
this application. Instead, we are interested in providing
empirical evidence for the expressive power of SH pat-
terns for the general purpose of knowledge extraction.
To discover OIE patterns, we took advantage of the
Wikipedia part of the open text corpus that we developed
to train and validate the parser, and that we discussed in
section 4 – Wikipedia content is a naturally rich source of
factual assertions in NL. The resulting hypergraph con-
tains 62528 top-level hyperedges.
We then employed a simple process of generalization
to transform hyperedges into abstract patterns. It con-
sists in replacing each element of a hyperedge with its
corresponding type-annotated wildcard, for example:
(is/P.sc aragorn/C (of/B.ma king/C gondor/C))
2https://github.com/graphbrain/graphbrain
becomes:
(*/P.sc */C */C)
The process can continue recursively, further expand-
ing subedges, for instance:
(*/P.sc */C (*/B.ma */C */C))
These expansions have to conform to Table 2 (taken in
reverse order i.e., from a resulting type to its antecedent),
which generally leaves a small number of possibilities.
We further introduced some restrictions in the genera-
tion of such patterns, to focus on simple patterns with
likely relevance to our task. We limited recursive expan-
sion to depth 2, and only consider relations of sizes 3 or
4 – smaller ones cannot contain triplets, larger ones that
are useful are likely to contain the triplet (with optional
extension) within a core that generalizes to patterns with
no more than 4 elements. We excluded conjunctions
(these are previously decomposed, as explained in sub-
section 5.2), and modifiers. These latter connectors
could certainly be used to improve the OIE task, but en-
tail more semantic complexity, and we are more inter-
ested in simplicity at this stage. We will focus on mod-
ifiers in a subsequent section. Finally, we allow for the
special builder (+/B) to be explicitly included in the gen-
eralized patterns, given that compound concepts triv-
ially correspond to ontological relationships of OIE inter-
est, e.g.: “Film director David Lynch” implies that David
Lynch is a film director.
We considered the 50 most common such patterns,
which we present in the appendix, in table 14. Following
the process described in section 5.3 and the annotation
guidelines document provided with the benchmark [35],
we found that 36 of these patterns can be transformed
into valid OIE relationships, given correct parses.
We then compressed these 36 patterns into the most
general ones that: (a) imply one or more of the original
patterns, and (b) do not imply patterns found to be in-
correct in some way. For example, the two patterns:
(+/B.{ma} (ARG1/C...) (ARG2/C...))
(+/B.{mm} (ARG1/C...) (ARG2/C...))
are compressed to:
(+/B.{m[ma]} */C */C)
Such a compression/generalization process could fea-
sibly be algorithmically automated.
We thus arrived at the 5 patterns which are shown
in table 6. The extracted variables imply the usual OIE
tuples: 〈REL, ARG1, ARG2, ARG3...〉, with argument(s)
ARG3... being optional. Naturally, we convert hyper-
edges to the actual text they correspond to before feed-
ing them to the benchmark. In the absence of REL, the
relationship “is” is assumed. In some cases (patterns 3,
4, 5), notice also that REL is split into two or thee vari-
ables: REL1, REL2 and REL2. We just concatenate their
16
# Pattern Extractions F1 (cumulative) Rank
1 (REL/P.{[sp][cora]x} ARG1/C ARG2 ARG3...) 107 .265 4
2 (+/B.{m[ma]} (ARG1/C...) (ARG2/C...)) 38 .311 3
3 (REL1/P.{sx}-oc ARG1/C (REL2/T ARG2)) 20 .334 3
4 (REL1/P.{px} ARG1/C (REL2/T ARG2)) 12 .351 2
5 (REL1/P.{sc} ARG1/C (REL3/B REL2/C ARG2/C)) 16 .365 1
Table 6: Open Information Extraction patterns, ordered by decreasing contribution to F1 (presented cumulatively).
Ranks correspond to the rank achieved in the benchmark of Table 7 by using patterns up to the given line.
textual representation in the order indicated by the vari-
able names, with interleaving space characters.
Notice that the first pattern is almost a tautology of the
SH representation itself, producing triples where the first
argument is the active or passive subject, the relation is
the predicate, and the second argument is the direct or
indirect object, or complement, or agent, with the op-
tional argument being one of the specifications, if they
exist. To illustrate with a real and straightforward exam-
ple from the benchmark, consider the sentence: “The
population of the special wards is over 9 million people,
with the total population of the prefecture exceeding 13
million”. It is parsed to:
(is/P.scx (of/B.ma (the/M population/C) (the/M
(special/M wards/C))) ((over/M (9/M million/M))
people/C) (with/T (exceeding/P.so (of/B.ma (the/M
(total/M population/C)) (the/M prefecture/C)) (13/M
million/C))))
It matches pattern 1 with variables REL = is/P.scx;
ARG1 = (of/B.ma (the/M population/C) (the/M (special/M
wards/C))); ARG2 = ((over/M (9/M million/M)) people/C
and ARG3 = (with/T (exceeding/P.so (of/B.ma (the/M (to-
tal/M population/C)) (the/M prefecture/C)) (13/M mil-
lion/C))), resulting in the extraction: 〈the population of
the special wards, is, over 9 million people, with the total
population of the prefecture exceeding 13 million〉.
Pattern 2 can also be seen as a direct consequence of
SH representation, in this case inferring ontological rela-
tionship from the (+/B) builder structure. Cases where
both arguments have the role “m” can be interpreted as
two expressions of the same concept. Again, using a real
example, the emphasized part of the sentence “He is the
younger brother of the prolific film composer Christophe
Beck” was parsed as:
(+/B.mm (the/M (prolific/M (+/B.am film/C
composer/C))) (+/B.am christophe/C beck/C))
leading to the symmetrical extractions: 〈the prolific film
composer, is, Christophe Beck〉 and 〈Christophe Beck, is,
the prolific film composer〉. We discuss below in sec-
tion 6.4 how easy it is to further extract, from this point
and thanks to the recursive hypergraphic structure, the
relation 〈Christophe Beck, is, film composer〉; for now
however this is not needed for our comparison with the
OIE benchmark.
For a non-symmetrical example with (+/B.ma), let us
consider the sentence: “Finnish police reprimanded a
man for traveling in a car boot to hide his meeting with
Prime Minister Juha Sipila during a government crisis
last summer, saying this was breach of the traffic code”,
with the emphasized concept parsed as:
(+/B.am (+/B.am prime/C minister/C) (+/B.am juha/C
sipila/C))
Using the same pattern, this leads to the single extrac-
tion: 〈Juha Sipila, is, Prime Minister〉, while avoiding the
potentially excessive generalization: 〈Prime Minister, is,
Juha Sipila〉. We do know that Juha Sipila is one Prime
Minister, but not necessarily the only one in the context.
The restriction requiring non-atomic edges in the argu-
ments of this pattern is a simple mechanism to avoid too
trivial, and potentially silly inferences, such as “Barack is
Obama”.
As a final example, let us illustrate pattern 3 with the
sentence: “Gonzales graduated from Crescent School in
Toronto, Ontario, Canada”, parsed as:
(graduated/P.sx gonzales/C (from/T (in/B.ma (+/B.am
crescent/C school/C) (,/J toronto/C (,/J ontario/C
canada/C)))))
and resulting in extractions where the first part of
REL is extracted from the predicate and the second
from the trigger: 〈Gonzales, graduated from, Crescent
School in Toronto〉. The previously discussed conjunc-
tion decomposition process leads to the further extrac-
tions: 〈Gonzales, graduated from, Crescent School in
Ontario〉 and 〈Gonzales, graduated from, Crescent School
in Canada〉.
We stop illustrating how these patterns apply here for
the sake of succinctness, but hope to have sufficiently
shown how they are generic and straightforward manip-
ulations of the structures enabled by SH.
Table 7 shows the full benchmark, comparing the per-
formance of our approach with seven other methods.
SH outperformed all baseline systems in 24.6% of the
cases that tend to consist of complicated combinations
of conjunctions and prepositional phrases. For exam-
ple, the sentence where the next best system is defeated
17
System Extractions Matches Exact Prec. of Recall of Prec. Recall F1
matches matches matches
Semantic hypergraphs with 5 rules 201 120 19 .70 .93 .416 .326 .365
MinIE [26] 252 134 10 .75 .83 .400 .323 .358
ClausIE [20] 223 121 24 .74 .84 .401 .298 .342
OpenIE 4 [41] 101 74 5 .68 .84 .501 .182 .267
Semantic hypergraphs with 1 rule 107 74 7 .69 .85 .475 .184 .265
Ollie [40] 145 74 8 .73 .81 .347 .175 .239
ReVerb [25] 79 54 13 .83 .77 .569 .121 .200
Stanford [4] 371 99 2 .79 .65 .210 .188 .198
PropS [71] 184 69 0 .59 .80 .222 .162 .187
Table 7: Performance of OpenIE systems, ordered by descending F1. Bold figures indicate the best performing system
for each category.
by the highest margin is: “A very detailed treatment of
the EM method for exponential families was published
by Rolf Sundberg in his thesis and several papers fol-
lowing his collaboration with Per Martin-Löf and Anders
Martin-Löf.” Furthermore, the mean number of words
per sentence where SH is not the best is 21.2, vs. 23.8
where it is the best (31.0 when outperforming by a fac-
tor ≥ 1.5), plausibly indicating an advantage with more
complicated sentences.
6 Computations on the hypergraph:
concepts, ontologies and corefer-
ence resolution
We have shown how SH representation makes it possi-
ble to infer knowledge using simple symbolic rules. We
will now address how it enables knowledge inference
using probabilistic and heuristic rules. More specifi-
cally, we will show how to derive ontologies and per-
form coreference resolution among concepts. For exam-
ple, how automated methods can reach the conclusion
that “President Obama” is a type of “President”, or that
“Obama”, “Barack Obama” and “President Obama” re-
fer to the same external entity, while “Michelle Obama”
refers to another one. Before proceeding, let us consider
implicit taxonomies.
6.1 More about concepts and implicit tax-
onomies
Hyponyms of a concept can be found by looking for hy-
peredges where the concept appears either as the main
argument of a builder-defined concept or as the argu-
ment of a modifier-defined concept. It follows from
these structures that the SH representation implicitly
builds a taxonomy. More generally, we can talk of an
implicit ontology. Beyond the taxonomical relationships
that we described, the concepts that form a concept
hyperedge are related to it in a non-specified fashion.
For example, we know that (germany/C) is related in an
unspecified way to (of/B.ma capital/C germany/C). Of
course, this is not to say that a more specific relation can-
not be inferred by further processing with other meth-
ods. Here we are simply highlighting the ontological re-
lations that come “for free” with the hypergraphic repre-
sentation.
When parsing sentences to hyperedges, and taking
advantage of another classical NLP task offered by the
upstream package, we also store auxiliary hyperedges
connecting every atom that corresponds to word to the
lemma of that word, with the help of a special connector
“lemma/J”. For instance:
(lemma/J saying/P say/P)
In the next section we will make use of this, but it is
easy to see how lemmas facilitate the inference of various
types of correspondences, for example between singular
and plural forms such as (apple/C) and (apples/C) with
the help of (lemma/J apple/C apples/C), and thus more
sophisticated structural variations such as (+/B.am ap-
ple/C season/C) and (of/B.ma season/C apples/C).
6.2 Coreference resolution: co-occurrence
graph
A common but challenging task in NLP is that of
coreference resolution, a usual disambiguation issue
which consists in identifying different sequences of n-
grams that refer to the same entity (such as “Barack
Obama” and “President Obama”). This is an old research
topic [65] that has been revived lately with modern ma-
chine learning methods [55]. ML approaches such as
deep learning require large training sets and tend to pro-
vide black box models, where precision/recall can be
measured and improved upon, but the exact mecha-
nisms by which the models operate remain opaque. Here
we do not mean to provide a complete solution to this
problem, but instead show that several cases of corefer-
ence resolution can be performed in a simpler and un-
derstandable manner through the use of semantic hy-
18
Figure 4: Example of coreference resolution. On the left panel we can see the co-occurrence graph and its compo-
nents, identified by different colors and leading to corresponding coreference sets on the right panel. The probabili-
ties for each coreference set are shown to their left, including the ratios of total degree of the set to total degree, used
to compute them. Individual degrees are shown next to each edge. * indicates the assignment of the seed to one of
the coreference sets. ** indicates the recursive nature of the process, with (+/B michelle/C obama/C) taking the role
of seed in another instance of this coreference resolution process.
pergraphs for situations that are nevertheless common
and useful, especially in the context of social science re-
search.
We will discuss in the following section several exper-
imental results that we obtained on a dataset of several
years of news headlines. This corpus is largely focused
on political issues, and it is dominated by reports of ac-
tors of various types making claims or interacting with
each other. These actors can be people, institutions,
countries and so on. In our hypergraphic representa-
tion, such actors will very frequently be referred to by hy-
peredges forming compound nouns, with the use of the
(+/B) connector, as discussed previously.
In figure 4 we can see such a case: a number of
compound concept edges with the main atomic con-
cept (obama/C) refer to actors. How can we group them
in sets, such that all the cases in a given set refer to
the same entity? Here, we start taking advantage of the
hypergraph as a type of network, and of the analysis
graphs that we can easily distill from the hypergraph.
Semantic graph-based disambiguation has been exten-
sively explored since the mid-2000s, especially empha-
sizing the importance of centrality and proximity in de-
ciding which sense correspond to a given word in a cer-
tain context, and semantic hypergraphs are no exception
[43, 51, 2].
We can trivially traverse all the concepts in the hyper-
graph, finding the subset of concepts that play the role of
main concept in the above mentioned compound con-
cept constructs. For each of these seed concepts, we can
then attempt to find coreference relationships between
the concepts they help build. In the figure, we see an ex-
ample using the seed concept (obama/C). On the right
side of the figure, we see all the compound concepts con-
taining the seed as the main concept (except for the ones
marked with * and **). It is possible then to build a graph
of all the auxiliary concepts that appear together with
the seed. A connection between two concepts in this
graph means that there is at least a compound concept
in which they take part together. In the example, we can
see that this graph has three maximal cliques, which we
identified with different colors. We then apply this sim-
ple rule: two compound concepts are assumed to refer to
the same entity if all of their auxiliary concepts belong to
the same maximal clique. The intuition is that, if auxil-
iary concepts belong to a maximal clique, then they tend
to be used interchangeably along with the seed, which
indicates that they are very likely to refer to the same en-
tity. We will show that this intuition is empirically con-
firmed in our corpus, from where the example in the fig-
ure was extracted.
The co-occurrence graph method produces the coref-
erence sets seen on the right of the figure, except for
the items marked with * and **. As can be seen, it cor-
rectly groups several variations of hyperedges that refer
to Barack Obama (president of the United States dur-
ing most of the time period covered by our news cor-
pus), and it correctly identifies a separate set referring
to Michelle Obama, his wife. It can also be seen that it
fails to identify that “Mr. Obama” is also likely to refer to
Barack Obama. We will say more about this specific case
when we discuss claim analysis, in the next section.
But what about the seed concept itself, in this case
(obama/C)? The co-occurrence method is not able to as-
sign it to one of the sets. Here we employ another simple
method, this time of a more probabilistic nature. Before
tackling this method, we have to make a small detour to
discuss the semantic hypergraph from a network analy-
sis perspective.
19
6.3 Simple hypergraph metrics
In a conventional graph, it is common to talk of the de-
gree of a vertex. This refers simply to the number of
edges that include this vertex or, in other words, the
number of other vertices that it is directly connected
to (we assume here an undirected graph without self-
loops). With a semantic hypergraph, such measure is
not so straightforward, given that an edge can have more
than two participants, and that recursivity is permitted.
Let us first define the set De , containing all the edges
in with a given edge e participates:
De = {ei |ei ∈ E ∧ e ∈ ei } (1)
We define the degree of a hyperedge e as:
d(e) = ∑
ei ∈De
(|ei | − 1) (2)
This is to say, the hypergraphic degree is the number
of edges with which a given edge is connected to by outer
hyperedges. It is intuitively equivalent to the conven-
tional graph degree.
Another useful metric that we can define is the deep
degree, which considers edges connected by hyperedges
not necessarily at the same level, but appearing recur-
sively at any level of the connecting hyperedge. Let
us consider the set ∆e , containing the edges that co-
participate in other edges with e at any level. This set
is recursively defined, so we describe how to generate it,
in Algorithm 2.
Function Generate∆(e)
Data: An edge e
Result: ∆e neighborhood of edge e
∆e ←− De
for e′ ∈ De do
∆′ ←− Generate∆(e’)
∆e ←− ∆e ∪ ∆′
end for
return ∆e
end
Algorithm 2: Generating the neighborhood ∆e of
an edge e.
We can now define the deep degree δ as:
δ(e) = ∑
ei ∈∆e
(|ei | − 1) (3)
To provide a more intuitive understanding of these
metrics, let us consider the edge “(is/P berlin/C (of/B
capital/C germany/C))”. Let us also assume that no other
edges exist in the hypergraph. In this case, the edges
(is/P), (of/B capital/C germany/C) and (germany/C) all
have degree d = 1, because they all participate exactly
in one edge. The first two ((is/P) and (of/B capital/C
germany/C)) also have deep degree δ = 1, but the lat-
ter (germany/C) has deep degree δ = 2, because not only
does it participate directly in the edge (of/B capital/C ger-
many/C), but it also participates at a deeper level in the
outer edge (is/P berlin/C (of/B capital/C germany/C)). In
other words, the higher deep degree of (germany/C) indi-
cates that it plays an increased role as a building block in
other edges.
6.4 Coreference resolution: probabilistic
seed assignment
Back to figure 4, each coreference set is labeled with a
probability p, representing the chance that a given seed
appears in one of its edges, if we were to uniformly enu-
merate all edges that rely on this seed. This configures
a simple estimation of the probability of the seed by it-
self being used with a certain meaning, represented by
the given coreference set. These probabilities are thus
the ratio between the sum of the degrees of the edges in
each coreference set and the total degree of all edges that
include the seed, i.e. of all coreference sets.
Two simple heuristics drive this step. One is that peo-
ple will tend to use an ambiguous abbreviation of a con-
cept when the popularity of one of the interpretations is
sufficiently high in relation to all the others. For exam-
ple, both (+/B barack/C obama/C) and (+/B michelle/C
obama/C) share the seed (obama/C), but when referring
only to (obama/C) during the period he was a US pres-
ident, people tend to assume that it refers to the most
frequently mentioned entity – Barack Obama. The other
is that a given seed should only be considered as an ab-
breviation if it is used a sufficient amount of times as a
primary concept in relations, i.e. if there is evidence that
it is in fact used on its own to refer directly to some ex-
ternal concept, and not only as a common component
of primary concepts. Put differently, seeds referring to
common concepts which act often as building blocks of
other concepts (i.e., higher deep degree with respect to
degree) are less likely to be valid abbreviations. Such is
the case for “house” (which may indifferently refer to the
White House or Dr. House) and “qaida” (which is typi-
cally used as a building block for Al Qaida and never by
itself).
We thus establish a criterion that consists of the ful-
fillment of each of these two conditions, corresponding
respectively to the heuristics above. A given seed s is as-
signed to the coreference set C with the highest p if:
• p is above a certain threshold θ
• ds /δs is above a certain threshold θ′
We set the threshold to the values θ = .7 and θ′ = .05,
that we verified empirically to produce good results. Nat-
urally, these thresholds can be fine-tuned using methods
more akin to hyper-parameter optimization in ML, but
20
such optimizations are outside of the scope of this work.
When the criterion is not met, the seed is left as a refer-
ence to a distinct entity. In our corpus, this happens for
example with "Korea", which remains an ambiguous ref-
erence to either “North Korea” or “South Korea”.
6.5 Further disambiguation cases
We do not present here a general solution for corefer-
ence resolution and synonym detection, let alone disam-
biguation as a whole. Some further cases beyond coref-
erence resolution will nonetheless be covered in the next
section, notably anaphora resolution, given that this re-
quires the discussion of predicates and relations in more
detail, and along with empirical results. Other cases are
left out of this work, but we would like to provide a quick
insight into how they may be treated.
One obvious example is that of synonyms, which are
not implied by a pure structural analysis of hyperedges
– e.g. red and crimson, as well as U.S. and United States,
for they share no common seed (as opposed to the cases
emphasized in the previous subsection). This type of
synonym detection may be achieved with the help of a
general-knowledge ontology such as Wordnet or DBPe-
dia, and/or with the help of word embeddings such as
word2vec. This is a foreseeable and desirable improve-
ment to hypergraph-based text analysis that we leave for
future work.
Another case is the inverse problem of synonym
detection: disambiguating atoms that correspond to
the same word but to different entities, for exam-
ple distinguishing “Cambridge (UK)” from “Cambridge
(USA/Massachusetts)”. We do not perform this type of
distinction in this work, but we present another syntac-
tic detail that enables them from a knowledge represen-
tation perspective: the atom namespace. Quite sim-
ply, beyond the human-readable part and the type and
other machine-oriented codes, a third optional slash-
separated part can be added to atoms, allowing to dis-
tinguish them in cases such as the above, e.g.: cam-
bridge/C/1 and cambridge/C/2.
Finally, coreference resolution can also apply to cases
where neither seed concepts are shared, nor anaphoras
are present. Let us say that one sentence refers to
“Kubrick” and the next one to “the film director”. Both
this type of case and the above mentioned disambigua-
tion cases are likely to be more easily solved with the help
of structured knowledge surrounding the concepts in
the semantic hypergraph, eventually including general
knowledge as mentioned. For example, it could be de-
tected that a certain reference to “Cambridge” is closer to
references related to the United States, or that “Kubrick”
is structurally close to the concept of “film director”. Al-
ternatively, a hybrid approach taking advantage of deep
learning models can be employed. In fact, we success-
fully integrated such a system3 with the Graphbrain li-
brary, but avoid using it in this work for the sake of sim-
plicity while defining SH methodological foundations.
7 Integrated Case Study: Claim and
Conflict Analysis
We arrive at the point where we can propose an inte-
grated application of the formalisms and methods dis-
cussed so far to the analysis of a large corpus of real
text, combining symbolic and probabilistic rules. More
specifically, we worked with a corpus of news titles that
were shared on the social news aggregator Reddit. We
extracted all titles shared between January 1st, 2013 and
August 1st, 2017 on r/worldnews, a community that is
described as: “A place for major news from around the
world, excluding US-internal news.” This resulted in a
corpus of 404,043 news titles. We applied the methods
described in sections 4 and 6 to generate a hypergraph
from the titles.
We decided to focus on two specific categories of ut-
terances that are very frequent in news sources, and of
special interest for the social sciences [72], especially the
study of public spaces [59, 74]: a claim made by an ac-
tor about some topic and an expression of conflict of one
actor against another, over some topic. Helpfully, the de-
tection of such categories also allows us to illustrate sim-
ple symbolic inference over the hypergraph.
7.1 Knowledge Inference
The English language allows for vast numbers of verb
constructions that indicate claims or expressions of con-
flict. Instead of attempting to identify all of them, we
considered the 100 most common predicate lemmas in
the hypergraph, and from there we identified a set of
“claim predicates” and a set of “conflict predicates”, that
we detail below. Overall, we found 3730 different predi-
cate lemmas, and their rank-frequency distribution is ex-
pectedly heavy-tailed. In this case, this small fraction of
the set accounts for 60.6% of the hyperedges. Naturally,
coverage could be improved by considering more predi-
cates, but with diminishing returns.
We then employed the same process described in sub-
section 5.3 to discover rules that are capable of detecting
claims and expressions of conflict, whereby a hyperedge
contains an attributable:
• claim, if the following conjunction of patterns is sat-
isfied:
(PRED/P.{sr} ACTOR/C CLAIM/[RS])
∧ (lemma/J >PRED/P [say,claim]/P)
3https://github.com/huggingface/neuralcoref
21
“Germany warns Russia against military engagement in Syria”
(warns/P.sox
germany/C
russia/C
(against/T
(military/M
(in/B.ma engagement/C syria/C))))
predicate
subject
object
trigger
specification
(says/P.sr
(+/B.am north/C korea/C)
((not/M ’s/P.sc)
it/C
(of/B.ma
afraid/C
(military/M (+/B.am us/C strike/C)))))
outer predicate
outer subject
relative
relation
inner predicate
inner subject
inner
complement
“North Korea says it's not afraid of US military strike”(a) (b)Figure 5: Two examples of relations starting with either a claim or a conflict predicate.
• expression of conflict, if the following conjunction
of patterns is satisfied:
( PRED/P.{so,x} SOURCE/C TARGET/C
[against,for,of,over]/T TOPIC/[RS] )
∧ ( lemma/J >PRED/P
[accuse,arrest,clash,condemn,kill,slam,warn]/P )
So, a claim is essentially a relation, based on a predi-
cate of lemma “say” or “claim”, between an actor and a
claim, which may also be a relation or a specifier. These
patterns additionally rely on a new notation, “>”. As we
have seen in section 3, a predicate can be a non-atomic
hyperedge. As with concepts, the meaning of predicate
atoms can be extended with a modifier. For example, the
English verb conjugation “was saying” is represented as
(was/M saying/P). Eventually, there is always a predicate
atom that corresponds to the main verb in the predicate:
the notation refers to the innermost atom, i.e. removing
an arbitrary amount of nesting based on modifiers. For
example, “>PRED” matches “been/P”, “(has/M been/P)”,
“(not/M (has/M been/P))”, and so on.
In figure 5 we present two real sentences from our cor-
pus and their respective hyperedges. Example (a) was
classified as a claim and example (b) as an expression of
conflict. These examples were purposely chosen to be
simple, but the above rules can match more complicated
cases. For example, the following sentence was correctly
identified and parsed as a claim:
U.S. Secretary of State John Kerry was the in-
tended target of rocket strikes in Afghanistan’s
capital Saturday, the Taliban said in a state-
ment claiming responsibility for the attacks.
Validation. In table 8 we present an evaluation of accu-
racy based on the manual inspection of 100 claims and
100 conflicts that were randomly selected from the hy-
pergraph. Defects are deemed to be minor if they do not
interfere with the overall meaning of the hyperedge (e.g.,
by leading to one of the other, more serious errors listed
in the table). To illustrate with a minor defect from our
dataset:
task error type error error
count rate
claim inference not a claim 0/100 0%
wrong actor 0/100 0%
wrong topic 2/100 2%
bad anaphora resolution 1/13 8%
minor defects in topic 8/100 8%
conflict inference not a conflict 0/100 0%
wrong origin actor 0/100 0%
wrong target actor 0/100 0%
wrong topic 0/100 0%
minor defects in topic 4/100 4%
Table 8: Evaluation of several types of error in claim and
conflict inference. Error counts and rates are presented,
based on the manual inspection of 100 randomly se-
lected claims and 100 randomly selected conflicts.
(claims/P.sr (+/B.am google/C boss/C) ((does/M (not/M
know/P.so)) he/C (in/B.ma (his/M salary/C) (+/B.am
commons/C grilling/C))))
In this case, the concept “commons grilling” should be a
separate specification:
(claims/P.sr (+/B.am google/C boss/C) ((does/M (not/M
know/P.sox)) he/C (his/M salary/C) (in/T (+/B.am
commons/C grilling/C))))
Subjects and actors. Both claim and conflict structures
imply that the hyperedge playing the role of subject in
the relation is an actor. Using the methods described
in section 6, we can identify the coreference set of each
actor and replace all occurrences of this actor with the
same hyperedge. For each coreference set we choose
the hyperedge with the highest degree as the main iden-
tifier, following the heuristic that the most commonly
used designation of an entity should be both recogniz-
able and sufficiently compact.
As seen in figure 5(a), the inner subject (i.e., the sub-
ject of the relative relation that represents what is being
claimed) can be a pronoun. These cases are very com-
mon, and almost always correspond to a case where the
actor is referencing itself in the content of the claim. On
one hand, we perform simple anaphora resolution: if the
inner subject is a pronoun in the set {he/C, it/C, she/C,
they/C}, then we replace it with the outer subject. On the
22
Type Rank Actor Hyperedges (coreference set) Degree
non-human
1 China china/C, (+/B.am south/C china/C) 6199
2 Russia russia/C 5861
3 U.S. us/C, (the/M us/C) 3824
male
8 Vladimir Putin (+/B.am president/C putin/C), putin/C, (+/B.am vladimir/C putin/C), (+/B.am
president/C (+/B.am vladimir/C putin/C)), (+/B.am (russian/M president/C)
(+/B.am vladimir/C putin/C)), (+/B.am (russian/M president/C) putin/C)
2338
10 Barack Obama (+/B.am (+/B.am us/C president/C) (+/B.am barack/C obama/C)), (+/B.am
president/C obama/C), (+/B.am barack/C obama/C), (+/B.am president/C
(+/B.am barack/C obama/C)), obama/C, (+/B.am (+/B.am u.s./C president/C)
(+/B.am barack/C obama/C))
2069
23 Donald Trump (+/B.am president/C (+/B.am donald/C trump/C)), (+/B.am (+/B.am us/C pres-
ident/C) (+/B.am donald/C trump/C)), (+/B.am donald/C trump/C), trump/C,
(+/B.am president/C trump/C)
1082
female
32 Angela Merkel merkel/C, (+/B.am angela/C merkel/C), (+/B.am (german/M chancellor/C)
(+/B.am angela/C merkel/C)), (+/B.am chancellor/C (+/B.am angela/C
merkel/C)), (+/B.am (german/M chancellor/C) merkel/C)
750
78 Theresa May may/C, (+/B.am theresa/C may/C), (+/B.am (+/B.am prime/C minister/C)
(+/B.am theresa/C may/C))
270
201 Nicola Sturgeon sturgeon/C, (+/B.am nicola/C sturgeon/C) 81
group
46 The Palestinians palestinians/C, (the/M palestinians/C) 487
70 The Kurds kurds/C, (the/M kurds/C) 302
113 The Russians russians/C, (the/M russians/C) 184
Table 9: Three actors with highest hypergraphic degree in each category: non-human, male, female, group (in de-
creasing order of highest degree).
other hand, we take advantage of the pronoun to infer
more things about the actor. The four pronouns men-
tioned indicate, respectively, that the actor is a male hu-
man, a non-human entity, a female human, or a group.
We take the majority case, when available, to assign one
of these categories to actors. The pronoun they is being
increasingly used as a gender-neutral third person singu-
lar case, but we have not found such cases in our corpus.
Table 9 shows the top three actors per category, ranked
by their degree in the hypergraph, along with their coref-
erence set. Obviously, more sophisticated rules can be
devised, both for anaphora resolution and category clas-
sification. Our goal here is to illustrate that, thanks to the
SH abstraction, it becomes possible to perform powerful
inferences (i.e., both useful and at a high level of seman-
tic abstraction) with very simple rules.
7.2 Topic Structure
The very definition of topic, for the purpose of automatic
text analysis, is somewhat contingent on the method be-
ing employed. One of the most popular topic detection
methods in use nowadays is Latent Dirichlet Allocation
(LDA) [12], which is a probabilistic topic model [11] that
views topics as latent entities that explain similarities be-
tween sets of documents. In LDA, topics are abstract
constructs. Documents are seen as a random mixture of
topics, and topics are characterized by their probability
of generating each of the words found in the document
set. LDA uses a generative process to statistically infer
these probabilities. Ultimately, a topic is described by
the set of words with the highest probabilities of being
generated by it. Human observers can then infer some
higher-level concept from these words and probabilities.
For example, if the five highest probability words for a
topic X are {EU, Ursula von der Leyen, Boris Johnson,
Barnier, Trade}, a human observer may guess that a good
label for this topic is Brexit Negotiations. LDA is appli-
cable to sets of documents, for a predefined number of
topics, where each document is considered to be a bag-
of-words. Distributional topic detection methods have
recently generated a variety of research endeavors, in-
cluding the application of stochastic blockmodels to dis-
cover joint groups of documents which use keywords in
a similar fashion [27].
A different approach to topic detection is Tex-
tRank [44], which is capable of detecting topics within a
single document. With TextRank, the document is first
transformed into a word co-occurrence graph. Com-
mon NLP approaches are used to filter out certain classes
words from the graph (e.g., do not consider articles such
as “the”). Topics are considered to be the words with
the highest network centrality in this graph, according
to some predefined threshold condition. Simple statis-
tical methods over the co-occurrence graph can be used
to derive ngram topics from the previous step. Given that
the order in which words appear in the document is im-
portant, TextRank cannot be said to be a bag-of-words
approach such as LDA. It relies a bit more on the mean-
ing of the text, and it is more local – in the sense that it
works inside a single document instead of requiring sta-
tistical analysis over a corpus of documents.
In this work, we move significantly more in the direc-
tion of text understanding and locality. Our topics are
23
actor topic
→ scuppering syria peace talks
=⇒ assad → war crimes in aleppo
=⇒ damascus → continuing to use chemical weapons
=⇒ the united states → the weakening of europe
=⇒ us → espionage
← mistral delay
→ meddling in election⇐⇒ russia
→ rapid eu sanctions
← being europe’s biggest problem child
⇐⇒ germany → wage dumping in the meat sector
⇐= al qaeda ← new attacks
⇐= council of europe ← allowing to hit parents and spank their children
⇐= iraqis ← imminent isis attacks
⇐= kagame ← rwanda genocide
⇐= london ← plot
⇐= syria’s assad ← supporting terrorism
⇐= un ← racist attacks on black minister
⇐= united nations top hu-
man rights official
← delays
Table 10: List of actors criticizing or being criticized by ego (here, France), and the topics over which the critique ap-
plies. Single arrows show the critique direction (left to right: ego criticizes that actor) for each underlying hyperedge,
double arrows indicate the overall critique direction (which can thus go both ways).
firstly inferred from the meaning of sentences. As we
have shown, pattern analysis of hyperedges can be used
to infer relationships such as claim and conflict, which
imply both actors and topics. Given coreference detec-
tion, such topics are characterized by sets of hyperedges,
but these sets are not probabilistic in the sense that LDA’s
are. Instead, they are a best guess of symbolic represen-
tations that map to some unique concept. Our approach
relies even more on meaning than TextRank, and it al-
lows for topic detection at an even more local scale: sin-
gle sentences.
In the examples given in figure 5, the claim shown in
(a) implies the rather specific topic “afraid of military us
strike”, and (b) the topic “military engagement in syria”.
Another important aspect of our approach is that top-
ics can be composed of other topics or concepts, forming
a hierarchical structure. This is a natural consequence
of how we model language, as explained in section 6.1.
This allows us to explore topics at different levels of de-
tail. The topic implied by a claim or conflict can be very
specific and possibly unique in the dataset, but the more
general subtopics or concepts that it contains can be
used to find commonalities across the hypergraph. Con-
sidering the hyperedge from one of the topic examples
above, from (military/M (in/B engagement/C syria/C)) it
is possible to extract concepts from inner edges that cor-
respond to more general concepts, for example (syria/C),
(engagement/C) and (in/B engagement/C syria/C). With
the help of the implicit taxonomy, which indicates that
(in/B engagement/C syria/C) is a type of engagement/C, a
simple rule could also infer that (military/M (in/B engage-
ment/C syria/C)) is a type of (in/B engagement/C syria/C).
In the various tables of results that we will subse-
quently present, actors and topics are represented by la-
bels in natural language. During the transformation of
text to hyperedge, every hyperedge that is generated is
associated with the chunks of text from which it comes.
These chunks are then used as textual labels for the hy-
peredges.
7.3 Inter-actor criticism
Focusing on France and Germany as target actors a, we
gather the results for the detection of conflict patterns in
the tables 10 and 11. Each of these actors is involved in
active or passive criticism of other actors, i.e. either crit-
ical (→ ) of or criticized by (←) other actors. The critique
is related to a topic, and may go in both directions, i.e.
Germany criticizes Greece for debt commitments (sec-
ond row of table 11).
The topics presented here correspond to the detailed
topics discussed in the previous section. This structured
enumeration provides a way to scan the direction, target
and frequency of claims by actors on other actors in a
given text corpus.
7.4 Dyadic claims
Here we focus on claims that actors make about other
actors (or themselves). In other words, we refer to claims
where the subject of the claim is itself an actor. Fur-
thermore, we consider only claims for which the claim
relation contains an argument playing the rule of com-
plement, meaning that the subject of the claim is being
linked with some concept, for example expressing mem-
bership in a class (e.g.: “Pablo is a cat.”) or the possession
of some property (e.g.: “North Korea is afraid”).
We also recursively extract context edges that are con-
nected to the outer claim edge through nestings of (:/J).
To give an example from our corpus:
(:/J (says/P.sr russia/C (’s/P.sc it/C ready/C)) ((to/M
deal/P.x) (with/T (new/M (+/B.am ukraine/C
president/C)))))
24
actor topic
=⇒ fiat → using illegal emissions device
=⇒ greece → debt commitments
=⇒ israel → latest settlement expansion in east jerusalem
=⇒ kurds → one sided referendum plans
=⇒ maduro → holding venezuelans’ hostage
=⇒ mexico city → brexit
→ cold war reflexes
=⇒ putin → moscow up beefs nuclear arsenal
=⇒ syrian → alleged car bomb plot
=⇒ uk → leaving eu
=⇒ ukraine → graft
=⇒ us → stasi methods ahead of obama
← halting arms deal
→ cyber attack on ukraine peace monitors
→ kremlin dismisses us intelligence claims as a witch hunt
⇐⇒ russia
→ military engagement in syria
← wage dumping in the meat sector
⇐⇒ france → being europe’s biggest problem child
← causing instability
⇐⇒ u.s. → ceding lead role to china
← backing failed coup
← cultural racism over eu accession
← engaging in diplomatic rudeness and double standards
← genocide speech
← harbouring terrorists
← succor providing to its enemies
← succour providing to its enemies
← working against erdogan
→ blackmailing eu
→ itself further distancing from europe by the death penalty reinstating after a disputed referendum
→ monday
→ nazi
⇐⇒ turkey
→ supporting terrorism
⇐= erdogan ← nazi practices over blocked political rallies
⇐= eu commission ← air pollution breaches
⇐= eu leaders ← pressure on migrant quotas
⇐= french far right
leader marine le
pen
← doors opening to refugees
⇐= italy ← undermining its economic efforts
⇐= moscow ← up hushing russian girl’s rape
⇐= orban ← rude tone over refugees
⇐= snowden ← nsa aiding in spying efforts
⇐= turkey’s president
tayyip erdogan
← behaving like nazis
⇐= un ← institutional racism and racist stereotyping against people of african descent
⇐= un committee ← an anti racism - convention violating by not prosecuting a politician’s comments about turks and arabs
Table 11: List of actors criticizing or being criticized by ego (here, Germany), and the topics over which the critique
applies. Single arrows show the critique direction (left to right: ego criticizes that actor) for each underlying hyper-
edge, double arrows indicate the overall critique direction (which can thus go both ways).
The edge “((to/M deal/P.x) (with/T (new/M (+/B.am
ukraine/C president/C))))” is extracted as a context edge.
Finally, specification edges of the claim and context
edges are extracted out and grouped together.
The predicate of the relative relation that expressed
the claim is inspected to further determine the tense
of the attribution (present, past, future), and to identify
negations. Once again, this is achieved by simple rules
over the hypergraphic representation:
• The presence of a negation modifier (not/M, n’t/M),
as is in fact the case with the first example of figure 5.
• The presence of the predicate was/P implies the
past.
• The presence of the modifier will/M implies the fu-
ture tense.
In table 12, we present such attributions between the
actors: North Korea, Russia, Putin and U.S.
7.5 Topic-based conflict network
So far we have presented actor-centric results. Here we
will consider all conflicts that contain “Syria” as topic
or subtopic (according to the definitions of section 7.2).
25
source target property, context and <specification>
just the place for you
the victim of intensive cyberattacks
ready; to strike u.s. aircraft carrier
able; to nuke u.s. mainland
a great place for human rights
ready for war with us
close; developing a new satellite; speculation fuelling it might attempt a long range rocket to fire to mark a key
political anniversary; <next month>
open; holding talks with south korea; <including the suspension of the south’s joint military drills with the
united states>; <if are met certain conditions>
open; to talk with south korea
the biggest victim in u.s. student’s death
responsible for righteous sony hacking
north korea says north korea is
not afraid of us military strike
was ready; to put russia’s nuclear weapons; <during tensions over the crisis in ukraine and crimea>; <on standby>
possible convinced solution to ukraine crisis
willing; to play a mediating role between the two koreas; relieve the state of crisis on the korean peninsula;
<according to president moon jae in’s special envoy to moscow>; <to help>; <by dispatching an emissary>; <to
pyongyang>
ready; to sell s-400 anti aircraft system; <to turkey>
is
not russia’s president for life
russia’s president; 2024
putin
will be not president for life
ready; to improve ties with the us
moral compass of the world
interested; other brics brazil, russia, india, china, south africa members; using national currencies; <after
agreeing on such an arrangement with china>
willing; over to hand to us house of representatives and senate
russia is
not a threat to anyone
was the mastermind of the ukrainian coup
us
says putin
is world’s only superpower; back walks trump compliments
open; coordinating with u.s. in syria
ready; to deal with new ukraine president; to retaliate for u.s. election sanctions
ready for dialogue with petro poroshenko, ukraine’s next president
ready; to provide the free syrian army; <with air support in fight against islamic state>
russia says russia is
building naval bases in asia, latin america
disappointed over china’s failure; over to hand fugitive intelligence analyst edward snowden
open; to work with new iranian president hussain rowhanius says us is
not surprised; <if north korea launches missiles>
Table 12: List of claims, or attributions by subject actors (sources) about other actors (targets). Automatically identi-
fied negative claims are emphasized.
From this set of hyperedges we extracted a directed net-
work connecting actors engaging in expressions of con-
flict over Syria. A visualization of this network is pre-
sented in figure 6.
We devised a very simple algorithm to identify two fac-
tions in this conflict graph. Firstly, we attribute a score
si j to every hyperedge (ei j ):
si j = min(di , d j )
where di is the degree (in- and out-) of node i . Then we
iterate through hyperedges in descending order of s. This
heuristic assumes that hyperedges connecting more ac-
tive nodes are more likely to represent the fundamental
dividing lines of the overall conflict. The first hyperedge
assigns one node to faction A and another to faction B.
From then on, a node is assigned to a faction if it does not
have a conflict with any current member of this faction,
and has a conflict with a current member of the opposite
faction. In the case that the node cannot be assigned to
any faction, it remains unassigned.
This resulted in faction A containing the actors: {rus-
sia, iran, moscow, putin, china, erdogan, palestinians}, fac-
tion B the actors: {us, west, israel, the united states, france,
netanyahu, uk, germany, obama} and the following ac-
tors remaining unassigned: {turkey, assad, the european
union}. Faction A is shown in blue in figure 6, and faction
B in red. This categorization and network visualization
suggest that the main axis of the conflict around Syria is
a Russia / U.S conflict. Factions A and B contain state ac-
tors and political leaders that are typically aligned with,
respectively, Russia and the U.S.
Naturally, more sophisticated faction and alliance de-
tection methods can be employed. Here we are mostly
interested in showing the effectiveness of our approach
in summarizing complex situations from large natural
language corpora, and to provide some empirical valida-
tion that these results are sensible. This conflict graph
was built from a total of 53 hyperedges, and we manually
verified that they all correspond to expressions of con-
flict, and that both the intervening actors and main topic
were correctly identified in all cases.
8 Conclusions
We have presented the novel SH formalism, aimed at a
new approach to language understanding based on the
26
Figure 6: Network of conflicts between actors over the topic “Syria”. Arrows point from the originator of the conflict
to its target. Size of nodes is proportional to their degree. Two factions were identified by a simple algorithm. One
faction is represented as red, the other blue. Gray nodes do not belong to any faction.
idea of translating NL into a structured and formal rep-
resentation, while allowing room for the inherent and of-
ten irreducible ambiguities found in human communi-
cation. Developing the formalism entailed an effort of
modeling of NL into a set of types and syntactic rules that
(1) preserves the richness of NL, (2) facilitates computa-
tional language understanding tasks and (3) can act as a
lingua franca for hybrid systems that include both hu-
man and computational intelligence of various natures
(e.g. symbolic, graph-based and statistical).
Surrounding this central idea, we presented a viable
parser of NL to SH using standard contemporary ML
techniques as well as higher-level linguistic features pro-
vided by standards NLP libraries. Furthermore, we have
shown that inference rules and knowledge extraction
patterns can be represented in SH notation itself, and we
have developed a procedural template for systems capa-
ble of learning inference rules with the collaboration of
humans and reference hypergraphs extracted from open
text.
We believe to have empirically validated our approach
from several angles and in several ways: in terms of the
completeness of the representation by its ability to rep-
resent all grammatical constructs found in the Univer-
sal Dependencies; in terms of the precision of the parser
across of variety of text categories; in terms of the ex-
pressive power of SH in being able to produce compet-
itive results in a task for which a number of dedicated
competing systems and an external benchmark exists;
and finally in its ability to tackle a set of specific and re-
lated tasks of language understanding of particular inter-
est to the social sciences (actor and gender detection, co-
reference resolution, claim and conflict analysis), pro-
ducing results that reasonably match common-sense in-
tuitions about the ground truth, and also display good
precision when manually verified.
The central goal has been to lay the foundations of this
approach and demonstrate its potential. We do not claim
to have the best performing system in any of the tasks
that we tackled, nor was this our aim, but we do hope
to have demonstrated the versatility, completeness and
potential of SH.
Our further ambition is to apply this method to a va-
riety of language understanding tasks where the under-
standability of results is desirable, for example in news
/ social media analysis (detection of actors, topics, con-
flicts, agreements, causality, beliefs), or to extract view-
points from scientific articles, or to help in the study of
cultural objects such as literary works (known as Distant
Reading [48]). Our hope is that we were able to con-
vince the reader of the potential of SH for such tasks,
enabling large-scale text corpus analysis while preserv-
ing the rich understanding expected in social science en-
deavors which normally require a significant amount of
tedious manual coding [72].
We created the Graphbrain open-source software li-
brary that implements all the ideas we described4, aim-
4https://github.com/graphbrain/graphbrain
27
ing not only to facilitate the replicability of all the ex-
periments that we performed in this work, but also to
facilitate the adoption and extension of SH and related
methodology by the research community at large.
Acknowledgements
This research has been supported by the “Socsemics”
Consolidator grant funded by the European Research
Council (ERC) under the European Union Horizon 2020
research and innovation program, grant agreement No.
772743.
References
[1] Adida, B., Birbeck, M., McCarron, S., Pemberton, S., 2008.
RDFa in XHTML: Syntax and processing. Recommenda-
tion, W3C 7.
[2] Agirre, E., Soroa, A., 2009. Personalizing pagerank for
word sense disambiguation, in: Proceedings of the 12th
Conference of the European Chapter of the Association
for Computational Linguistics, Association for Computa-
tional Linguistics. pp. 33–41.
[3] Allen, J.F., Frisch, A.M., 1982. What’s in a semantic net-
work?, in: Proceedings of the 20th annual meeting on As-
sociation for Computational Linguistics, Association for
Computational Linguistics. pp. 19–27.
[4] Angeli, G., Premkumar, M.J.J., Manning, C.D., 2015. Lever-
aging linguistic structure for open domain information
extraction, in: Proc. 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and 7th Intl. Joint Con-
ference on Natural Language Processing, pp. 344–354.
[5] Angelov, D., 2020. Top2vec: Distributed representations
of topics. arXiv preprint arXiv:2008.09470 .
[6] Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R.,
Ives, Z., 2007. Dbpedia: A nucleus for a web of open data,
in: The semantic web. Springer, pp. 722–735.
[7] Banarescu, L., Bonial, C., Cai, S., Georgescu, M., Grif-
fitt, K., Hermjakob, U., Knight, K., Koehn, P., Palmer, M.,
Schneider, N., 2013. Abstract meaning representation for
sembanking, in: Proc. 7th linguistic annotation workshop
and interoperability with discourse, pp. 178–186.
[8] Battiston, F., Cencetti, G., Iacopini, I., Latora, V., Lucas, M.,
Patania, A., Young, J.G., Petri, G., 2020. Networks beyond
pairwise interactions: structure and dynamics. Physics
Reports 874, 1–92.
[9] Berge, C., 1984. Hypergraphs: combinatorics of finite sets.
volume 45 of North-Holland mathematical library. North-
Holland, Amsterdam.
[10] Berners-Lee, T., Hendler, J., 2001. Publishing on the se-
mantic web. Nature 410, 1023–1024.
[11] Blei, D.M., 2012. Probabilistic topic models. Communica-
tions of the ACM 55, 77–84.
[12] Blei, D.M., Ng, A.Y., Jordan, M.I., 2003. Latent dirichlet
allocation. Journal of machine Learning research 3, 993–
1022.
[13] Boley, H., 1977. Directed recursive labelnode hyper-
graphs: A new representation-language. Artificial Intel-
ligence 9, 49–85.
[14] Bosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyil-
maz, A., Choi, Y., 2019. Comet: Commonsense transform-
ers for automatic knowledge graph construction. arXiv
1906.05317.
[15] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,
A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan,
T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter,
C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,
Chess, B., Clark, J., Berner, C., McCandlish, S., Radford,
A., Sutskever, I., Amodei, D., 2020. Language models are
few-shot learners. arXiv 2005.14165.
[16] Cattuto, C., Schmitz, C., Baldassarri, A., Servedio, V.D.,
Loreto, V., Hotho, A., Grahl, M., Stumme, G., 2007. Net-
work properties of folksonomies. Ai Communications 20,
245–262.
[17] Chavalarias, D., Wallach, J.D., Li, A.H.T., Ioannidis, J.P.,
2016. Evolution of reporting p values in the biomedical
literature, 1990-2015. Jama 315, 1141–1148.
[18] Choi, J.D., Tetreault, J., Stent, A., 2015. It depends: Depen-
dency parser comparison using a web-based evaluation
tool, in: Proc. 53rd Annual Meeting of the Association for
Computational Linguistics and 7th Intl. Joint Conference
on Natural Language Processing, pp. 387–396.
[19] Collobert, R., Weston, J., 2008. A unified architecture for
natural language processing: Deep neural networks with
multitask learning, in: Proceedings of the 25th interna-
tional conference on Machine learning, pp. 160–167.
[20] Del Corro, L., Gemulla, R., 2013. Clausie: clause-based
open information extraction, in: Proceedings of the 22nd
international conference on World Wide Web, pp. 355–
366.
[21] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019.
BERT: Pre-training of deep bidirectional transformers for
language understanding, in: Proc. 2019 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, ACL,
Minneapolis, Minnesota. pp. 4171–4186.
[22] Diesner, J., Carley, K., 2005. Revealing social structure
from texts: Meta-matrix text analysis as a novel method
for network text analysis, in: Causal mapping for research
in information technology. IGI Global, pp. 81–108.
[23] Eslahchi, C., Rahimi, A., 2007. Some properties of ordered
hypergraphs. Matematiˇcki vesnik 59, 9–13.
28
[24] Etzioni, O., Banko, M., Soderland, S., Weld, D.S., 2008.
Open information extraction from the web. Communi-
cations of the ACM 51, 68–74.
[25] Fader, A., Soderland, S., Etzioni, O., 2011. Identifying re-
lations for open information extraction, in: Proc. Conf. on
empirical methods in natural language processing, ACL.
pp. 1535–1545.
[26] Gashteovski, K., Gemulla, R., Del Corro, L., 2017. Minie:
minimizing facts in open information extraction, in: Proc.
of the 2017 Conf. on Empirical Methods in Natural Lan-
guage Processing, Association for Computational Linguis-
tics. p. 2620–2630.
[27] Gerlach, M., Peixoto, T.P., Altmann, E.G., 2018. A network
approach to topic models. Science advances 4, eaaq1360.
[28] Goertzel, B., 2006. Patterns, hypergraphs and embod-
ied general intelligence, in: IJCNN’06 International Joint
Conference on Neural Networks, IEEE. pp. 451–458.
[29] Grimmer, J., Stewart, B.M., 2013. Text as data: The
promise and pitfalls of automatic content analysis meth-
ods for political texts. Political Analysis 21, 267–297.
[30] Hart, D., Goertzel, B., 2008. Opencog: A software frame-
work for integrative artificial general intelligence, in: Arti-
ficial General Intelligence, IOS Press. pp. 468–472.
[31] Honnibal, M., Johnson, M., et al., 2015. An improved non-
monotonic transition system for dependency parsing, in:
EMNLP’15 Proc. of the 2015 Conf, on Empirical Methods
in Natural Language Processing, pp. 1373–1378.
[32] Iordanov, B., 2010. HyperGraphDB: A generalized graph
database, in: Shen, H.T., Pei, J., Özsu, M.T., Zou, L., Lu,
J., Ling, T.W., Yu, G., Zhuang, Y., Shao, J. (Eds.), Web-
Age Information Management, Springer Berlin Heidel-
berg, Berlin, Heidelberg. pp. 25–36.
[33] Le, Q., Mikolov, T., 2014. Distributed representations of
sentences and documents, in: International conference
on machine learning, PMLR. pp. 1188–1196.
[34] Léchelle, W., Gotti, F., Langlais, P., 2019. Wire57 : A fine-
grained benchmark for open information extraction, in:
Friedrich, A., Zeyrek, D., Hoek, J. (Eds.), Proc. of the 13th
Linguistic Annotation Workshop, LAW at ACL 2019, Flo-
rence, Italy, August 1, 2019, Association for Computa-
tional Linguistics. pp. 6–15.
[35] Léchelle, W., Gotti, F., Langlais, P., 2020. Resources for the
open information extraction benchmark WiRe57, com-
panion to Léchelle et al., 2019. URL: https://github.
com/rali-udem/WiRe57.
[36] Lenat, D.B., Guha, R.V., Pittman, K., Pratt, D., Shepherd,
M., 1990. Cyc: toward programs with common sense.
Communications of the ACM 33, 30–49.
[37] Leydesdorff, L., Nerghes, A., 2017. Co-word maps and
topic modeling: A comparison using small and medium-
sized corpora (n < 1,000). Journal of the American Society
for Information Science and Technology 68, 1024–1035.
[38] Lippi, M., Torroni, P., 2016. Argumentation mining: State
of the art and emerging trends. ACM Transactions on In-
ternet Technology (TOIT) 16, 10.
[39] Lowe, W., 2008. Understanding wordscores. Political Anal-
ysis 16, 356–371.
[40] Mausam, Schmitz, M., Soderland, S., Bart, R., Etzioni, O.,
2012. Open language learning for information extraction,
in: Proceedings of the 2012 joint conference on empiri-
cal methods in natural language processing and compu-
tational natural language learning, pp. 523–534.
[41] Mausam, M., 2016. Open information extraction sys-
tems and downstream applications, in: Proceedings of the
Twenty-Fifth International Joint Conference on Artificial
Intelligence, pp. 4074–4077.
[42] McCarthy, J., 1960. Recursive functions of symbolic ex-
pressions and their computation by machine, part i. Com-
munications of the ACM 3, 184–195.
[43] Mihalcea, R., 2005. Unsupervised large-vocabulary word
sense disambiguation with graph-based algorithms for
sequence data labeling, in: Proceedings of Human Lan-
guage Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, pp. 411–418.
[44] Mihalcea, R., Tarau, P., 2004. Textrank: Bringing order into
text, in: EMNLP’04 Proc. 2004 Conf. on Empirical Meth-
ods in Natural Language Processing, pp. 404–411.
[45] Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013. Efficient
estimation of word representations in vector space. arXiv
preprint arXiv:1301.3781 .
[46] Miller, G.A., 1995. Wordnet: a lexical database for english.
Communications of the ACM 38, 39–41.
[47] Monroe, B.L., Colaresi, M.P., Quinn, K.M., 2008.
Fightin’words: Lexical feature selection and evalua-
tion for identifying the content of political conflict.
Political Analysis 16, 372–403.
[48] Moretti, F., 2013. Distant reading. Verso Books.
[49] Murakami, K., Nichols, E., Mizuno, J., Watanabe, Y., Ma-
suda, S., Goto, H., Ohki, M., Sao, C., Matsuyoshi, S., Inui,
K., et al., 2010. Statement map: reducing web information
credibility noise through opinion classification, in: Pro-
ceedings of the fourth workshop on Analytics for noisy
unstructured text data, ACM. pp. 59–66.
[50] Nadeau, D., Sekine, S., 2007. A survey of named en-
tity recognition and classification. Lingvisticae Investiga-
tiones 30, 3–26.
[51] Navigli, R., Lapata, M., 2007. Graph connectivity mea-
sures for unsupervised word sense disambiguation., in:
IJCAI, pp. 1683–1688.
[52] Nivre, J., De Marneffe, M.C., Ginter, F., Goldberg, Y., Ha-
jic, J., Manning, C.D., McDonald, R., Petrov, S., Pyysalo,
S., Silveira, N., Tsarfaty, R., Zeman, D., 2016. Univer-
sal dependencies v1: A multilingual treebank collection,
in: Proceedings of the Tenth International Conference on
29
Language Resources and Evaluation (LREC’16), pp. 1659–
1666.
[53] Palmer, M., Gildea, D., Kingsbury, P., 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Com-
putational linguistics 31, 71–106.
[54] Pang, B., Lee, L., et al., 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval 2, 1–135.
[55] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark,
C., Lee, K., Zettlemoyer, L., 2018. Deep contextualized
word representations. arXiv 1802.05365.
[56] Reddy, S., Täckström, O., Collins, M., Kwiatkowski, T., Das,
D., Steedman, M., Lapata, M., 2016. Transforming de-
pendency structures to logical forms for semantic pars-
ing. Transactions of the Association for Computational
Linguistics 4, 127–140.
[57] Ritter, A., Clark, S., Etzioni, O., et al., 2011. Named entity
recognition in tweets: an experimental study, in: Proceed-
ings of the Conference on Empirical Methods in Natural
Language Processing, Association for Computational Lin-
guistics. pp. 1524–1534.
[58] Roth, C., 2013. Socio-semantic frameworks. Advances in
Complex Systems 16, 1350013.
[59] Ruiz, P., Plancq, C., Poibeau, T., 2016. More than word
cooccurrence: Exploring support and opposition in inter-
national climate negotiations with semantic parsing, in:
LREC: The 10th Language Resources and Evaluation Con-
ference, pp. 1902–1907.
[60] Salton, G., Buckley, C., 1988. Term-weighting approaches
in automatic text retrieval. Information processing &
management 24, 513–523.
[61] Sap, M., Le Bras, R., Allaway, E., Bhagavatula, C., Lourie,
N., Rashkin, H., Roof, B., Smith, N.A., Choi, Y., 2019.
Atomic: An atlas of machine commonsense for if-then
reasoning, in: Proceedings of the AAAI Conference on Ar-
tificial Intelligence, pp. 3027–3035.
[62] Shadbolt, N., Berners-Lee, T., Hall, W., 2006. The semantic
web revisited. IEEE intelligent systems 21, 96–101.
[63] Sim, Y., Acree, B.D., Gross, J.H., Smith, N.A., 2013. Mea-
suring ideological proportions in political speeches, in:
EMLP’13 Proc. 2013 Conf. on Empirical Methods in Nat-
ural Language Processing, pp. 91–101.
[64] Singhal, A., 2001. Modern information retrieval: A brief
overview. IEEE Data Eng. Bull. 24, 35–43.
[65] Soon, W.M., Ng, H.T., Lim, D.C.Y., 2001. A machine learn-
ing approach to coreference resolution of noun phrases.
Computational linguistics 27, 521–544.
[66] Sowa, J.F., 2014. Principles of semantic networks: Explo-
rations in the representation of knowledge. Morgan Kauf-
mann.
[67] spaCy, 2020. Models documentation. URL: https://
spacy.io/models/en.
[68] Speer, R., Chin, J., Havasi, C., 2017. Conceptnet 5.5: An
open multilingual graph of general knowledge, in: Pro-
ceedings of the AAAI Conference on Artificial Intelligence,
pp. 4444–4451.
[69] Srivastava, A.N., Sahami, M., 2009. Text mining: Classifi-
cation, clustering, and applications. CRC Press.
[70] Staab, S., Studer, R., 2010. Handbook on ontologies.
Springer Science & Business Media.
[71] Stanovsky, G., Ficler, J., Dagan, I., Goldberg, Y., 2016. Get-
ting more out of syntax with props. arXiv 1603.01648.
[72] Tilly, C., 1997. Parliamentarization of popular contention
in great britain, 1758-1834. Theory and Society 26, 245–
273.
[73] Van Atteveldt, W., Kleinnijenhuis, J., Ruigrok, N., 2008.
Parsing, semantic networks, and political authority using
syntactic analysis to extract semantic relations from dutch
newspaper articles. Political Analysis 16, 428–446.
[74] Van Atteveldt, W., Sheafer, T., Shenhav, S.R., Fogel-Dror,
Y., 2017. Clause analysis: using syntactic information to
automatically extract source, subject, and predicate from
texts with an application to the 2008–2009 Gaza War. Po-
litical Analysis 25, 207–222.
[75] Vrandeˇci´c, D., 2012. Wikidata: A new platform for collab-
orative data collection, in: Proceedings of the 21st inter-
national conference on World Wide Web, pp. 1063–1064.
[76] Wang, H., Can, D., Kazemzadeh, A., Bar, F., Narayanan, S.,
2012. A system for real-time twitter sentiment analysis of
2012 US presidential election cycle, in: Proceedings of the
ACL 2012 System Demonstrations, Association for Com-
putational Linguistics. pp. 115–120.
[77] Wilkerson, J., Casas, A., 2017. Large-scale computerized
text analysis in political science: Opportunities and chal-
lenges. Annual Review of Political Science 20, 529–544.
A Mapping Universal Stanford De-
pendencies to hyperedges
We used the Universal Stanford Dependencies [52] to
guide the development of the Semantic Hypergraphs rep-
resentation. In table 13, we present one example of hy-
peredge for each grammatical relation in the Universal
Dependencies. This is meant as empirical evidence for
the completeness of our model, in terms of its ability to
map to natural language constructs in most human lan-
guages.
B Most common hyperedge patterns
Table 14 lists the 50 most commons hyperedge patterns
in a hypergraph built from a Wikipedia sample.
30
Grammatical Relation Hyperedge Example
nsubj: nominal subject (is/P.sc (the/M dog/C) cute/C)
nsubjpass: passive nominal subject ((was/M played/P.pa) (the/M piano/C) (by/T mary/C))
dobj: direct object (accusative) (gave/P.sio mary/C john/C (a/M gift/C))
iobj: indirect object (dative) (gave/P.sio mary/C john/C (a/M gift/C))
csubj: clausal subject (makes/P.so (said/P.os what/C she/C) sense/C)
csubjpass: clausal passive subject ((was/M suspected/P.pa) (that/T (lied/P.s she/C)) (by/T everyone/C))
ccomp: clausal complement (says/P.so he/C (like/P.so you/C (to/M swim/C)))
xcomp: open clausal complement (says/P.so he/C (like/P.so you/C (to/M swim/C)))
nmod: nominal modifier ((of/B some/C (the/M toys/C))
advcl: adverbial clause modifier (talked/P.sxx he/C (to/T him/C) (to/T (secure/P.o (the/M account/C))))
advmod: adverb modifier ((genetically/M modified/M) food/C)
neg: negation modifier ((not/M is/P.sc) bill/C (a/M scientist/C))
vocative: vocative (know/P.sv i/C john/C)
discourse: discourse N/A
expl: expletive ((there/M is/P) (a/M (in/B ghost/C (the/M room/C))))
aux: auxiliary ((has/M (been/M killed/P.p)) kennedy/C)
auxpass: passive auxiliary ((has/M (been/M killed/P.p)) kennedy/C)
cop: copula (is/P.sc bill/C big/C)
mark: marker (says/P.so he/C (that/T (like/P.so you/C (to/M swim/C))))
punct: punctuation N/A
conj: conjunction (is/P.so bill/C (and/J big/C honest/C))
cc: coordination (is/P.so bill/C (and/J big/C honest/C))
nummod: numeric modifier (ate/P.so sam/C (3/M sheep/C))
relcl: relative clause modifier (saw/P.so i/C (the/M (love/P.so you/C man/C)))
det: determiner (is/P.sc (the/M man/C) here/C)
compound: compound (has/P.so john/C (the/M (+/B phone/C book/C)))
name: multi-word proper nouns (+/B marie/C curie/C)
mwe: multi-word expression (cried/P.sx he/C (because/T (of/M you/C)))
foreign: foreign words misc.
goeswith: goes with (come/P.sox they/C here/C (without/T permission/C))
case: case marking (the/M (’s/B school/C grounds/C))
list: list (’s/B mary (:/J list/C (:/B phone/C 555-981/C) (:/B age/C 33/C)))
dislocated: dislocated elements (is/P.scd this/C (our/M office/C) (and/J me/C sam/C))
parataxis: parataxis (left/P.tsi (said/P.s john/C) (the/M guy/C) (in/B early/C (the/M morning/C)))
remnant: remnant in ellipsis (won/P.sor john/C bronze/C (:/B mary/C silver/C))
reparandum: overridden disfluency (go/P.eo (to/T (the/M righ-/C)) (to/T (the/M left/C)))
root: sentence head (is/P.sc (the/M dog/C) cute/C)
dep: unspecified dependency misc.
Table 13: Examples of hyperedges for each grammatical relation in the Universal Dependencies.
31
# Pattern # Cases OIE Pattern
1 (*/B.{ma} */C */C) 32396 -
2 (+/B.{ma} */C */C) 13602 2
3 (*/T */C) 12536 -
4 (*/B.{mm} */C */C) 5331 -
5 (+/B.{mm} */C */C) 5331 2
6 (*/T */R) 2796 -
7 (*/P.{so} */C */C) 1572 1
8 (*/P.{sx} */C */S) 964 3
9 (*/P.{sc} */C */C) 916 1
10 (*/P.{sox} */C */C */S) 861 1
11 (*/P.{ox} */C */S) 631 -
12 (*/P.{px} */C */S) 604 4
13 (*/P.{sr} */C */R) 557 1
14 (*/P.{sox} */C (*/B.{ma} */C */C) */S) 337 1
15 (*/P.{scx} */C */C */S) 322 1
16 (*/P.{so} (*/B.{ma} */C */C) */C) 305 1
17 (*/P.{ox} (*/B.{ma} */C */C) */S) 253 -
18 (*/P.{sr} */C */S) 249 1
19 (*/P.{pa} */C */S) 232 1
20 (*/P.{sc} (*/B.{ma} */C */C) */C) 215 1
21 (*/B.{aa} */C */C) 202 -
22 (+/B.{aa} */C */C) 202 -
23 (*/P.{sx} (*/B.{ma} */C */C) */S) 176 3
24 (*/P.{px} (*/B.{ma} */C */C) */S) 162 4
25 (*/P.{sxr} */C */S */R) 161 3
26 (*/P.{sor} */C */C */R) 158 1
27 (*/P.{pr} */C */R) 150 1
28 (*/P.{sox} (*/B.{ma} */C */C) */C */S) 147 1
29 (*/P.{scx} */C (*/B.{ma} */C */C) */S) 121 5
30 (*/P.{ox} */C */R) 120 -
31 (*/P.sr (*/B.{ma} */C */C) */R) 117 1
32 (*/P.{sxr} */C (*/T */C) */R) 115 3
33 (*/P.{scx} (*/B.{ma} */C */C) */C */S) 111 1
34 (*/P.{sox} */C */C */R) 109 1
35 (*/P.so (+/B.{ma} */C */C) */C) 93 1
36 (*/P.{pax} */C */S */S) 90 1
37 (*/P.{pax} */C (*/T */C) */S) 86 1
38 (*/P.{sc} (*/B.{mm} */C */C) */C) 84 1
39 (*/P.{sc} (+/B.{mm} */C */C) */C) 84 1
40 (*/P.{cx} */C */S) 81 -
41 (*/P.{sxr} */C */S */S) 80 1
42 (*/P.{sr} (*/B.{ma} */C */C) */S) 76 1
43 (*/P.{sx} (+/B.{ma} */C */C) */S) 70 -
44 (*/P.{sxr} */C (*/T */C) */S) 69 3
45 (*/P.{sc} (+/B.{ma} */C */C) */C) 69 1
46 (*/P.{sox} (+/B.{ma} */C */C) */C */S) 68 1
47 (*/P.{scx} */C */C */R) 66 1
48 (*/P.{sx} */C */R) 66 -
49 (*/P.{ox} (+/B.{ma} */C */C) */S) 65 -
50 (*/P.{or} */C */R) 65 -
Table 14: 50 most commons hyperedge patterns found in an SH extracted from a Wikipedia sample, excluding mod-
ifiers and conjunctions and restricting expansions to depth two. Only relations with 2 or 3 arguments are accepted,
and the special builder (+/B) is explicitly considered.
32