Semantic Hypergraphs
Telmo Menezes*1 and Camille Roth†1,2
1Computational Social Science Team, Centre Marc Bloch Berlin (CNRS/HU), Friedrichstr. 191, 10117 Berlin, Germany
2CAMS (Centre Analyse et Mathématique Sociales, UMR 8557 CNRS/EHESS), 54 Bd Raspail, 75007 Paris, France
Abstract
Approaches to Natural language processing (NLP) may
be classified along a double dichotomy open/opaque –
strict/adaptive. The former axis relates to the possibility
of inspecting the underlying processing rules, the latter
to the use of fixed or adaptive rules. We argue that many
techniques fall into either the open-strict or opaque-
adaptive categories. Our contribution takes steps in the
open-adaptive direction, which we suggest is likely to
provide key instruments for interdisciplinary research.
The central idea of our approach is the Semantic Hyper-
graph (SH), a novel knowledge representation model that
is intrinsically recursive and accommodates the natural
hierarchical richness of natural language. The SH model
is hybrid in two senses. First, it attempts to combine the
strengths of ML and symbolic approaches. Second, it is
a formal language representation that reduces but tol-
erates ambiguity and structural variability. We will see
that SH enables simple yet powerful methods of pattern
detection, and features a good compromise for intelligi-
bility both for humans and machines. It also provides
a semantically deep starting point (in terms of explicit
meaning) for further algorithms to operate and collabo-
rate on. We show how modern NLP ML-based building
blocks can be used in combination with a random for-
est classifier and a simple search tree to parse NL to SH,
and that this parser can achieve high precision in a diver-
sity of text categories. We define a pattern language rep-
resentable in SH itself, and a process to discover knowl-
edge inference rules. We then illustrate the efficiency of
the SH framework in a variety of tasks, including con-
junction decomposition, open information extraction,
concept taxonomy inference and co-reference resolu-
tion, and an applied example of claim and conflict anal-
ysis in a news corpus.
Keywords: natural language understanding; knowledge
representation; information extraction; inference sys-
tems; explainable artificial intelligence; hypergraphs
*menezes@cmb.hu-berlin.de ()
†roth@cmb.hu-berlin.de
1 Introduction
Natural language processing (NLP) approaches generally
belong to either one of two main strands, which also of-
ten appear to be mutually exclusive. On the one hand
we essentially have symbolic methods and models which
are open, in the sense that their internal mechanisms as
well as their conclusions are easy to inspect and under-
stand, but which deal with linguistic patterns in a rela-
tively strict manner. On the other hand, we have adap-
tive models based on machine learning (ML) which are
usually opaque to inspection and too complex for their
reasoning to be intelligible, but which achieve increas-
ingly impressive feats that suggest deeper understand-
ing.
Presently, there is a strong research focus on the lat-
ter, and for good reason. Among adaptive models, deep
neural networks, for one, managed to jointly learn and
improve performance in classic NLP tasks such as part-
of-speech tagging, chunking, named-entity recognition,
and semantic role-labeling [as early as 19]. In other
cases, modern ML enabled methods that did not ex-
ist before e.g., estimation of semantic similarity using
word embeddings [45]. More recently, Bidirectional En-
coder Representations from Transformers (BERT) have
shown that pre-trained general models can be fine-
tuned to achieve state-of-the-art performance in specific
language understanding tasks such as question answer-
ing and language inference [21]. Nonetheless, symbolic
methods possess several proper and important features,
namely that they can offer human-readable knowledge
representations of knowledge, as well as language under-
standing through formal and inspectable rule-based log-
ical inference.
Why do we observe this apparent trade-off between
openness and adaptivity? Initial approaches to NLP were
of a symbolic nature, based on rules written by hand,
or in algorithms akin to the ones that are used for pro-
gramming language interpreters and compilers, such as
recursive descent parsers. It became apparent that the
diversity of grammatical constructs that can be found in
natural language is too large to be tackled in such a way.
The problem is compounded by the frequent use of un-
grammatical constructs that are nevertheless frequent in
real-world language usage (e.g. simple mistakes, neol-
1
arXiv:1908.10784v2 [cs.IR] 18 Feb 2021
ogisms, slang). In other words, content in natural lan-
guage is generated by actors that are much more com-
plex, and also more error-prone and error-tolerant than
conventional algorithms. ML is a natural fit for this type
of problem and, as we mentioned, vastly surpasses the
capabilities of human-created symbolic systems in a va-
riety of tasks.
We suggest however that there is a “hidden rela-
tionship” between explicit symbolic manipulation rules
and modern ML: the latter can be seen as a form of
“automatic programming” through large-scale statisti-
cal learning processes, that amount to the generation of
highly complex programs through adaptive pressure in-
stead of human programmers’ efforts. It does not matter
if it is gradient descent on a multi-layered network topol-
ogy, or something more prosaic like entropy reduction
in a decision tree, it is still program generation through
adaptation. The capability of these methods to generate
such complex programs is what allows them to tackle the
complexities of NL, but it is also this very complexity that
makes them opaque.
We can thus imagine a double dichotomy
open/opaque – strict/adaptive. We argue that existing
approaches generally fall into either the open-strict or
opaque-adaptive categories. A few approaches have
ventured into the open-adaptive domain [7, 40] and our
contribution aims at significantly expanding this direc-
tion. Before discussing our approach, let us consider
why open-adaptive is a desirable goal. The work we
present here was performed in the context of a computa-
tional social science (CSS) research team, where NLP is a
scientific instrument capable of assisting in the analysis
of text corpora that are too vast for humans to study
in detail. We argue that further progress in the study
of socio-technical systems and their dynamics could
be enabled by open-adaptive scientific instruments for
language understanding.
In current CSS research, the more common ap-
proaches aim to transform natural language documents
into structured data that can be more easily analyzed
by scholars and are referred to by a variety of umbrella
terms such as “text mining” [69], “automated text anal-
ysis” [29] or “text-as-data methods” [77]. They exhibit
a wide range of sophistication, from simple numerical
statistics to more elaborate ML algorithms. Some meth-
ods indeed rely essentially on scalar numbers, for in-
stance by measuring text similarity (e.g., with cosine dis-
tance [64]) or attributing a valence to text, as in the case
of ideological estimation [63] or sentiment analysis [54],
which in practice may be used to appraise the emotional
content of a text (anger, happiness, disagreement, etc.)
or public sentiment towards political candidates in so-
cial media [76]. Similarly, political positions in docu-
ments may be inferred from so-called “Wordscores” [39]
– a popular method in political science that also relies
on the summation of pre-computed scores for individ-
ual words, and has more refined elaborations, e.g. with
Bayesian regularization [47]. Other methods preserve
the level of words: such is the case with term and pattern
extraction (i.e., discovering salient words through the use
of helper measures like term frequency–inverse docu-
ment frequency (TF-IDF) [60]), so-called “Named Entity
Recognition” [50] (used to identify people, locations, or-
ganizations, and other entities mentioned in corpuses,
for example in news corpora [22] or Twitter streams [57])
and ad-hoc uses of conventional computer science ap-
proaches such as regular expressions to identify chunks
of text matching against a certain pattern (for example,
extracting all p-values from a collection of scientific arti-
cles [17]). Another strand of approaches operates at the
level of word sets, including those geared at topic detec-
tion (such as co-word analysis [37], Latent Dirichlet Allo-
cation (LDA) [12] and TextRank [44], used to extract the
topics addressed in a text) or used for relationship ex-
traction (meant at deriving semantic relations between
entities mentioned in a text, e.g., is(Berlin, City)) [4]. Re-
cent advances in embedding techniques have also made
it possible to describe topics extensionally as clusters of
documents in some properly defined space [5, 33].
Overall, these techniques provide useful approaches
to analyze text corpora at a high level, for example, with
regard to their main entities, relationships, sentiment,
and topics. However, there is limited support to detect,
for instance, more sophisticated claim patterns across
a large volume of texts, what recurring statements are
made about actors or actions, and what are the qual-
itative relationships among actors and concepts. This
type of goal, for example, extends semantic analysis to a
socio-semantic framework [58] which also takes into ac-
count actors who make claims or who are the target of
claims [22].
It is also particularly interesting to consider the
model of knowledge representation that is implicitly
or explicitly associated with the various NLP/text min-
ing/information extraction approaches. To illustrate,
on one extreme we can consider traditional knowledge
bases and semantic graphs, which are open in our sense,
but also limited in their expressiveness and depth. On
the other, we have the extensive knowledge opaquely en-
coded in neural network models such as BERT or GPT-
2/3 [e.g. 15]. Beyond the desirability of open knowledge
bases for their own sake, we propose that a language rep-
resentation that is convenient for both humans and ma-
chines can constitute a lingua franca, through which sys-
tems of cognitive agents of different natures can coop-
erate in a way that is understandable and inspectable.
Such systems could be used to combine the strengths of
symbolic and statistical inference.
The central idea of our approach is the Semantic Hy-
pergraph (SH), a novel knowledge representation model
that is intrinsically recursive and accommodates the nat-
ural hierarchical richness of NL. The SH model is hybrid
2
in two senses. First, it attempts to combine the strengths
of ML and symbolic approaches. Second, it is a formal
language representation that reduces but tolerates ambi-
guity, and that also reduces structural variability. We will
see that SH enables simple methods of pattern detec-
tion to be more powerful and less brittle, that it is a good
compromise for intelligibility both for humans and ma-
chines, and that it provides a semantically deeper start-
ing point (in terms of explicit meaning) for further algo-
rithms to operate and collaborate on.
In the next section we discuss the state of the art,
comparing SH to a number of approaches from various
fields and eras. We then describe the structure and syn-
tax of SH, followed by an explanation on how modern
and standard NLP ML-based building blocks provided
by an open source software library [31] can be used in
combination with a random forest classifier and a sim-
ple search tree to parse NL to SH. Here we also pro-
vide precision benchmarks of our current parser, which
is then employed in the experiments that follow. We at-
tempted to perform a set of experiments of a rather di-
verse nature, to gather evidence of SH usefulness in a
variety of roles, and of its potential to tackle the chal-
lenge that we started by stating in this introduction, and
to gather empirical insights. One important language
understanding task is information extraction from text.
One formulation of such a task that attracts significant
attention is that of Open Information Extraction (OIE)
— the domain-free extraction from text of tuples (typ-
ically triplets) representing semantic relationships [24].
We will show that a small and simple set of SH patterns
can produce competitive results in an OIE benchmark,
when pitted against more complex and specialized sys-
tems in that domain. We will demonstrate concept tax-
onomy inference and co-reference resolution, followed
by claim and conflict identification in a database of news
headers. We will show how SH can be used to generate
semantically rich visual summaries of text.
2 Related Work
Knowledge bases. As a knowledge representation for-
malism, it is interesting to compare SH with traditional
approaches. Let us start with triplet-based ones. For
example, the Semantic Web [10, 62] community tends
to use standards such as RDFa [1], which represent
knowledge as subject-predicate-object expressions, and
are conceptually equivalent to semantic graphs [3, 66]
(similarly, a particular type of hypergraph has been used
in [16] to represent tagged resources by users, yet this
also reduces to fixed triplet conceptualization). Despite
their usefulness for simple cases, such approaches can-
not hope to match the semantic sophistication of what
can be conveyed with open text. Binary relationships
and lack of recursion limit the expressive power of se-
mantic graphs, and we sill see how SHs can represent se-
mantic information that is lost in the graphic represen-
tation, for example the ability to express n-ary relation-
ships, propositions about propositions and constructive
definitions of concepts.
A further type of approaches relying on knowledge
bases is epitomized by the famous Cyc [36] project, a
multi-decade enterprise to build a general-purpose and
comprehensive system of concepts and rules. It is an im-
pressive effort, nevertheless hindered by the limitations
that we alluded to in the previous section concerning
the ambiguity and diversity of semantic structures con-
tained in NL, given that it relies purely on symbolic rea-
soning. Cyc belongs to a category of systems that are
mostly concerned with question answering, a different
aim that the one of the work that we propose here, which
is more concerned with aiding in the analysis and sum-
marization of large corpora of text for research purposes,
especially in the social sciences, while not requiring full
disambiguation of meaning nor perfect reasoning or un-
derstanding.
Several other notable knowledge bases of a similar se-
mantic graph nature have been developed, some relying
on collaborative human efforts to gather ground asser-
tions, for example MIT’s ConceptNet [68], ATOMIC [61]
from the Allen Institute, or very rigorous scholarly ef-
forts of annotation, as is the case with WordNet [46]
and its multiple variants, or relying on wiki-like plat-
forms such as WikiData [75], or mining relationship
from Wikipedia proper, as is the case with DBPedia [6],
and more recently a transformer language model has
been proposed to automatically extend common-sense
knowledge bases [14]. We envision that such general-
knowledge bases could be fruitfully integrated with SHs
for various purposes, but such endeavours are beyond
the scope of this work. We are instead interested in
demonstrating what can be achieve by going beyond
such non-hypergraphic appraches.
Hypergraphic approaches to knowledge representa-
tion. Hypergraphs have been proposed already in the
1970s as a general solution for knowledge representa-
tion [13]. More recently, Ben Goertzel produced simi-
lar insights [28], and in fact included an hypergraphic
database called AtomSpace as the core knowledge rep-
resentation of his OpenCog framework [30], an attempt
to make Artificial General Intelligence emerge from the
interaction of a collection of heterogeneous system. As
is the case with Cyc, the goals of OpenCog are however
quite distinct from the aim of our work.
A model that shares similarities with ours but purely
aims at solving a meaning matching problem is that
of Abstract Meaning Representation (AMR) [7]. AMR is
based on PropBank verbal propositions and their ar-
guments [53], ensuring that all such meaning struc-
tures can be represented. SH completeness is based in-
stead on Universal Dependencies [52], ensuring instead
3
that all cataloged grammatical constructs can be repre-
sented. AMR’s goal is to purely abstract meaning, while
SH accommodates the ambiguity of the original NL ut-
terances, bringing several important benefits: it makes
their computational processing tractable in further ways,
tolerates mistakes better and preserves communication
nuance that would otherwise be lost. Furthermore, it
remains open to structures that may not be currently
envisioned. Parsing AMR to NL is a particularly hard
task and, to our knowledge, there is currently no parser
that approaches the capabilities of what we will demon-
strate in this work. In part, this is a practical problem:
we will see how we can take advantage of intermediary
NLP tasks that are well studied and developed to achieve
NL to SH parser. Doing the same for AMR requires the
construction of training data by extensive annotation ef-
forts by humans. It could be argued that this is still a
preferable goal, no matter how distant, given that AMR
removes all ambiguity from statements. Here we point
out that this aspect of AMR is also a downside, firstly be-
cause it makes all failures of understanding catastrophic
(we will see how this is not the case for SH), and secondly
because NL is inherently ambiguous. It is often the case
that even human beings cannot fully resolve ambigui-
ties, or that an ambiguous statement gains importance
later on, with more information. We aim to define SH as
a lingua franca for the collaboration of human an algo-
rithmic actors of several natures, a less rigid goal than the
one embodied by AMR.
Free text parsing. A classical NLP task is that of mak-
ing explicit the grammatical structure of a sentence in
the form of a parse tree. A particularly common type of
such a tree in current use is the Dependency Parse Tree
(DPT), based on dependency grammars. We will see that
our own parser takes advantage of DPTs (among other
high-level grammatical / linguistic features) as interme-
diary steps, but it is also interesting to notice that DPTs
themselves can be considered as a type of hypergraphic
representation of language [56]. In fact, as we will discuss
below, they are already employed in various targeted lan-
guage understanding tasks in a CSS context.
From the perspective of hypergraphic representation
of language, the fundamental difference between DPTs
and SHs is that the former aims at expressing the gram-
matical structure of language, while the latter its seman-
tic structure, in the simplest possible way that enables
meaning extraction in a principled and predictable way.
In contrast to the ad-hoc nature of information extrac-
tion from DPTs, we will see that SHs structure NL in a way
akin to functional computer languages, and allow for ex-
ample for a generic methodology of extracting patterns.
The expressive power of such patterns will be demon-
strated in several ways, namely by demonstrating com-
petitive results in a standard Open Information Extrac-
tion task. We will see that the type system of SHs (relying
on 8 types) is much simpler than the diversity of gram-
matical roles contained in a typical set of dependency la-
bels (such as Universal Dependencies), and we will also
provide empirical evidence that SHs are not isomorphic
to DPTs.
In the realm of OIE, one approach in particular with
which our work shares some similarities is that of learn-
ing open pattern templates [40]. These pattern templates
combine at the same symbolic level dependency parse
labels and structure, part-of-speech tags, explicit lexical
constraints and higher-order inferences (e.g. that some
term refers to a person), to achieve sophisticated lan-
guage understanding in the extraction of OIE tuples, be-
ing able to extract relations that are not only of a verbal
nature, and demonstrating sensitivity to context. The
work we will present does not attempt to directly com-
bine diverse linguistic features at the service of a spe-
cific language understanding task. Instead, we propose
to use such features to aid in the translation of NL into
a structured representation, which relies by comparison
on a very simple and uniform type system, and from
which complex NL understanding tasks become easier,
and that is of general applicability to a diversity of such
tasks, while remaining fully readable and understand-
able by humans. Furthermore, it defines a system of
knowledge representation in itself, that is directly fo-
cused on meaning instead of grammar.
Text mining. We have already covered in the previ-
ous section the most commonly used text mining ap-
proaches, while emphasizing the relative lack of sophis-
tication in understanding text meaning. The need for
such sophistication is all the more pregnant for social
sciences. On the one hand, qualitative social science
methods of text analysis do not scale to the enormous
datasets that are now available. Furthermore, quantita-
tive approaches allow for other types of analysis that are
enriching and complementary to qualitative research,
yet may simplify extensively the processing in such a way
that it hinders their adoption by scholars used to the re-
finement of qualitative approaches. And the more so-
phisticated the NLP techniques become, the further they
tend to be from being used for large-scale text analy-
sis purposes. Indeed, these systems are fast and accu-
rate enough to form a starting point for more advanced
computer-supported analysis in a CSS context, and they
enable approaches that are substantially more sophis-
ticated than the text mining state of the art discussed
above. Yet, the results of such systems may seem rela-
tively simplistic compared to human-level understand-
ing of natural language.
The literature already features some works which at-
tempt at going beyond language models based on word
distributions (such as bags of words, co-occurrence clus-
ters, or so-called “topics”) or triplets. For instance, State-
ment Map [49] is aimed at mining the various viewpoints
4
expressed around a topic of interest in the web. Here
a notion of claim is employed. A statement provided
by the user is compared against statements from a cor-
pus of text extracted from various web sources. Text
alignment techniques are used to match statements that
are likely to refer to the same issue. A machine learn-
ing model trained over NLP-annotated chunks of text
classifies pairs of claims as “agreement”, “conflict”, “con-
finement” and “evidence”. More broadly, the subfield
of argumentation mining [38] also makes extensive use
of machine learning and statistical methods to extract
portions of text corresponding to claims, arguments and
premises. These approaches generally rely on surface
linguistic features, there is however an increasing trend
of dealing with structured and relational data. Already in
2008, [73] proposed a system to extract binary semantic
relationships from Dutch newspaper articles. A recent
work [59] presents a system aimed at analysing claims in
the context of climate negotiations. It leverages depen-
dency parse trees and general ontologies [70] to extract
tuples of the form: 〈actor, predicate, negotiation_point〉
where the actors are stakeholders (e.g., countries), the
predicates express agreement, opposition or neutrality
and the negotiation point is identified by chunk of text.
Similarly, in another recent work [74], parse trees are
used to automatically extract source-subject-predicate
clauses in the context of news reporting over the 2008-
2009 Gaza war, and used to show differences in citation
and framing patterns between U.S. and Chinese sources.
These works help demonstrate the feasibility of using
parse trees and other modern NLP techniques to iden-
tify viewpoints and extract more structured claims from
text. Being a step forward from pure bag-of-words analy-
sis, they still leave out a considerable amount of informa-
tion contained in natural language texts, namely by rely-
ing on topic detection, or on pre-defined categories, or
on working purely on source-subject-predicate clauses.
We propose to introduce a more sophisticated language
model, where all entities participating in a statement are
identified, where entities can be described as combina-
tions of other entities, and where statements can be enti-
ties themselves, allowing for claims about claims, or even
claims about claims about claims. The formal backbone
of this model consists of an extended type of hypergraph
that is both recursive and directed, thus generalizing se-
mantic graphs and inducing powerful representation ca-
pabilities.
3 Semantic hypergraphs – structure
and syntax
3.1 Structure
The SH model is essentially a recursive, ordered hyper-
graph that makes the structure contained in natural lan-
guage (NL) explicit. On one hand, NL is recursive, al-
lowing for concepts constructed from other concepts as
well as statements about statements, and on the other
hand, it can express n-ary relationships. We will see how
a hypergraphic formalism provides a satisfactory struc-
ture for NL constructs.
While a graph G = (V, E ) is based on a vertex set V
and an edge set E ⊂ V × V describing dyadic connec-
tions, a hypergraph [8, 9] generalizes such structure by
allowing n-ary connections. In other words, it can be de-
fined as H = (V, E ), where V is again a vertex set yet E
is a set of hyperedges (ei )i ∈1..M connecting an arbitrary
number of vertices. Formally, ei = {v1, ...vn } ∈ E = P (V ).
We further generalize hypergraphs in two ways: hyper-
edges may be ordered [23] and recursive [32]. Ordering
entails that the position in which a vertex participates
in the hyperedge is relevant (as is the case with directed
graphs). Recursivity means that hyperedges can partici-
pate as vertices in other hyperedges. The corresponding
hypergraph may be defined as H = (V, E ) where E ⊂ EV
the recursive set of all possible hyperedges generated
by V : EV = {(ei )i ∈{1..n} | n ∈ N, ∀i ∈ {1..n}, ei ∈ V ∪ EV
}. In
this sense, V configures a set of irreducible hyperedges
