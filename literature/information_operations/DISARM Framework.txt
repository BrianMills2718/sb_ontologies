
DISARM Objects

DISARM is a set of frameworks for describing and understanding disinformation incidents. Learn more about DISARM
DISARM Objects
Framework objects:

    Frameworks
    Phases: A phase is the highest-level grouping of tactics and their associated techniques, corresponding to a logical stage in the execution of an influence campaign.
    Tactics: A tactic is a way or means by which the objectives (ends) are met, and which does not prescribe either the specific techniques or procedures used to achieve it.
    Techniques: Techniques are the “how” of a particular tactic. Techniques are associated with one or more tactics, as a particular technique may be used to accomplish different goals. Techniques may also be formulated as what an actor gains by doing something. Like Tactics, a Technique name should always begin with a simple present tense action verb. Techniques can be combined to create procedures.
    Tasks
    Countermeasures
    Detections
    Responsetypes
    Metatechniques
    Playbooks
    Resources

Datasets:

    Incidents
    Examples
    External Groups
    Tools


DISARM Update: Version 1.3
Adam
Adam
14 min read
·
Sep 18, 2023

Overview

We’re updating the Red Framework to provide a wider range of Objectives for analysts to pick from when categorising their campaigns, along with introducing some consistency updates, and changes requested by the community.

We’re also introducing the DISARM Navigator and Tagger; two new tools which should help analysts more efficiently benefit from DISARM’s Frameworks.

Finally, we’re revealing plans for upcoming updates to the DISARM Red Framework.
Improvements to Objective Tracking in DISARM Red

Here we’re going to explain why we felt it was important to make these changes. Check out the “Full Patch Notes” at the bottom of the post if you’re just interested in what’s new.

Previously, analysts had the following options under TA02: Plan Objectives:

    T0002: Facilitate State Propaganda
    T0066: Degrade Adversary
    T0075: Dismiss
    — T0075.001: Discredit Credible Sources
    T0076: Distort
    T0077: Distract
    T0078: Dismay
    T0079: Divide

The 5 Ds work great for documenting narrative devices, but don’t give us much insight into what actors’s Objectives are; they’re too abstract and not pragmatic enough to describe the goals of influence operations (actors don’t Distort for the sake of Distorting, they do it to achieve an ulterior motive), and in a future update we’ll move the 5 Ds to live under a more appropriate Tactic. This leaves only a few Techniques remaining, which aren’t enough to enable analysts to benefit from tagging actors’ objectives.

To fix this problem, we’re adding the following Tactics to TA02: Plan Objectives:

    Cultivate Support
    Make Money
    Undermine
    Cause Harm
    Motivate to Act
    Dissuade from Acting

Each Technique has their own Sub-Techniques to allow analysts to further delineate between campaigns’ perceived objectives. For example, Cultivate Support contains the following Sub-Techniques:

    Recruit Members
    Energise Supporters
    Boost Reputation
    Defend Reputation
    Justify Action
    Increase Prestige
    Cultivate Support for Ally
    Cultivate Support for Policy

We identified these Techniques and Sub-Techniques by reviewing literature on common objectives of Influence Operations, and associating them with real-world campaigns.
Community Requests

We’re introducing our first updates based on feedback from the community! We had requests to fix issues with existing Framework items under TA07: Select Channels and Affordances:

    T0104.002: Dating Apps mistakenly displayed the Summary intended for T0103.001: Video Livestream.
    T0104.003: Private/Closed Social Networks mistakenly displayed the Summary intended for T0103.002: Audio Livestream.

These issues have been resolved, and the Summaries have been updated to include example platforms. To ensure that the Techniques were mutually exclusive, LinkedIn was redefined to a Private/Closed Social Network from its previous categorisation as a Mainstream Social Network.

We were also asked to expand the scope of T0128: Conceal People under TA11: Persist in the Information Environment so that the Technique can be used to catalogue concealment of a wider variety of operational assets:

    T0128: Conceal People has been updated to T0128: Conceal Information Assets
    T0128.005: Change Names of Accounts has been updated to T0128.005: Change Names of Information Assets
    T0128.004: Launder Accounts has been updated to T0128.004: Launder Information Assets

Please reach out to us with any suggestions you have for updates to DISARM’s Frameworks; we love to hear from the community and know the path to the best possible Framework is one where we work together to build something great.
Other Framework Updates

We’re standardising the Framework to use British English, and title case in items’ names. We’re also adding Sub-Techniques to T0074: Determine Strategic Ends:

    Domestic Political Advantage
    Geopolitical Advantage
    Economic Advantage
    Ideological Advantage

These Sub-Techniques were initially designed to be classified as Objectives, but felt better suited to Strategic Ends after review. We plan to make further improvements to TA01: Plan Strategy, including moving T0073: Determine Target Audiences to live under TA13: Target Audience Analysis.
Future Framework Updates

We have lots of plans for improvements to DISARM’s Frameworks, including:

    More community driven change requests
    The addition of more Incidents to the Frameworks, with detailed explanations of how they match each Technique (with the goal of supporting a shared understanding with practical examples)
    Further improvements to TA01: Plan Strategy, and TA13: Target Audience Analysis to enhance the granularity of techniques and enable more tailored defensive actions
    Further changes to the DISARM Red Framework with the goal of making it less initially overwhelming, while also encompassing more actor behaviours through a focus on increasing the number of Sub-Techniques while paring back the top-level Techniques.

If you have any changes which you think would be valuable to include in our upcoming updates, please reach out to us!
The DISARM Navigator

We have modified the ATT&CK Navigator to allow for viewing DISARM’s Red Framework, which you can access here.
The DISARM Navigator

To view the Framework in the Navigator, select Create a new empty layer and click DISARM.
Create a new empty layer > DISARM

By default all Sub-Techniques are hidden behind their parent Techniques, which we hope provides a more manageable experience when first accessing the Framework (Sub-Techniques can be drawn out using the Layer Controls toolbar in the top right of the page). The Navigator also introduces the ability to search across Framework Items’ names and descriptions, which makes finding the Technique you’re looking for a LOT easier.
The search feature and Layer Controls toolbar

You can also easily identify which behaviours you’ve already tagged by adding scoring or colouration to selected Techniques. For advanced users scoring also enables comparison of different campaigns, to help identification of behaviours common across influence operations.

All of these features combined should make for a much more approachable and user-friendly experience for navigating the Framework, while also helping analysts produce valuable insights for selecting efficient strategic defender actions. The Navigator is available for use now, but we will also be producing tutorials in the near future if you’d prefer to wait for guided help before diving in.
The DISARM Tagger

A tool developed in-house to support tagging of reports with DISARM Techniques is being made publicly available on DISARM’s GitHub repository. The Tagger adds an embedded toolbar to Microsoft Word’s ribbon, enabling analysts to search for Techniques to tag highlighted text in their reports, while also generating a summary table displaying all tagged Techniques and associated evidence at the end of the document.

We hope this tool enables analysts to spend more time analysing and less time tagging. In future releases we will update the Tagger to enable its usage in other software used by analysts.
Full Patch Notes

3 Updated Techniques, 7 Updated Sub-Techniques, 5 New Techniques, 36 New Sub-Techniques

TA01: Plan Strategy
1 Updated Technique, 4 New Sub-Techniques

T0074: Determine Strategic Ends: (Updated Summary) These are the long-term end-states the campaign aims to bring about. They typically involve an advantageous position vis-a-vis competitors in terms of power or influence. The strategic goal may be to improve or simply to hold one’s position. Competition occurs in the public sphere in the domains of war, diplomacy, politics, economics, and ideology, and can play out between armed groups, nation-states, political parties, corporations, interest groups, or individuals.

(New Sub-Technique) Domestic Political Advantage: Favourable position vis-à-vis national or sub-national political opponents such as political parties, interest groups, politicians, candidates.

(New Sub-Technique) Geopolitical Advantage: Favourable position on the international stage in terms of great power politics or regional rivalry. Geopolitics plays out in the realms of foreign policy, national security, diplomacy, and intelligence. It involves nation-state governments, heads of state, foreign ministers, intergovernmental organisations, and regional security alliances.

(New Sub-Technique) Economic Advantage: Favourable position domestically or internationally in the realms of commerce, trade, finance, industry. Economics involves nation-states, corporations, banks, trade blocs, industry associations, cartels.

(New Sub-Technique) Ideological Advantage: Favourable position domestically or internationally in the market for ideas, beliefs, and world views. Competition plays out among faith systems, political systems, and value systems. It can involve sub-national, national or supra-national movements.

—

TA02: Plan Objectives
1 Updated Technique, 5 New Techniques, 26 New Sub-Techniques

(New Technique) Cultivate Support: Grow or maintain the base of support for the actor, ally, or action. This includes hard core recruitment, managing alliances, and generating or maintaining sympathy among a wider audience, including reputation management and public relations. Sub-techniques assume support for actor (self) unless otherwise specified.

(New Sub-Technique) Cultivate Support: Motivate followers to join or subscribe as members of the team. Organisations may mount recruitment drives that use propaganda to entice sympathisers to sign up.

(New Sub-Technique) Energise Supporters: Raise the morale of those who support the organization or group. Invigorate constituents with zeal for the mission or activity. Terrorist groups, political movements, and cults may indoctrinate their supporters with ideologies that are based on warped versions of religion or cause harm to others.

(New Sub-Technique) Boost Reputation: Elevate the estimation of the actor in the public’s mind. Improve their image or standing. Public relations professionals use persuasive overt communications to achieve this goal; manipulators use covert disinformation.

(New Sub-Technique) Defend Reputation: Preserve a positive perception in the public’s mind following an accusation or adverse event. When accused of a wrongful act, an actor may engage in denial, counter accusations, whataboutism, or conspiracy theories to distract public attention and attempt to maintain a positive image.

(New Sub-Technique) Justify Action: To convince others to exonerate you of a perceived wrongdoing. When an actor finds it untenable to deny doing something, they may attempt to exonerate themselves with disinformation which claims the action was reasonable. To be vindicated is a special case of the objective “Defend Reputation”.

(New Sub-Technique) Increase Prestige: Improve personal standing within a community. Gain fame, approbation, or notoriety. Conspiracy theorists, those with special access, and ideologues can gain prominence in a community by propagating disinformation, leaking confidential documents, or spreading hate.

(New Sub-Technique) Cultivate Support for Ally: Elevate or fortify the public backing for a partner. Governments may interfere in other countries’ elections by covertly favoring a party or candidate aligned with their interests. They may also mount an influence operation to bolster the reputation of an ally under attack.

(New Sub-Technique) Cultivate Support for Initiative: Elevate or fortify the public backing for a policy, operation, or idea. Domestic and foreign actors can use artificial means to fabricate or amplify public support for a proposal or action.

—

(New Technique) Make Money: Profit from disinformation, conspiracy theories, or online harm. In some cases, the sole objective is financial gain, in other cases the objective is both financial and political. Making money may also be a way to sustain a political campaign.

(New Sub-Technique) Extort: Coerce money or favors from a target by threatening to expose or corrupt information. Ransomware criminals typically demand money. Intelligence agencies demand national secrets. Sexual predators demand favors. The leverage may be critical, sensitive, or embarrassing information.

(New Sub-Technique) Raise Funds: Solicit donations for a cause. Popular conspiracy theorists can attract financial contributions from their followers. Fighting back against the establishment is a popular crowdfunding narrative.

(New Sub-Technique) Generate Ad Revenue: Earn income from digital advertisements published alongside inauthentic content. Conspiratorial, false, or provocative content drives internet traffic. Content owners earn money from impressions of or clicks on or conversions of ads published on their websites, social media profiles, or streaming services, or ads published when their content appears in search engine results. Fraudsters simulate impressions, clicks, and conversions, or they spin up inauthentic sites or social media profiles just to generate ad revenue. Conspiracy theorists and political operators generate ad revenue as a byproduct of their operation or as a means of sustaining their campaign.

(New Sub-Technique) Scam: Defraud a target or trick a target into doing something that benefits the attacker. A typical scam is where a fraudster convinces a target to pay for something without the intention of ever delivering anything in return. Alternatively, the fraudster may promise benefits which never materialize, such as a fake cure. Criminals often exploit a fear or crisis or generate a sense of urgency. They may use deepfakes to impersonate authority figures or individuals in distress.

(New Sub-Technique) Sell Items under False Pretences: Offer products for sale under false pretenses. Campaigns may hijack or create causes built on disinformation to sell promotional merchandise. Or charlatans may amplify victims’ unfounded fears to sell them items of questionable utility such as supplements or survival gear.

(New Sub-Technique) Manipulate Stocks: Artificially inflate or deflate the price of stocks or other financial instruments and then trade on these to make profit. The most common securities fraud schemes are called “pump and dump” and “poop and scoop”.

—

(New Technique) Undermine: Weaken, debilitate, or subvert a target or their actions. An influence operation may be designed to disparage an opponent; sabotage an opponent’s systems or processes; compromise an opponent’s relationships or support system; impair an opponent’s capability; or thwart an opponent’s initiative.

(New Sub-Technique) Thwart: Prevent the successful outcome of a policy, operation, or initiative. Actors conduct influence operations to stymie or foil proposals, plans, or courses of action which are not in their interest.

(New Sub-Technique) Smear: Denigrate, disparage, or discredit an opponent. This is a common tactical objective in political campaigns with a larger strategic goal. It differs from efforts to harm a target through defamation.

(New Sub-Technique) Subvert: Sabotage, destroy, or damage a system, process, or relationship. The classic example is the Soviet strategy of “active measures” involving deniable covert activities such as political influence, the use of front organizations, the orchestration of domestic unrest, and the spread of disinformation.

(New Sub-Technique) Polarise: To cause a target audience to divide into two completely opposing groups. This is a special case of subversion. To divide and conquer is an age old approach to subverting and overcoming an enemy.

—

(New Technique) Cause Harm: Persecute, malign, or inflict pain upon a target. The objective of a campaign may be to cause fear or emotional distress in a target. In some cases, harm is instrumental to achieving a primary objective, as in coercion, repression, or intimidation. In other cases, harm may be inflicted for the satisfaction of the perpetrator, as in revenge or sadistic cruelty.

(New Sub-Technique) Spread Hate: Publish and/or propagate demeaning, derisive, or humiliating content targeting an individual or group of individuals with the intent to cause emotional, psychological, or physical distress. Hate speech can cause harm directly or incite others to harm the target. It often aims to stigmatize the target by singling out immutable characteristics such as color, race, religion, national or ethnic origin, gender, gender identity, sexual orientation, age, disease, or mental or physical disability. Thus, promoting hatred online may involve racism, antisemitism, Islamophobia, xenophobia, sexism, misogyny, homophobia, transphobia, ageism, ableism, or any combination thereof. Motivations for hate speech range from group preservation to ideological superiority to the unbridled infliction of suffering.

(New Sub-Technique) Defame: Attempt to damage the target’s personal reputation by impugning their character. This can range from subtle attempts to misrepresent or insinuate, to obvious attempts to denigrate or disparage, to blatant attempts to malign or vilify. Slander applies to oral expression. Libel applies to written or pictorial material. Defamation is often carried out by online trolls.

(New Sub-Technique) Intimidate: Coerce, bully, or frighten the target. An influence operation may use intimidation to compel the target to act against their will. Or the goal may be to frighten or even terrify the target into silence or submission. In some cases, the goal is simply to make the victim suffer.

—

(New Technique) Motivate to Act: Persuade, impel, or provoke the target to behave in a specific manner favorable to the attacker. Some common behaviors are joining, subscribing, voting, buying, demonstrating, fighting, retreating, resigning, boycotting.

(New Sub-Technique) Compel: Force target to take an action or to stop taking an action it has already started. Actors can use the threat of reputational damage alongside military or economic threats to compel a target

(New Sub-Technique) Provoke: Instigate, incite, or arouse a target to act. Social media manipulators exploit moral outrage to propel targets to spread hate, take to the streets to protest, or engage in acts of violence.

(New Sub-Technique) Encourage: Inspire, animate, or exhort a target to act. An actor can use propaganda, disinformation, or conspiracy theories to stimulate a target to act in its interest.

—

(New Technique) Dissuade from Acting: Discourage, deter, or inhibit the target from actions which would be unfavourable to the attacker. The actor may want the target to refrain from voting, buying, fighting, or supplying.

(New Sub-Technique) Deter: Prevent target from taking an action for fear of the consequences. Deterrence occurs in the mind of the target, who fears they will be worse off if they take an action than if they don’t. When making threats, aggressors may bluff, feign irrationality, or engage in brinksmanship.

(New Sub-Technique) Silence: Intimidate or incentivise target into remaining silent or prevent target from speaking out. A threat actor may cow a target into silence as a special case of deterrence. Or they may buy the target’s silence. Or they may repress or restrict the target’s speech.

(New Sub-Technique) Discourage: To make a target disinclined or reluctant to act. Manipulators use disinformation to cause targets to question the utility, legality, or morality of taking an action.

—

TA07: Select Channels and Affordances
5 Updated Sub-Techniques

T0103.002: Dating Apps: (Updated Summary) Examples include Tinder, Bumble, Tantan, Badoo, Plenty of Fish, hinge, LOVOO, OkCupid, happn, Mamba.

T0103.001: Video Livestream: (Updated Summary) A video livestream refers to an online video broadcast capability that allows for real-time communication to closed or open networks. Generic examples include Facebook Live, YouTube Live, TikTok Live, Instagram Live, Douyu, 17LIVE. Video game streaming examples include Twitch, Facebook Gaming, YouTube Gaming.

T0103.002: Audio Livestream: (Updated Summary) An audio livestream refers to an online audio broadcast capability that allows for real-time communication to closed or open networks. This includes internet radio stations such as TuneIn Radio, iHeartRadio, Sirius XM; podcasts available on music streaming platforms such as Spotify, Pandora, Apple Podcasts, Google Podcasts, and Amazon Music; and social audio services such as Twitter (X) Spaces, Clubhouse, Reddit Talk, Facebook Live Audio Rooms, LinkedIn Live Rooms, and Fireside

T0104.003: Private/Closed Social Networks: (Updated Summary) Social networks that are not open to people outside of family, friends, neighbors, or co-workers. Non-work-related examples include Couple, FamilyWall, 23snaps, and Nextdoor. Some of the larger social network platforms enable closed communities: examples are Instagram Close Friends and Twitter (X) Circle. Work-related examples of private social networks include LinkedIn, Facebook Workplace, and enterprise communication platforms such as Slack or Microsoft Teams

T0104.001: Mainstream Social Networks: (Updated Summary) Examples include Facebook, Twitter (X), Weibo, VKontakte (VK), Odnoklassniki

—

TA11: Persist in the Information Environment
1 Updated Technique, 2 Updated Sub-Techniques

(Updated Name) T0128: Conceal Information Assets: (Updated Summary) Conceal the identity or provenance of campaign information assets such as accounts, channels, pages etc. to avoid takedown and attribution.

(Updated Name) T0128.005: Change Names of Information Assets: (Updated Summary) Changing names or brand names of information assets such as accounts, channels, pages etc. An operation may change the names or brand names of its assets throughout an operation to avoid detection or alter the names of newly acquired or repurposed assets to fit operational narratives.

(Updated Name) T0128.004: Launder Information Assets: (Updated Summary) Laundering occurs when an influence operation acquires control of previously legitimate information assets such as accounts, channels, pages etc. from third parties through sale or exchange and often in contravention of terms of use. Influence operations use laundered assets to reach target audience members from within an existing information community and to complicate attribution.


DISARM Framework Explorer

Welcome to DISARM

DISARM is a set of frameworks for describing and understanding disinformation incidents.
Learn more about DISARM
DISARM Objects
The disarm frameworks contain many object types, including tactic stages (steps in an incident), and techniques (activities at each tactic stage). We also have data objects to show how the frameworks are used in practice, and to make our datasets on tools and responders available.
Framework objects
Frameworks 	Phases 	Tactics 	Techniques 	Tasks 	Countermeasures
Detections 	Responsetypes 	Metatechniques 	Playbooks 	Resources
Data objects
Incidents 	Examples 	External Groups 	Tools
Disarm objects are described in detail here.
DISARM Frameworks

Frameworks are organised ways of describing and analysing disinformation behaviours. DISARM has two main frameworks: DISARM Red, for describing incident creator behaviours, and DISARM Blue, to describe potential response behaviours.

DISARM Red Framework - incident creator TTPs
PLAN	PREPARE	EXECUTE	ASSESS
TA01:
Plan
Strategy	TA02:
Plan
Objectives	TA13:
Target
Audience
Analysis	TA14:
Develop
Narratives	TA06:
Develop
Content	TA15:
Establish
Social
Assets	TA16:
Establish
Legitimacy	TA05:
Microtarget	TA07:
Select
Channels
and
Affordances	TA08:
Conduct
Pump
Priming	TA09:
Deliver
Content	TA17:
Maximize
Exposure	TA18:
Drive
Online
Harms	TA10:
Drive
Offline
Activity	TA11:
Persist
in
the
Information
Environment	TA12:
Assess
Effectiveness
T0073:
Determine
Target
Audiences	T0002:
Facilitate
State
Propaganda	T0072:
Segment
Audiences	T0003:
Leverage
Existing
Narratives	T0015:
Create
hashtags
and
search
artifacts	T0007:
Create
Inauthentic
Social
Media
Pages
and
Groups	T0009:
Create
fake
experts	T0016:
Create
Clickbait	T0029:
Online
polls	T0020:
Trial
content	T0114:
Deliver
Ads	T0049:
Flooding
the
Information
Space	T0047:
Censor
social
media
as
a
political
force	T0017:
Conduct
fundraising	T0059:
Play
the
long
game	T0132:
Measure
Performance
T0074:
Determine
Strategic
Ends	T0066:
Degrade
Adversary	T0072.001:
Geographic
Segmentation	T0004:
Develop
Competing
Narratives	T0019:
Generate
information
pollution	T0010:
Cultivate
ignorant
agents	T0009.001:
Utilize
Academic/Pseudoscientific
Justifications	T0018:
Purchase
Targeted
Advertisements	T0043:
Chat
apps	T0039 :
Bait
legitimate
influencers	T0114.001:
Social
media	T0049.001:
Trolls
amplify
and
manipulate	T0048:
Harass	T0017.001:
Conduct
Crowdfunding
Campaigns	T0060:
Continue
to
Amplify	T0132.001:
People
Focused
	T0075:
Dismiss	T0072.002:
Demographic
Segmentation	T0022:
Leverage
Conspiracy
Theory
Narratives	T0019.001:
Create
fake
research	T0013:
Create
inauthentic
websites	T0011:
Compromise
legitimate
accounts	T0101:
Create
Localized
Content	T0043.001:
Use
Encrypted
Chat
Apps	T0042:
Seed
Kernel
of
truth	T0114.002:
Traditional
Media	T0049.002:
Hijack
existing
hashtag	T0048.001:
Boycott/"Cancel"
Opponents	T0057:
Organize
Events	T0128:
Conceal
People	T0132.002:
Content
Focused
	T0075.001:
Discredit
Credible
Sources	T0072.003:
Economic
Segmentation	T0022.001:
Amplify
Existing
Conspiracy
Theory
Narratives	T0019.002:
Hijack
Hashtags	T0014:
Prepare
fundraising
campaigns	T0097:
Create
personas	T0102:
Leverage
Echo
Chambers/Filter
Bubbles	T0043.002:
Use
Unencrypted
Chats
Apps	T0044:
Seed
distortions	T0115:
Post
Content	T0049.003:
Bots
Amplify
via
Automated
Forwarding
and
Reposting	T0048.002:
Harass
People
Based
on
Identities	T0057.001:
Pay
for
Physical
Action	T0128.001:
Use
Pseudonyms	T0132.003:
View
Focused
	T0076:
Distort	T0072.004:
Psychographic
Segmentation	T0022.002:
Develop
Original
Conspiracy
Theory
Narratives	T0023:
Distort
facts	T0014.001:
Raise
funds
from
malign
actors	T0097.001:
Backstop
personas
	T0102.001:
Use
existing
Echo
Chambers/Filter
Bubbles	T0103:
Livestream	T0045:
Use
fake
experts	T0115.001:
Share
Memes	T0049.004:
Utilize
Spamoflauge	T0048.003:
Threaten
to
Dox	T0057.002:
Conduct
Symbolic
Action	T0128.002:
Conceal
Network
Identity	T0133:
Measure
Effectiveness
	T0077:
Distract	T0072.005:
Political
Segmentation	T0040:
Demand
insurmountable
proof	T0023.001:
Reframe
Context	T0014.002:
Raise
funds
from
ignorant
agents	T0098:
Establish
Inauthentic
News
Sites	T0102.002:
Create
Echo
Chambers/Filter
Bubbles	T0103.001:
Video
Livestream	T0046:
Use
Search
Engine
Optimization	T0115.002:
Post
Violative
Content
to
Provoke
Takedown
and
Backlash	T0049.005:
Conduct
Swarming	T0048.004:
Dox	T0061:
Sell
Merchandise	T0128.003:
Distance
Reputable
Individuals
from
Operation	T0133.001:
Behavior
changes
	T0078:
Dismay	T0080:
Map
Target
Audience
Information
Environment	T0068:
Respond
to
Breaking
News
Event
or
Active
Crisis	T0023.002:
Edit
Open-Source
Content	T0065:
Prepare
Physical
Broadcast
Capabilities	T0098.001:
Create
Inauthentic
News
Sites	T0102.003:
Exploit
Data
Voids	T0103.002:
Audio
Livestream	T0113:
Employ
Commercial
Analytic
Firms	T0115.003:
One-Way
Direct
Posting	T0049.006:
Conduct
Keyword
Squatting	T0123:
Control
Information
Environment
through
Offensive
Cyberspace
Operations	T0126:
Encourage
Attendance
at
Events	T0128.004:
Launder
Accounts	T0133.002:
Content
	T0079:
Divide	T0080.001:
Monitor
Social
Media
Analytics	T0082:
Develop
New
Narratives	T0084:
Reuse
Existing
Content	T0090:
Create
Inauthentic
Accounts	T0098.002:
Leverage
Existing
Inauthentic
News
Sites		T0104:
Social
Networks		T0116:
Comment
or
Reply
on
Content	T0049.007:
Inauthentic
Sites
Amplify
News
and
Narratives	T0123.001:
Delete
Opposing
Content	T0126.001:
Call
to
action
to
attend
	T0128.005:
Change
Names
of
Accounts	T0133.003:
Awareness
		T0080.002:
Evaluate
Media
Surveys	T0083:
Integrate
Target
Audience
Vulnerabilities
into
Narrative	T0084.001:
Use
Copypasta	T0090.001:
Create
Anonymous
Accounts	T0099:
Prepare
Assets
Impersonating
Legitimate
Entities		T0104.001:
Mainstream
Social
Networks		T0116.001:
Post
inauthentic
social
media
comment	T0118:
Amplify
Existing
Narrative	T0123.002:
Block
Content	T0126.002:
Facilitate
logistics
or
support
for
attendance	T0129:
Conceal
Operational
Activity	T0133.004:
Knowledge
		T0080.003:
Identify
Trending
Topics/Hashtags		T0084.002:
Plagiarize
Content	T0090.002:
Create
Cyborg
Accounts	T0099.001:
Astroturfing		T0104.002:
Dating
Apps		T0117:
Attract
Traditional
Media	T0119:
Cross-Posting	T0123.003:
Destroy
Information
Generation
Capabilities	T0127:
Physical
Violence	T0129.001:
Conceal
Network
Identity	T0133.005:
Action/attitude
		T0080.004:
Conduct
Web
Traffic
Analysis		T0084.003:
Deceptively
Labeled
or
Translated	T0090.003:
Create
Bot
Accounts	T0099.002:
Spoof/parody
account/site		T0104.003:
Private/Closed
Social
Networks			T0119.001:
Post
Across
Groups	T0123.004:
Conduct
Server
Redirect	T0127.001:
Conduct
Physical
Violence	T0129.002:
Generate
Content
Unrelated
to
Narrative	T0134:
Measure
Effectiveness
Indicators
(or
KPIs)
		T0080.005:
Assess
Degree/Type
of
Media
Access		T0084.004:
Appropriate
Content	T0090.004:
Create
Sockpuppet
Accounts	T0100:
Co-opt
Trusted
Sources		T0104.004:
Interest-Based
Networks			T0119.002:
Post
Across
Platform	T0124:
Suppress
Opposition	T0127.002:
Encourage
Physical
Violence	T0129.003:
Break
Association
with
Content	T0134.001:
Message
reach
		T0081:
Identify
Social
and
Technical
Vulnerabilities		T0085:
Develop
Text-based
Content	T0091:
Recruit
malign
actors	T0100.001:
Co-Opt
Trusted
Individuals		T0104.005:
Use
hashtags			T0119.003:
Post
Across
Disciplines	T0124.001:
Report
Non-Violative
Opposing
Content		T0129.004:
Delete
URLs	T0134.002:
Social
media
engagement
		T0081.001:
Find
Echo
Chambers		T0085.001:
Develop
AI-Generated
Text	T0091.001:
Recruit
Contractors	T0100.002:
Co-Opt
Grassroots
Groups		T0104.006:
Create
dedicated
hashtag			T0120:
Incentivize
Sharing	T0124.002:
Goad
People
into
Harmful
Action
(Stop
Hitting
Yourself)		T0129.005:
Coordinate
on
encrypted/closed
networks	
		T0081.002:
Identify
Data
Voids		T0085.002:
Develop
False
or
Altered
Documents	T0091.002:
Recruit
Partisans	T0100.003:
Co-opt
Influencers		T0105:
Media
Sharing
Networks			T0120.001:
Use
Affiliate
Marketing
Programs	T0124.003:
Exploit
Platform
TOS/Content
Moderation		T0129.006:
Deny
involvement	
		T0081.003:
Identify
Existing
Prejudices		T0085.003:
Develop
Inauthentic
News
Articles	T0091.003:
Enlist
Troll
Accounts			T0105.001:
Photo
Sharing			T0120.002:
Use
Contests
and
Prizes	T0125:
Platform
Filtering		T0129.007:
Delete
Accounts/Account
Activity	
		T0081.004:
Identify
Existing
Fissures		T0086:
Develop
Image-based
Content	T0092:
Build
Network			T0105.002:
Video
Sharing			T0121:
Manipulate
Platform
Algorithm			T0129.008:
Redirect
URLs	
		T0081.005:
Identify
Existing
Conspiracy
Narratives/Suspicions		T0086.001:
Develop
Memes	T0092.001:
Create
Organizations			T0105.003:
Audio
sharing			T0121.001:
Bypass
Content
Blocking			T0129.009:
Remove
Post
Origins	
		T0081.006:
Identify
Wedge
Issues		T0086.002:
Develop
AI-Generated
Images
(Deepfakes)	T0092.002:
Use
Follow
Trains			T0106:
Discussion
Forums			T0122:
Direct
Users
to
Alternative
Platforms			T0129.010:
Misattribute
Activity	
		T0081.007:
Identify
Target
Audience
Adversaries		T0086.003:
Deceptively
Edit
Images
(Cheap
fakes)	T0092.003:
Create
Community
or
Sub-group			T0106.001:
Anonymous
Message
Boards						T0130:
Conceal
Infrastructure	
		T0081.008:
Identify
Media
System
Vulnerabilities		T0086.004:
Aggregate
Information
into
Evidence
Collages	T0093:
Acquire/Recruit
Network			T0107:
Bookmarking
and
Content
Curation						T0130.001:
Conceal
Sponsorship	
				T0087:
Develop
Video-based
Content	T0093.001:
Fund
Proxies			T0108:
Blogging
and
Publishing
Networks						T0130.002:
Utilize
Bulletproof
Hosting	
				T0087.001:
Develop
AI-Generated
Videos
(Deepfakes)	T0093.002:
Acquire
Botnets			T0109:
Consumer
Review
Networks						T0130.003:
Use
Shell
Organizations	
				T0087.002:
Deceptively
Edit
Video
(Cheap
fakes)	T0094:
Infiltrate
Existing
Networks			T0110:
Formal
Diplomatic
Channels						T0130.004:
Use
Cryptocurrency	
				T0088:
Develop
Audio-based
Content	T0094.001:
Identify
susceptible
targets
in
networks			T0111:
Traditional
Media						T0130.005:
Obfuscate
Payment	
				T0088.001:
Develop
AI-Generated
Audio
(Deepfakes)	T0094.002:
Utilize
Butterfly
Attacks			T0111.001:
TV						T0131:
Exploit
TOS/Content
Moderation	
				T0088.002:
Deceptively
Edit
Audio
(Cheap
fakes)	T0095:
Develop
Owned
Media
Assets			T0111.002:
Newspaper						T0131.001:
Legacy
web
content	
				T0089:
Obtain
Private
Documents	T0096:
Leverage
Content
Farms			T0111.003:
Radio						T0131.002:
Post
Borderline
Content	
				T0089.001:
Obtain
Authentic
Documents	T0096.001:
Create
Content
Farms			T0112:
Email							
				T0089.002:
Create
Inauthentic
Documents	T0096.002:
Outsource
Content
Creation
to
External
Organizations										
				T0089.003:
Alter
Authentic
Documents											

DISARM Blue Framework - responder TTPs
TA01:
Plan Strategy	TA02:
Plan Objectives	TA05:
Microtarget	TA06:
Develop Content	TA07:
Select Channels and Affordances	TA08:
Conduct Pump Priming	TA09:
Deliver Content	TA11:
Persist in the Information Environment	TA12:
Assess Effectiveness	TA15:
Establish Social Assets
C00016:
Censorship	C00207:
Run a competing disinformation campaign - not recommended	C00065:
Reduce political targeting	C00085:
Mute content	C00195:
Redirect searches away from disinformation or extremist content 	C00117:
Downgrade / de-amplify so message is seen by fewer people	C00147:
Make amplification of social media posts expire (e.g. can't like/ retweet after n days)	C00138:
Spam domestic actors with lawsuits	C00140:
"Bomb" link shorteners with lots of calls	C00040:
third party verification for people
C00017:
Repair broken social connections	C00164:
compatriot policy	C00066:
Co-opt a hashtag and drown it out (hijack it back)	C00014:
Real-time updates to fact-checking database	C00098:
Revocation of allowlisted or "verified" status	C00119:
Engage payload and debunk.	C00128:
Create friction by marking content with ridicule or other "decelerants"	C00139:
Weaponise youtube content matrices	C00148:
Add random links to network graphs	C00059:
Verification of project before posting fund requests
C00019:
Reduce effect of division-enablers	C00092:
Establish a truth teller reputation score for influencers	C00178:
Fill information voids with non-disinformation content	C00032:
Hijack content and link to truth- based info	C00105:
Buy more advertising than misinformation creators	C00120:
Open dialogue about design of platforms to produce different outcomes	C00129:
Use banking to cut off access 	C00131:
Seize and analyse botnet servers	C00149:
Poison the monitoring & evaluation data	C00058:
Report crowdfunder as violator
C00021:
Encourage in-person communication	C00222:
Tabletop simulations	C00216:
Use advertiser controls to stem flow of funds to bad actors	C00071:
Block source of pollution	C00103:
Create a bot that engages / distract trolls	C00121:
Tool transparency and literacy for channels people follow. 	C00182:
Redirection / malware detection/ remediation	C00143:
(botnet) DMCA takedown requests to waste group time		C00172:
social media source removal
C00022:
Innoculate. Positive campaign to promote feeling of safety	C00070:
Block access to disinformation resources	C00130:
Mentorship: elders, youth, credit. Learn vicariously.	C00072:
Remove non-relevant content from special interest groups - not recommended	C00101:
Create friction by rate-limiting engagement	C00112:
"Prove they are not an op!"	C00200:
Respected figure (influencer) disavows misinfo			C00056:
Encourage people to leave social media
C00006:
Charge for social media	C00169:
develop a creative content hub		C00074:
Identify and delete or rate limit identical content	C00097:
Require use of verified identities to contribute to poll or comment	C00100:
Hashtag jacking	C00109:
Dampen Emotional Reaction			C00053:
Delete old accounts / Remove unused social media accounts
C00024:
Promote healthy narratives	C00060:
Legal action against for-profit engagement factories		C00075:
normalise language	C00099:
Strengthen verification methods	C00154:
Ask media not to report false information	C00211:
Use humorous counter-narratives			C00052:
Infiltrate platforms
C00026:
Shore up democracy based messages	C00156:
Better tell your country or organization story		C00076:
Prohibit images in political discourse channels	C00090:
Fake engagement system	C00136:
Microtarget most likely targets then send them countermessages	C00122:
Content moderation			C00062:
Free open library sources worldwide
C00027:
Create culture of civility	C00028:
Make information provenance available		C00078:
Change Search Algorithms for Disinformation Content		C00188:
Newsroom/Journalist training to counter influence moves	C00123:
Remove or rate limit botnets			C00162:
Unravel/target the Potemkin villages
C00153:
Take pre-emptive action against actors' infrastructure	C00144:
Buy out troll farm employees / offer them jobs		C00080:
Create competing narrative		C00184:
Media exposure	C00124:
Don't feed the trolls			C00067:
Denigrate the recipient/ project (of online funding)
C00096:
Strengthen institutions that are always truth tellers	C00029:
Create fake website to issue counter narrative and counter narrative through physical merchandise		C00081:
Highlight flooding and noise, and explain motivations		C00113:
Debunk and defuse a fake expert / credentials.	C00125:
Prebunking			C00189:
Ensure that platforms are taking down flagged accounts
C00111:
Reduce polarisation by connecting and presenting sympathetic renditions of opposite views	C00030:
Develop a compelling counter narrative (truth based)		C00082:
Ground truthing as automated response to pollution		C00114:
Don't engage with payloads	C00126:
Social media amber alert			C00051:
Counter social engineering training
C00223:
Strengthen Trust in social media platforms	C00031:
Dilute the core narrative - create multiple permutations, target / amplify		C00084:
Modify disinformation narratives, and rebroadcast them		C00115:
Expose actor and intentions				C00160:
find and train influencers
C00221:
Run a disinformation red team, and design mitigation factors	C00009:
Educate high profile influencers on best practices		C00086:
Distract from noise with addictive content		C00116:
Provide proof of involvement				C00197:
remove suspicious accounts
C00220:
Develop a monitoring and intelligence plan	C00011:
Media literacy. Games to identify fake news		C00087:
Make more noise than the disinformation		C00118:
Repurpose images with new text				C00077:
Active defence: run TA15 "develop people” - not recommended
C00212:
build public resilience by making civil society more vibrant			C00091:
Honeypot social community						C00036:
Infiltrate the in-group to discredit leaders (divide)
C00205:
strong dialogue between the federal government and private sector to encourage better reporting			C00094:
Force full disclosure on corporate sponsor of research						C00203:
Stop offering press credentials to propaganda outlets
C00190:
open engagement with civil society			C00106:
Click-bait centrist content						C00048:
Name and Shame Influencers
C00176:
Improve Coordination amongst stakeholders: public and private			C00107:
Content moderation						C00047:
Honeypot with coordinated inauthentics
C00174:
Create a healthier news environment			C00142:
Platform adds warning label and decision point when sharing content						C00155:
Ban incident actors from funding sites
C00170:
elevate information as a critical domain of statecraft			C00165:
Ensure integrity of official documents						C00046:
Marginalise and discredit extremist groups
C00161:
Coalition Building with stakeholders and Third-Party Inducements			C00202:
Set data 'honeytraps'						C00093:
Influencer code of conduct
C00010:
Enhanced privacy regulation for social media			C00219:
Add metadata to content that’s out of the control of disinformation creators						C00042:
Address truth contained in narratives
C00073:
Inoculate populations through media literacy training									C00135:
Deplatform message groups and/or message boards
C00012:
Platform regulation									C00133:
Deplatform Account*
C00013:
Rating framework for news									C00044:
Keep people from posting to social media immediately
C00008:
Create shared fact-checking database									C00034:
Create more friction at account creation
C00159:
Have a disinformation response plan									
DISARM Frameworks provided CC-by-SA by the DISARM Foundation
