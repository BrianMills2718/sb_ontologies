#!/usr/bin/env python3
"""
Generated HarnessComponent
Auto-generated by V5.0 Two-Phase Generation Pipeline
"""

import asyncio
import logging
import time
import json
from typing import Dict, Any, Optional, List

# Import HarnessComponent base class
from evidence.phase6_harness.day1_harness_component.harness_component import HarnessComponent
from evidence.phase2_component_library.day1_core_component_classes.enhanced_base import ComponentConfiguration
import numpy as np
from datetime import datetime


class Data_Processor(HarnessComponent):
    """
    Generated data_processor component
    
    Auto-generated by V5.0 Two-Phase Generation Pipeline
    Extends HarnessComponent for stream-based communication
    """


    def __init__(self, config: Dict[str, Any]):
        """Initialize data_processor component"""
        component_config = ComponentConfiguration(
            name="data_processor",
            service_type="data_processor",
            **config
        )
        
        super().__init__(component_config)
        
        # Component-specific initialization
        self.config = config
        self.processing_stats = {
            "messages_processed": 0,
            "processing_time_total": 0.0,
            "errors_count": 0
        }
        
        self.logger.info(f"ðŸ”§ {self.name} (data_processor) initialized")


    async def _initialize_component_resources(self):
        """Initialize component-specific resources"""
        self.logger.info(f"ðŸ”§ Initializing {self.name} resources")
        
        try:
            
        # Initialize stream error tracking
        self.stream_errors = {}
        
        # Initialize resource handles
        self.resource_handles = {}
        
        # Validate expected streams will be available
        self.expected_input_streams = ['input', 'process']
        self.expected_output_streams = ['processed_data', 'results']
        
        # Initialize component state
        self.component_state = {
            "initialized": True,
            "initialization_time": time.time(),
            "status": "ready"
        }
            
            
        # Initialize processing configuration
        self.batch_size = self.config.get('batch_size', 10)
        self.processing_interval = self.config.get('processing_interval', 1.0)
        self.processing_mode = self.config.get('processing_mode', 'async')
        
        # Initialize processing buffers
        self.processing_buffer = []
        self.processed_count = 0
        
        # Setup processing pipeline
        self.processing_pipeline = {
            "stages": [],
            "current_stage": 0,
            "processing_stats": {
                "total_processed": 0,
                "average_processing_time": 0.0,
                "error_count": 0
            }
        }
            
            
            
            self.logger.info(f"âœ… {self.name} resources initialized successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.name} resource initialization failed: {e}")
            raise


    async def process(self):
        """Main processing loop with stream handling"""
        try:
            
            # Poll multiple input streams
            streams_checked = 0
            
                # Check input stream
                if 'input' in self.receive_streams:
                    try:
                        message = await self.receive_message('input', timeout=0.1)
                        if message is not None:
                            result = await self.process_message(message)
                            if result:
                                await self.send_result(result)
                    except Exception as e:
                        await self.handle_stream_error('input', e)

                # Check process stream
                if 'process' in self.receive_streams:
                    try:
                        message = await self.receive_message('process', timeout=0.1)
                        if message is not None:
                            result = await self.process_message(message)
                            if result:
                                await self.send_result(result)
                    except Exception as e:
                        await self.handle_stream_error('process', e)
            
            # Brief pause if no messages processed
            if streams_checked == 0:
                await asyncio.sleep(0.01)
            
        except Exception as e:
            self.logger.error(f"âŒ Processing error in {self.name}: {e}")
            await self.handle_processing_error(e)
    
    async def process_message(self, message_data: Any) -> Optional[Dict[str, Any]]:
        """Process individual message"""
        start_time = time.time()
        
        try:
            # Business logic processing
            result = await self.process_data(message_data)
            
            # Update statistics
            processing_time = time.time() - start_time
            self.processing_stats["messages_processed"] += 1
            self.processing_stats["processing_time_total"] += processing_time
            
            return result
            
        except Exception as e:
            self.processing_stats["errors_count"] += 1
            self.logger.error(f"âŒ Message processing error: {e}")
            raise
    
    async def send_result(self, result: Dict[str, Any]):
        """Send processing result to appropriate output streams"""
        
        # Send to processed_data stream
        if 'processed_data' in self.send_streams:
            try:
                await self.send_message('processed_data', result)
                self.logger.debug(f"âœ… Sent result to processed_data")
            except Exception as e:
                self.logger.error(f"âŒ Failed to send to processed_data: {e}")

        # Send to results stream
        if 'results' in self.send_streams:
            try:
                await self.send_message('results', result)
                self.logger.debug(f"âœ… Sent result to results")
            except Exception as e:
                self.logger.error(f"âŒ Failed to send to results: {e}")
    
    async def handle_processing_error(self, error: Exception):
        """Handle processing errors with recovery"""
        self.logger.warning(f"âš ï¸ Handling processing error: {error}")
        
        # Error recovery logic
        await asyncio.sleep(0.1)  # Brief pause before retry


    async def _cleanup_component_resources(self):
        """Clean up component-specific resources"""
        self.logger.info(f"ðŸ§¹ Cleaning up {self.name} resources")
        
        try:
            
            # Flush processing buffers
            if hasattr(self, 'processing_buffer'):
                self.processing_buffer.clear()
            
            # Clear processing pipeline
            if hasattr(self, 'processing_pipeline'):
                self.processing_pipeline['stages'].clear()
            
            
            
            # Close any remaining resources
            if hasattr(self, 'resource_handles'):
                for handle_name, handle in self.resource_handles.items():
                    try:
                        if hasattr(handle, 'close'):
                            await handle.close()
                        elif hasattr(handle, 'disconnect'):
                            await handle.disconnect()
                        self.logger.debug(f"âœ… Closed {handle_name}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Error closing {handle_name}: {e}")
            
            self.logger.info(f"âœ… {self.name} cleanup completed")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.name} cleanup failed: {e}")
            # Don't re-raise cleanup errors


    async def process_data(self, data: Any) -> Dict[str, Any]:
        """
        Process and transform data
        Generated from ComponentLogic definition
        """
        try:
            # Business logic implementation
            
            # Data transformation logic
            processed_data = data.copy() if isinstance(data, dict) else {"raw_data": data}
            
            # Add processing metadata
            processed_data.update({
                "processed": True,
                "processing_timestamp": time.time(),
                "processor": self.name
            })
            
            # Simulate data enrichment
            if "title" in processed_data:
                processed_data["title_length"] = len(str(processed_data["title"]))
            
            result = {
                "status": "processed",
                "data": processed_data
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Business method process_data error: {e}")
            raise


    async def handle_stream_error(self, stream_name: str, error: Exception):
        """Handle stream operation errors"""
        self.logger.warning(f"âš ï¸ Stream error on {stream_name}: {error}")
        
        # Record error for monitoring
        if not hasattr(self, 'stream_errors'):
            self.stream_errors = {}
        
        if stream_name not in self.stream_errors:
            self.stream_errors[stream_name] = 0
        
        self.stream_errors[stream_name] += 1
        
        # Exponential backoff for repeated errors
        if self.stream_errors[stream_name] > 3:
            await asyncio.sleep(min(2.0 ** (self.stream_errors[stream_name] - 3), 30.0))

    def get_stream_status(self) -> Dict[str, Any]:
        """Get status of all component streams"""
        status = {
            "input_streams": {},
            "output_streams": {},
            "stream_errors": getattr(self, 'stream_errors', {})
        }
        
        # Check input streams
        for stream_name in ['input', 'process']:
            if stream_name in self.receive_streams:
                connection = self.receive_streams[stream_name]
                status["input_streams"][stream_name] = {
                    "connected": True,
                    "message_count": connection.message_count,
                    "last_activity": connection.last_activity
                }
            else:
                status["input_streams"][stream_name] = {"connected": False}
        
        # Check output streams  
        for stream_name in ['processed_data', 'results']:
            if stream_name in self.send_streams:
                connection = self.send_streams[stream_name]
                status["output_streams"][stream_name] = {
                    "connected": True,
                    "message_count": connection.message_count,
                    "last_activity": connection.last_activity
                }
            else:
                status["output_streams"][stream_name] = {"connected": False}
                
        return status

    async def send_to_multiple_streams(self, data: Dict[str, Any], stream_names: List[str] = None):
        """Send data to multiple output streams"""
        target_streams = stream_names or ['processed_data', 'results']
        
        results = {}
        for stream_name in target_streams:
            try:
                success = await self.send_message(stream_name, data)
                results[stream_name] = success
            except Exception as e:
                results[stream_name] = False
                self.logger.error(f"âŒ Failed to send to {stream_name}: {e}")
        
        return results

    async def check_stream_connectivity(self) -> Dict[str, bool]:
        """Check connectivity of all streams"""
        connectivity = {}
        
        # Check input streams
        for stream_name, connection in self.receive_streams.items():
            try:
                # Simple connectivity check - stream should not be closed
                connectivity[f"input_{stream_name}"] = not connection.stream._closed
            except:
                connectivity[f"input_{stream_name}"] = False
        
        # Check output streams
        for stream_name, connection in self.send_streams.items():
            try:
                connectivity[f"output_{stream_name}"] = not connection.stream._closed
            except:
                connectivity[f"output_{stream_name}"] = False
        
        return connectivity

    def get_stream_metrics(self) -> Dict[str, Any]:
        """Collect stream performance metrics"""
        metrics = {
            "total_input_streams": len(self.receive_streams),
            "total_output_streams": len(self.send_streams),
            "total_messages_received": 0,
            "total_messages_sent": 0,
            "stream_errors": sum(getattr(self, 'stream_errors', {}).values()),
            "stream_details": {}
        }
        
        # Input stream metrics
        for stream_name, connection in self.receive_streams.items():
            metrics["total_messages_received"] += connection.message_count
            metrics["stream_details"][f"input_{stream_name}"] = {
                "messages": connection.message_count,
                "last_activity": connection.last_activity,
                "buffer_size": connection.buffer_size
            }
        
        # Output stream metrics
        for stream_name, connection in self.send_streams.items():
            metrics["total_messages_sent"] += connection.message_count
            metrics["stream_details"][f"output_{stream_name}"] = {
                "messages": connection.message_count,
                "last_activity": connection.last_activity,
                "buffer_size": connection.buffer_size
            }
        
        return metrics