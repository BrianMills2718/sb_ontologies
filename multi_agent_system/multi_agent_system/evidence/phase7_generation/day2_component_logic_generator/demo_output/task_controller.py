#!/usr/bin/env python3
"""
Generated HarnessComponent
Auto-generated by V5.0 Two-Phase Generation Pipeline
"""

import asyncio
import logging
import time
import json
from typing import Dict, Any, Optional, List

# Import HarnessComponent base class
from evidence.phase6_harness.day1_harness_component.harness_component import HarnessComponent
from evidence.phase2_component_library.day1_core_component_classes.enhanced_base import ComponentConfiguration


class Task_Controller(HarnessComponent):
    """
    Generated task_controller component
    
    Auto-generated by V5.0 Two-Phase Generation Pipeline
    Extends HarnessComponent for stream-based communication
    """


    def __init__(self, config: Dict[str, Any]):
        """Initialize task_controller component"""
        component_config = ComponentConfiguration(
            name="task_controller",
            service_type="task_controller",
            **config
        )
        
        super().__init__(component_config)
        
        # Component-specific initialization
        self.config = config
        self.processing_stats = {
            "messages_processed": 0,
            "processing_time_total": 0.0,
            "errors_count": 0
        }
        
        self.logger.info(f"ðŸ”§ {self.name} (task_controller) initialized")


    async def _initialize_component_resources(self):
        """Initialize component-specific resources"""
        self.logger.info(f"ðŸ”§ Initializing {self.name} resources")
        
        try:
            
        # Initialize stream error tracking
        self.stream_errors = {}
        
        # Initialize resource handles
        self.resource_handles = {}
        
        # Validate expected streams will be available
        self.expected_input_streams = ['input', 'commands', 'store_data']
        self.expected_output_streams = ['store_commands', 'responses', 'process_requests']
        
        # Initialize component state
        self.component_state = {
            "initialized": True,
            "initialization_time": time.time(),
            "status": "ready"
        }
            
            
        # Initialize task management
        self.max_concurrent_tasks = self.config.get('max_concurrent_tasks', 10)
        self.processing_timeout = self.config.get('processing_timeout', 30)
        
        # Initialize task tracking
        self.active_tasks = {}
        self.task_queue = []
        self.completed_tasks = {}
        
        # Initialize task statistics
        self.task_stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "average_processing_time": 0.0
        }
        
        # Setup task processing semaphore
        self.task_semaphore = asyncio.Semaphore(self.max_concurrent_tasks)
            
            
            
            self.logger.info(f"âœ… {self.name} resources initialized successfully")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.name} resource initialization failed: {e}")
            raise


    async def process(self):
        """Main processing loop with stream handling"""
        try:
            
            # Poll multiple input streams
            streams_checked = 0
            
                # Check input stream
                if 'input' in self.receive_streams:
                    try:
                        message = await self.receive_message('input', timeout=0.1)
                        if message is not None:
                            result = await self.process_message(message)
                            if result:
                                await self.send_result(result)
                    except Exception as e:
                        await self.handle_stream_error('input', e)

                # Check commands stream
                if 'commands' in self.receive_streams:
                    try:
                        message = await self.receive_message('commands', timeout=0.1)
                        if message is not None:
                            result = await self.process_message(message)
                            if result:
                                await self.send_result(result)
                    except Exception as e:
                        await self.handle_stream_error('commands', e)

                # Check store_data stream
                if 'store_data' in self.receive_streams:
                    try:
                        message = await self.receive_message('store_data', timeout=0.1)
                        if message is not None:
                            result = await self.process_message(message)
                            if result:
                                await self.send_result(result)
                    except Exception as e:
                        await self.handle_stream_error('store_data', e)
            
            # Brief pause if no messages processed
            if streams_checked == 0:
                await asyncio.sleep(0.01)
            
        except Exception as e:
            self.logger.error(f"âŒ Processing error in {self.name}: {e}")
            await self.handle_processing_error(e)
    
    async def process_message(self, message_data: Any) -> Optional[Dict[str, Any]]:
        """Process individual message"""
        start_time = time.time()
        
        try:
            # Business logic processing
            result = await self.process_task_operation(message_data)
            result = await self.handle_store_response(message_data)
            
            # Update statistics
            processing_time = time.time() - start_time
            self.processing_stats["messages_processed"] += 1
            self.processing_stats["processing_time_total"] += processing_time
            
            return result
            
        except Exception as e:
            self.processing_stats["errors_count"] += 1
            self.logger.error(f"âŒ Message processing error: {e}")
            raise
    
    async def send_result(self, result: Dict[str, Any]):
        """Send processing result to appropriate output streams"""
        
        # Send to store_commands stream
        if 'store_commands' in self.send_streams:
            try:
                await self.send_message('store_commands', result)
                self.logger.debug(f"âœ… Sent result to store_commands")
            except Exception as e:
                self.logger.error(f"âŒ Failed to send to store_commands: {e}")

        # Send to responses stream
        if 'responses' in self.send_streams:
            try:
                await self.send_message('responses', result)
                self.logger.debug(f"âœ… Sent result to responses")
            except Exception as e:
                self.logger.error(f"âŒ Failed to send to responses: {e}")

        # Send to process_requests stream
        if 'process_requests' in self.send_streams:
            try:
                await self.send_message('process_requests', result)
                self.logger.debug(f"âœ… Sent result to process_requests")
            except Exception as e:
                self.logger.error(f"âŒ Failed to send to process_requests: {e}")
    
    async def handle_processing_error(self, error: Exception):
        """Handle processing errors with recovery"""
        self.logger.warning(f"âš ï¸ Handling processing error: {error}")
        
        # Error recovery logic
        await asyncio.sleep(0.1)  # Brief pause before retry


    async def _cleanup_component_resources(self):
        """Clean up component-specific resources"""
        self.logger.info(f"ðŸ§¹ Cleaning up {self.name} resources")
        
        try:
            
            # Cancel active tasks
            if hasattr(self, 'active_tasks'):
                for task_id, task in self.active_tasks.items():
                    try:
                        if not task.done():
                            task.cancel()
                    except:
                        pass
                self.active_tasks.clear()
            
            # Clear task queue
            if hasattr(self, 'task_queue'):
                self.task_queue.clear()
            
            
            
            # Close any remaining resources
            if hasattr(self, 'resource_handles'):
                for handle_name, handle in self.resource_handles.items():
                    try:
                        if hasattr(handle, 'close'):
                            await handle.close()
                        elif hasattr(handle, 'disconnect'):
                            await handle.disconnect()
                        self.logger.debug(f"âœ… Closed {handle_name}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Error closing {handle_name}: {e}")
            
            self.logger.info(f"âœ… {self.name} cleanup completed")
            
        except Exception as e:
            self.logger.error(f"âŒ {self.name} cleanup failed: {e}")
            # Don't re-raise cleanup errors


    async def process_task_operation(self, data: Any) -> Dict[str, Any]:
        """
        Process various task operations
        Generated from ComponentLogic definition
        """
        try:
            # Business logic implementation
            
            operation_type = data.get('type', 'unknown')
            
            if operation_type == 'create_task':
                # Process task creation
                task_data = data.get('data', {})
                task_id = f"task_{int(time.time() * 1000)}"
                
                # Enrich task data
                enriched_task = {
                    'id': task_id,
                    'title': task_data.get('title', 'Untitled'),
                    'description': task_data.get('description', ''),
                    'status': 'pending',
                    'created_at': time.time(),
                    'updated_at': time.time()
                }
                
                result = {
                    'operation': 'store_task',
                    'data': enriched_task
                }
                
            elif operation_type == 'list_tasks':
                # Process task listing
                query = data.get('data', {})
                result = {
                    'operation': 'retrieve_tasks',
                    'data': query
                }
                
            else:
                result = {
                    'operation': 'error',
                    'message': f'Unknown operation: {operation_type}'
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Business method process_task_operation error: {e}")
            raise

    async def handle_store_response(self, data: Any) -> Dict[str, Any]:
        """
        Handle responses from task store
        Generated from ComponentLogic definition
        """
        try:
            # Business logic implementation
            
            # Process store response
            store_status = data.get('status', 'unknown')
            
            if store_status == 'stored':
                # Task successfully stored
                result = {
                    'status': 'success',
                    'message': 'Task created successfully',
                    'data': data.get('data', {})
                }
            elif store_status == 'retrieved':
                # Tasks successfully retrieved
                result = {
                    'status': 'success',
                    'message': 'Tasks retrieved successfully',
                    'data': data.get('data', [])
                }
            else:
                result = {
                    'status': 'error',
                    'message': f'Store operation failed: {store_status}'
                }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Business method handle_store_response error: {e}")
            raise


    async def handle_stream_error(self, stream_name: str, error: Exception):
        """Handle stream operation errors"""
        self.logger.warning(f"âš ï¸ Stream error on {stream_name}: {error}")
        
        # Record error for monitoring
        if not hasattr(self, 'stream_errors'):
            self.stream_errors = {}
        
        if stream_name not in self.stream_errors:
            self.stream_errors[stream_name] = 0
        
        self.stream_errors[stream_name] += 1
        
        # Exponential backoff for repeated errors
        if self.stream_errors[stream_name] > 3:
            await asyncio.sleep(min(2.0 ** (self.stream_errors[stream_name] - 3), 30.0))

    def get_stream_status(self) -> Dict[str, Any]:
        """Get status of all component streams"""
        status = {
            "input_streams": {},
            "output_streams": {},
            "stream_errors": getattr(self, 'stream_errors', {})
        }
        
        # Check input streams
        for stream_name in ['input', 'commands', 'store_data']:
            if stream_name in self.receive_streams:
                connection = self.receive_streams[stream_name]
                status["input_streams"][stream_name] = {
                    "connected": True,
                    "message_count": connection.message_count,
                    "last_activity": connection.last_activity
                }
            else:
                status["input_streams"][stream_name] = {"connected": False}
        
        # Check output streams  
        for stream_name in ['store_commands', 'responses', 'process_requests']:
            if stream_name in self.send_streams:
                connection = self.send_streams[stream_name]
                status["output_streams"][stream_name] = {
                    "connected": True,
                    "message_count": connection.message_count,
                    "last_activity": connection.last_activity
                }
            else:
                status["output_streams"][stream_name] = {"connected": False}
                
        return status

    async def send_to_multiple_streams(self, data: Dict[str, Any], stream_names: List[str] = None):
        """Send data to multiple output streams"""
        target_streams = stream_names or ['store_commands', 'responses', 'process_requests']
        
        results = {}
        for stream_name in target_streams:
            try:
                success = await self.send_message(stream_name, data)
                results[stream_name] = success
            except Exception as e:
                results[stream_name] = False
                self.logger.error(f"âŒ Failed to send to {stream_name}: {e}")
        
        return results

    async def check_stream_connectivity(self) -> Dict[str, bool]:
        """Check connectivity of all streams"""
        connectivity = {}
        
        # Check input streams
        for stream_name, connection in self.receive_streams.items():
            try:
                # Simple connectivity check - stream should not be closed
                connectivity[f"input_{stream_name}"] = not connection.stream._closed
            except:
                connectivity[f"input_{stream_name}"] = False
        
        # Check output streams
        for stream_name, connection in self.send_streams.items():
            try:
                connectivity[f"output_{stream_name}"] = not connection.stream._closed
            except:
                connectivity[f"output_{stream_name}"] = False
        
        return connectivity

    def get_stream_metrics(self) -> Dict[str, Any]:
        """Collect stream performance metrics"""
        metrics = {
            "total_input_streams": len(self.receive_streams),
            "total_output_streams": len(self.send_streams),
            "total_messages_received": 0,
            "total_messages_sent": 0,
            "stream_errors": sum(getattr(self, 'stream_errors', {}).values()),
            "stream_details": {}
        }
        
        # Input stream metrics
        for stream_name, connection in self.receive_streams.items():
            metrics["total_messages_received"] += connection.message_count
            metrics["stream_details"][f"input_{stream_name}"] = {
                "messages": connection.message_count,
                "last_activity": connection.last_activity,
                "buffer_size": connection.buffer_size
            }
        
        # Output stream metrics
        for stream_name, connection in self.send_streams.items():
            metrics["total_messages_sent"] += connection.message_count
            metrics["stream_details"][f"output_{stream_name}"] = {
                "messages": connection.message_count,
                "last_activity": connection.last_activity,
                "buffer_size": connection.buffer_size
            }
        
        return metrics